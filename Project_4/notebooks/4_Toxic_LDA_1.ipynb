{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 17)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('../data/toxictrain.pkl')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 17)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[-100:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2),  \n",
    "                                   stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "count_vectorizer.fit(df.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "counts = count_vectorizer.transform(df.comment_text).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5058, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in count_vectorizer.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5058"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-28 08:46:35,622 : INFO : using symmetric alpha at 0.3333333333333333\n",
      "2018-02-28 08:46:35,626 : INFO : using symmetric eta at 0.3333333333333333\n",
      "2018-02-28 08:46:35,631 : INFO : using serial LDA version on this node\n",
      "2018-02-28 08:46:35,767 : INFO : running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 100 documents, updating model once every 100 documents, evaluating perplexity every 100 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2018-02-28 08:46:36,596 : INFO : -9.926 per-word bound, 972.7 perplexity estimate based on a held-out corpus of 100 documents with 7004 words\n",
      "2018-02-28 08:46:36,598 : INFO : PROGRESS: pass 0, at document #100/100\n",
      "2018-02-28 08:46:37,334 : INFO : topic #0 (0.333): 0.004*\"wikipedia\" + 0.004*\"article\" + 0.003*\"talk\" + 0.003*\"page\" + 0.003*\"male\" + 0.002*\"like\" + 0.002*\"female\" + 0.002*\"salute\" + 0.002*\"articles\" + 0.002*\"don\"\n",
      "2018-02-28 08:46:37,335 : INFO : topic #1 (0.333): 0.005*\"article\" + 0.005*\"wikipedia\" + 0.004*\"male\" + 0.003*\"like\" + 0.003*\"female\" + 0.003*\"page\" + 0.002*\"salute\" + 0.002*\"subject\" + 0.002*\"te\" + 0.002*\"male female\"\n",
      "2018-02-28 08:46:37,337 : INFO : topic #2 (0.333): 0.004*\"article\" + 0.003*\"wikipedia\" + 0.002*\"articles\" + 0.002*\"page\" + 0.002*\"salute\" + 0.002*\"copyright\" + 0.002*\"thanks\" + 0.002*\"female\" + 0.002*\"male\" + 0.002*\"used\"\n",
      "2018-02-28 08:46:37,341 : INFO : topic diff=0.773183, rho=1.000000\n",
      "2018-02-28 08:46:37,807 : INFO : -9.108 per-word bound, 551.9 perplexity estimate based on a held-out corpus of 100 documents with 7004 words\n",
      "2018-02-28 08:46:37,808 : INFO : PROGRESS: pass 1, at document #100/100\n",
      "2018-02-28 08:46:38,118 : INFO : topic #0 (0.333): 0.004*\"talk\" + 0.004*\"page\" + 0.004*\"article\" + 0.003*\"wikipedia\" + 0.002*\"like\" + 0.002*\"way\" + 0.002*\"don\" + 0.002*\"articles\" + 0.002*\"really\" + 0.002*\"think\"\n",
      "2018-02-28 08:46:38,120 : INFO : topic #1 (0.333): 0.006*\"wikipedia\" + 0.006*\"male\" + 0.005*\"article\" + 0.004*\"female\" + 0.004*\"salute\" + 0.003*\"like\" + 0.003*\"te\" + 0.003*\"subject\" + 0.003*\"male female\" + 0.002*\"page\"\n",
      "2018-02-28 08:46:38,128 : INFO : topic #2 (0.333): 0.004*\"article\" + 0.003*\"wikipedia\" + 0.003*\"copyright\" + 0.002*\"page\" + 0.002*\"articles\" + 0.002*\"image\" + 0.002*\"mexican\" + 0.002*\"thanks\" + 0.002*\"source\" + 0.002*\"deletion\"\n",
      "2018-02-28 08:46:38,132 : INFO : topic diff=0.480324, rho=0.577350\n",
      "2018-02-28 08:46:38,543 : INFO : -8.793 per-word bound, 443.6 perplexity estimate based on a held-out corpus of 100 documents with 7004 words\n",
      "2018-02-28 08:46:38,546 : INFO : PROGRESS: pass 2, at document #100/100\n",
      "2018-02-28 08:46:38,766 : INFO : topic #0 (0.333): 0.004*\"talk\" + 0.004*\"page\" + 0.003*\"article\" + 0.003*\"like\" + 0.002*\"way\" + 0.002*\"don\" + 0.002*\"wikipedia\" + 0.002*\"really\" + 0.002*\"think\" + 0.002*\"people\"\n",
      "2018-02-28 08:46:38,768 : INFO : topic #1 (0.333): 0.006*\"wikipedia\" + 0.006*\"male\" + 0.005*\"article\" + 0.005*\"female\" + 0.004*\"salute\" + 0.003*\"te\" + 0.003*\"male female\" + 0.003*\"subject\" + 0.003*\"like\" + 0.002*\"used\"\n",
      "2018-02-28 08:46:38,774 : INFO : topic #2 (0.333): 0.004*\"article\" + 0.003*\"wikipedia\" + 0.003*\"copyright\" + 0.002*\"image\" + 0.002*\"mexican\" + 0.002*\"page\" + 0.002*\"articles\" + 0.002*\"source\" + 0.002*\"thanks\" + 0.002*\"deletion\"\n",
      "2018-02-28 08:46:38,777 : INFO : topic diff=0.264982, rho=0.500000\n",
      "2018-02-28 08:46:39,348 : INFO : -8.700 per-word bound, 415.8 perplexity estimate based on a held-out corpus of 100 documents with 7004 words\n",
      "2018-02-28 08:46:39,350 : INFO : PROGRESS: pass 3, at document #100/100\n",
      "2018-02-28 08:46:39,591 : INFO : topic #0 (0.333): 0.004*\"page\" + 0.004*\"talk\" + 0.003*\"article\" + 0.003*\"like\" + 0.002*\"way\" + 0.002*\"don\" + 0.002*\"really\" + 0.002*\"think\" + 0.002*\"people\" + 0.002*\"metaphysical\"\n",
      "2018-02-28 08:46:39,596 : INFO : topic #1 (0.333): 0.007*\"wikipedia\" + 0.006*\"male\" + 0.005*\"article\" + 0.005*\"female\" + 0.004*\"salute\" + 0.003*\"te\" + 0.003*\"male female\" + 0.003*\"subject\" + 0.003*\"like\" + 0.002*\"used\"\n",
      "2018-02-28 08:46:39,600 : INFO : topic #2 (0.333): 0.004*\"article\" + 0.003*\"copyright\" + 0.003*\"wikipedia\" + 0.002*\"image\" + 0.002*\"mexican\" + 0.002*\"page\" + 0.002*\"source\" + 0.002*\"articles\" + 0.002*\"thanks\" + 0.002*\"deletion\"\n",
      "2018-02-28 08:46:39,604 : INFO : topic diff=0.143483, rho=0.447214\n",
      "2018-02-28 08:46:40,142 : INFO : -8.672 per-word bound, 407.8 perplexity estimate based on a held-out corpus of 100 documents with 7004 words\n",
      "2018-02-28 08:46:40,144 : INFO : PROGRESS: pass 4, at document #100/100\n",
      "2018-02-28 08:46:40,355 : INFO : topic #0 (0.333): 0.004*\"page\" + 0.003*\"talk\" + 0.003*\"article\" + 0.003*\"like\" + 0.002*\"way\" + 0.002*\"don\" + 0.002*\"really\" + 0.002*\"think\" + 0.002*\"people\" + 0.002*\"metaphysical\"\n",
      "2018-02-28 08:46:40,358 : INFO : topic #1 (0.333): 0.007*\"wikipedia\" + 0.007*\"male\" + 0.006*\"article\" + 0.005*\"female\" + 0.004*\"salute\" + 0.003*\"te\" + 0.003*\"male female\" + 0.003*\"subject\" + 0.003*\"like\" + 0.003*\"used\"\n",
      "2018-02-28 08:46:40,362 : INFO : topic #2 (0.333): 0.004*\"article\" + 0.003*\"copyright\" + 0.003*\"wikipedia\" + 0.003*\"image\" + 0.003*\"mexican\" + 0.002*\"page\" + 0.002*\"source\" + 0.002*\"articles\" + 0.002*\"thanks\" + 0.002*\"deletion\"\n",
      "2018-02-28 08:46:40,366 : INFO : topic diff=0.082341, rho=0.408248\n",
      "2018-02-28 08:46:40,895 : INFO : -8.662 per-word bound, 404.9 perplexity estimate based on a held-out corpus of 100 documents with 7004 words\n",
      "2018-02-28 08:46:40,898 : INFO : PROGRESS: pass 5, at document #100/100\n",
      "2018-02-28 08:46:41,266 : INFO : topic #0 (0.333): 0.004*\"page\" + 0.003*\"talk\" + 0.003*\"article\" + 0.003*\"like\" + 0.002*\"way\" + 0.002*\"don\" + 0.002*\"really\" + 0.002*\"think\" + 0.002*\"people\" + 0.002*\"metaphysical\"\n",
      "2018-02-28 08:46:41,274 : INFO : topic #1 (0.333): 0.007*\"wikipedia\" + 0.007*\"male\" + 0.006*\"article\" + 0.005*\"female\" + 0.004*\"salute\" + 0.003*\"te\" + 0.003*\"male female\" + 0.003*\"subject\" + 0.003*\"like\" + 0.003*\"used\"\n",
      "2018-02-28 08:46:41,306 : INFO : topic #2 (0.333): 0.004*\"article\" + 0.003*\"copyright\" + 0.003*\"wikipedia\" + 0.003*\"image\" + 0.003*\"mexican\" + 0.002*\"page\" + 0.002*\"source\" + 0.002*\"articles\" + 0.002*\"thanks\" + 0.002*\"deletion\"\n",
      "2018-02-28 08:46:41,330 : INFO : topic diff=0.048902, rho=0.377964\n",
      "2018-02-28 08:46:41,985 : INFO : -8.658 per-word bound, 403.8 perplexity estimate based on a held-out corpus of 100 documents with 7004 words\n",
      "2018-02-28 08:46:41,987 : INFO : PROGRESS: pass 6, at document #100/100\n",
      "2018-02-28 08:46:42,222 : INFO : topic #0 (0.333): 0.004*\"page\" + 0.003*\"talk\" + 0.003*\"article\" + 0.003*\"like\" + 0.002*\"way\" + 0.002*\"don\" + 0.002*\"really\" + 0.002*\"think\" + 0.002*\"people\" + 0.002*\"metaphysical\"\n",
      "2018-02-28 08:46:42,225 : INFO : topic #1 (0.333): 0.007*\"wikipedia\" + 0.007*\"male\" + 0.006*\"article\" + 0.005*\"female\" + 0.005*\"salute\" + 0.003*\"te\" + 0.003*\"male female\" + 0.003*\"subject\" + 0.003*\"like\" + 0.003*\"used\"\n",
      "2018-02-28 08:46:42,231 : INFO : topic #2 (0.333): 0.004*\"article\" + 0.003*\"copyright\" + 0.003*\"wikipedia\" + 0.003*\"image\" + 0.003*\"mexican\" + 0.002*\"page\" + 0.002*\"source\" + 0.002*\"articles\" + 0.002*\"thanks\" + 0.002*\"deletion\"\n",
      "2018-02-28 08:46:42,234 : INFO : topic diff=0.030020, rho=0.353553\n",
      "2018-02-28 08:46:42,857 : INFO : -8.656 per-word bound, 403.4 perplexity estimate based on a held-out corpus of 100 documents with 7004 words\n",
      "2018-02-28 08:46:42,860 : INFO : PROGRESS: pass 7, at document #100/100\n",
      "2018-02-28 08:46:43,128 : INFO : topic #0 (0.333): 0.004*\"page\" + 0.003*\"talk\" + 0.003*\"article\" + 0.003*\"like\" + 0.002*\"way\" + 0.002*\"don\" + 0.002*\"really\" + 0.002*\"think\" + 0.002*\"people\" + 0.002*\"metaphysical\"\n",
      "2018-02-28 08:46:43,130 : INFO : topic #1 (0.333): 0.007*\"wikipedia\" + 0.007*\"male\" + 0.006*\"article\" + 0.005*\"female\" + 0.005*\"salute\" + 0.003*\"te\" + 0.003*\"male female\" + 0.003*\"subject\" + 0.003*\"like\" + 0.003*\"used\"\n",
      "2018-02-28 08:46:43,134 : INFO : topic #2 (0.333): 0.004*\"article\" + 0.003*\"copyright\" + 0.003*\"wikipedia\" + 0.003*\"image\" + 0.003*\"mexican\" + 0.002*\"page\" + 0.002*\"source\" + 0.002*\"articles\" + 0.002*\"thanks\" + 0.002*\"deletion\"\n",
      "2018-02-28 08:46:43,139 : INFO : topic diff=0.019026, rho=0.333333\n",
      "2018-02-28 08:46:43,894 : INFO : -8.655 per-word bound, 403.1 perplexity estimate based on a held-out corpus of 100 documents with 7004 words\n",
      "2018-02-28 08:46:43,895 : INFO : PROGRESS: pass 8, at document #100/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-28 08:46:44,141 : INFO : topic #0 (0.333): 0.004*\"page\" + 0.003*\"talk\" + 0.003*\"article\" + 0.003*\"like\" + 0.002*\"way\" + 0.002*\"don\" + 0.002*\"really\" + 0.002*\"think\" + 0.002*\"people\" + 0.002*\"metaphysical\"\n",
      "2018-02-28 08:46:44,143 : INFO : topic #1 (0.333): 0.007*\"wikipedia\" + 0.007*\"male\" + 0.006*\"article\" + 0.005*\"female\" + 0.005*\"salute\" + 0.003*\"te\" + 0.003*\"male female\" + 0.003*\"subject\" + 0.003*\"like\" + 0.003*\"used\"\n",
      "2018-02-28 08:46:44,150 : INFO : topic #2 (0.333): 0.004*\"article\" + 0.003*\"copyright\" + 0.003*\"wikipedia\" + 0.003*\"image\" + 0.003*\"mexican\" + 0.002*\"page\" + 0.002*\"source\" + 0.002*\"articles\" + 0.002*\"thanks\" + 0.002*\"deletion\"\n",
      "2018-02-28 08:46:44,153 : INFO : topic diff=0.012342, rho=0.316228\n",
      "2018-02-28 08:46:45,028 : INFO : -8.655 per-word bound, 403.0 perplexity estimate based on a held-out corpus of 100 documents with 7004 words\n",
      "2018-02-28 08:46:45,033 : INFO : PROGRESS: pass 9, at document #100/100\n",
      "2018-02-28 08:46:45,454 : INFO : topic #0 (0.333): 0.004*\"page\" + 0.003*\"talk\" + 0.003*\"article\" + 0.003*\"like\" + 0.002*\"way\" + 0.002*\"don\" + 0.002*\"really\" + 0.002*\"think\" + 0.002*\"people\" + 0.002*\"metaphysical\"\n",
      "2018-02-28 08:46:45,458 : INFO : topic #1 (0.333): 0.007*\"wikipedia\" + 0.007*\"male\" + 0.006*\"article\" + 0.005*\"female\" + 0.005*\"salute\" + 0.003*\"te\" + 0.003*\"male female\" + 0.003*\"subject\" + 0.003*\"like\" + 0.003*\"used\"\n",
      "2018-02-28 08:46:45,466 : INFO : topic #2 (0.333): 0.004*\"article\" + 0.003*\"copyright\" + 0.003*\"wikipedia\" + 0.003*\"image\" + 0.003*\"mexican\" + 0.002*\"page\" + 0.002*\"source\" + 0.002*\"articles\" + 0.002*\"thanks\" + 0.002*\"use\"\n",
      "2018-02-28 08:46:45,472 : INFO : topic diff=0.008189, rho=0.301511\n"
     ]
    }
   ],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=3, id2word=id2word, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-28 08:46:58,771 : INFO : topic #0 (0.333): 0.004*\"page\" + 0.003*\"talk\" + 0.003*\"article\" + 0.003*\"like\" + 0.002*\"way\" + 0.002*\"don\" + 0.002*\"really\" + 0.002*\"think\" + 0.002*\"people\" + 0.002*\"metaphysical\"\n",
      "2018-02-28 08:46:58,778 : INFO : topic #1 (0.333): 0.007*\"wikipedia\" + 0.007*\"male\" + 0.006*\"article\" + 0.005*\"female\" + 0.005*\"salute\" + 0.003*\"te\" + 0.003*\"male female\" + 0.003*\"subject\" + 0.003*\"like\" + 0.003*\"used\"\n",
      "2018-02-28 08:46:58,781 : INFO : topic #2 (0.333): 0.004*\"article\" + 0.003*\"copyright\" + 0.003*\"wikipedia\" + 0.003*\"image\" + 0.003*\"mexican\" + 0.002*\"page\" + 0.002*\"source\" + 0.002*\"articles\" + 0.002*\"thanks\" + 0.002*\"use\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"page\" + 0.003*\"talk\" + 0.003*\"article\" + 0.003*\"like\" + 0.002*\"way\" + 0.002*\"don\" + 0.002*\"really\" + 0.002*\"think\" + 0.002*\"people\" + 0.002*\"metaphysical\"'),\n",
       " (1,\n",
       "  '0.007*\"wikipedia\" + 0.007*\"male\" + 0.006*\"article\" + 0.005*\"female\" + 0.005*\"salute\" + 0.003*\"te\" + 0.003*\"male female\" + 0.003*\"subject\" + 0.003*\"like\" + 0.003*\"used\"'),\n",
       " (2,\n",
       "  '0.004*\"article\" + 0.003*\"copyright\" + 0.003*\"wikipedia\" + 0.003*\"image\" + 0.003*\"mexican\" + 0.002*\"page\" + 0.002*\"source\" + 0.002*\"articles\" + 0.002*\"thanks\" + 0.002*\"use\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x1a479f6128>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "lda_corpus = lda[corpus]\n",
    "lda_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the documents' topic vectors in a list so we can take a peak\n",
    "lda_docs = [doc for doc in lda_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.99753726)],\n",
       " [(0, 0.042118639), (1, 0.042645715), (2, 0.91523564)],\n",
       " [(1, 0.98526078)],\n",
       " [(0, 0.056080092), (1, 0.88772446), (2, 0.056195457)],\n",
       " [(2, 0.98377347)],\n",
       " [(0, 0.06069915), (1, 0.057171375), (2, 0.88212943)],\n",
       " [(0, 0.94292706), (1, 0.028429797), (2, 0.028643137)],\n",
       " [(0, 0.021256423), (1, 0.02168618), (2, 0.95705742)],\n",
       " [(2, 0.98217463)],\n",
       " [(2, 0.99028659)],\n",
       " [(0, 0.028442981), (1, 0.028236082), (2, 0.94332093)],\n",
       " [(0, 0.011543071), (1, 0.011115024), (2, 0.97734195)],\n",
       " [(1, 0.99157745)],\n",
       " [(1, 0.98850095)],\n",
       " [(0, 0.9308973), (1, 0.035490468), (2, 0.033612248)]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the document vectors in the topic space for the first 5 documents\n",
    "lda_docs[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"== August 2009 == \\nSurv1v4l1st (Talk|Contribs) \"'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.comment_text[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method LdaModel.log_perplexity of <gensim.models.ldamodel.LdaModel object at 0x1a4e291588>>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.log_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
