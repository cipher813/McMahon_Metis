{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cipher000/anaconda3/envs/tensorflow1.4/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import (sent_tokenize, TreebankWordTokenizer, WhitespaceTokenizer)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB,MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report,confusion_matrix, precision_score, \n",
    "                             recall_score, f1_score, roc_curve, roc_auc_score, average_precision_score, \n",
    "                             precision_recall_curve, auc)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def sentence_tokenizer(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def whitespace_tokenizer(sentences):\n",
    "    listy = []\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    for i in list(range(0,len(sentences))):\n",
    "        tokenized = tokenizer.tokenize(sentences[i])\n",
    "        listy.append(tokenized)\n",
    "    return listy\n",
    "\n",
    "def polarity(sentences):\n",
    "    listy = []\n",
    "    for i in list(range(0,len(sentences))):\n",
    "        pol = TextBlob(sentences[i]).polarity\n",
    "        listy.append(pol)\n",
    "    return np.min(listy), np.max(listy), np.mean(listy),listy\n",
    "\n",
    "def stemmer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    listy = []\n",
    "    for word in TextBlob(text).words:\n",
    "        listy.append(stemmer.stem(word))\n",
    "    return listy\n",
    "\n",
    "def token_clean(text):\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
    "    text = text.lower().split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 24)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('../data/toxictrain.pkl')\n",
    "print(df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic                 1.529400e+04\n",
       "severe_toxic          1.595000e+03\n",
       "obscene               8.449000e+03\n",
       "threat                4.780000e+02\n",
       "insult                7.877000e+03\n",
       "identity_hate         1.405000e+03\n",
       "rating                3.509800e+04\n",
       "clean                 1.433460e+05\n",
       "polarity_comment      2.065111e+03\n",
       "polarity_comment_s    1.902005e+03\n",
       "word_count            1.055252e+07\n",
       "char_count            6.288266e+07\n",
       "char_count_s          5.053248e+07\n",
       "polarity_min         -1.595756e+04\n",
       "polarity_max          3.604874e+04\n",
       "polarity_mean         7.919310e+03\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sum(axis=0,numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate', 'category', 'rating', 'clean',\n",
       "       'comment_text_s', 'comment_text_f', 'token_clean', 'sent_token',\n",
       "       'polarity_sentence', 'polarity_comment', 'polarity_comment_s',\n",
       "       'word_count', 'char_count', 'char_count_s', 'polarity_min',\n",
       "       'polarity_max', 'polarity_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143346, 24) (15294, 24) (1595, 24) (8449, 24) (478, 24) (7877, 24) (1405, 24)\n",
      "(263346, 24)\n"
     ]
    }
   ],
   "source": [
    "df_Cl = df[df.rating == 0]\n",
    "df_To = df[df.toxic == 1]\n",
    "df_ST = df[df.severe_toxic == 1]\n",
    "df_Ob = df[df.obscene == 1]\n",
    "df_Th = df[df.threat == 1]\n",
    "df_In = df[df.insult == 1]\n",
    "df_IH = df[df.identity_hate == 1]\n",
    "print(df_Cl.shape, df_To.shape,df_ST.shape,df_Ob.shape,df_Th.shape,df_In.shape,df_IH.shape)\n",
    "\n",
    "df_STu = resample(df_ST, replace=True, n_samples=20000)\n",
    "df_Obu = resample(df_Ob, replace=True, n_samples=20000)\n",
    "df_Thu = resample(df_Th, replace=True, n_samples=30000)\n",
    "df_Inu = resample(df_In, replace=True, n_samples=20000)\n",
    "df_IHu = resample(df_IH, replace=True, n_samples=30000)\n",
    "\n",
    "df = pd.concat([df_Cl, df_STu, df_Obu, df_Thu, df_Inu, df_IHu])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic                 1.132030e+05\n",
      "severe_toxic          4.078700e+04\n",
      "obscene               9.551200e+04\n",
      "threat                3.492200e+04\n",
      "insult                9.566400e+04\n",
      "identity_hate         4.550400e+04\n",
      "rating                4.255920e+05\n",
      "clean                 1.433460e+05\n",
      "polarity_comment      2.745763e+03\n",
      "polarity_comment_s    2.569387e+03\n",
      "word_count            1.639040e+07\n",
      "char_count            9.702554e+07\n",
      "char_count_s          7.235674e+07\n",
      "polarity_min         -4.683376e+04\n",
      "polarity_max          3.811849e+04\n",
      "polarity_mean        -5.702754e+03\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "dfs = df.sum(axis=0,numeric_only=True)\n",
    "print(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = shuffle(df)[-50000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [['rfc', RandomForestClassifier(max_features='auto',n_estimators=1000)], \n",
    "                ['gbc', GradientBoostingClassifier(n_estimators=500)],\n",
    "                ['xgbc', XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.7,\n",
    "                       gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=6,\n",
    "                       min_child_weight=11, missing=-999, n_estimators=500, nthread=4,\n",
    "                       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
    "                       scale_pos_weight=1, seed=0, silent=1, subsample=0.8)]] \n",
    "\n",
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_models_results(data, models, categories):\n",
    "    roc = {}\n",
    "    results_dict = {}\n",
    "    \n",
    "    for model in models:\n",
    "        model_name = model[0]\n",
    "        model_model = model[1]\n",
    "        \n",
    "        for cat in categories:\n",
    "            X = data['comment_text_s']\n",
    "            y = data[cat]\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42, stratify=y)\n",
    "    \n",
    "            pipe = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
    "                             ('to_dense', DenseTransformer()), ('pipe', model_model)])   \n",
    "        \n",
    "            accuracy = []\n",
    "            precision_1 = []\n",
    "            precision_0 = []\n",
    "            recall_1 = []\n",
    "            recall_0 = []\n",
    "            f1_1 = []\n",
    "            f1_0 = []\n",
    "            auc = []\n",
    "\n",
    "            # Perform K-Fold CV and calculate metrics for each fold\n",
    "            kf = KFold(5, random_state=42, shuffle=True) \n",
    "            for train_idx, test_idx in kf.split(X, y=y):\n",
    "                pipe.fit(X_train, y_train)\n",
    "                y_pred = pipe.predict(X_test)\n",
    "                accuracy.append(accuracy_score(y_test, y_pred))\n",
    "                precision_1.append(precision_score(y_test, y_pred ,pos_label=1))\n",
    "                precision_0.append(precision_score(y_test, y_pred ,pos_label=0))\n",
    "                recall_1.append(recall_score(y_test, y_pred, pos_label=1))\n",
    "                recall_0.append(recall_score(y_test, y_pred, pos_label=0))\n",
    "                f1_1.append(f1_score(y_test, y_pred, pos_label=1))\n",
    "                f1_0.append(f1_score(y_test, y_pred, pos_label=0))\n",
    "                auc.append(roc_auc_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "            # Calculate mean metric across K-folds\n",
    "            mean_accuracy = np.mean(accuracy)\n",
    "            mean_precision_1 = np.mean(precision_1)\n",
    "            mean_precision_0 = np.mean(precision_0)\n",
    "            mean_recall_1 = np.mean(recall_1)\n",
    "            mean_recall_0 = np.mean(recall_0)\n",
    "            mean_f1_1 = np.mean(f1_1)\n",
    "            mean_f1_0 = np.mean(f1_0)\n",
    "            mean_auc = np.mean(auc)\n",
    "            cm = confusion_matrix(y_test,y_pred)\n",
    "            cr = classification_report(y_test,y_pred)\n",
    "\n",
    "            # Capture TPR and FPR from last fold for plotting\n",
    "            y_score = pipe.predict_proba(X_test)[:,1]\n",
    "            roc[(model_name,cat)] = roc_curve(y_test, y_score), mean_auc\n",
    "            results_dict[(model_name,cat)] = {\"accuracy\": mean_accuracy, \"precision_s\": mean_precision_1, \n",
    "                                                \"precision_f\": mean_precision_0, \"recall_s\": mean_recall_1, \n",
    "                                                \"recall_f\": mean_recall_0, \"f1_s\": mean_f1_1, \"f1_f\": mean_f1_0,\n",
    "                                                \"auc\": mean_auc} \n",
    "    return roc, results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amr = shuffle(df)[-10000:]\n",
    "roc, results_dict = all_models_results(df_amr, models, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve from the last K-Fold split\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot 50-50 Line\n",
    "ax.plot([0,1],[0,1], ls='--', color='k', label='50-50')\n",
    "\n",
    "# Plot Classifier ROC Curves\n",
    "for key, value in roc.items():\n",
    "    label = '{}, AUC: {}%'.format(key, round(100*value[1],1))\n",
    "    ax.plot(roc[key][0][0], roc[key][0][1], label=label)\n",
    "    \n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR')\n",
    "ax.set_title('ROC Curve - All Models',fontweight='bold',fontsize=15)\n",
    "ax.legend(loc='best')\n",
    "plt.savefig('../charts/toxic_roc_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = pd.DataFrame(results_dict).T\n",
    "rd = rd.apply(lambda x: round(100*x,1).astype(str) + \"%\")\n",
    "rd = rd.sort_values(['auc'],ascending=[False])\n",
    "rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(dataframe, model):\n",
    "    categories = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "    cat_dict = {}\n",
    "    \n",
    "    for cat in categories:\n",
    "        X = dataframe['comment_text_s']\n",
    "        y = dataframe[cat]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42, stratify=y)\n",
    "#         X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "\n",
    "        pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                             ('tfidf', TfidfTransformer()),\n",
    "                             ('to_dense', DenseTransformer()),\n",
    "                             ('pipe', model)])\n",
    "        pipe.fit(X_train,y_train.ravel()) # added .ravel() due to feature names mismatch\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision_1 = precision_score(y_test, y_pred ,pos_label=1)\n",
    "        precision_0 = precision_score(y_test, y_pred ,pos_label=0)\n",
    "        recall_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "        recall_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "        f1_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "        f1_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "        auc = roc_auc_score(y_test, y_pred)        \n",
    "        \n",
    "        mn = np.mean(y_pred == y_test)\n",
    "        cm = confusion_matrix(y_test,y_pred)\n",
    "        cr = classification_report(y_test,y_pred)\n",
    "        cat_dict[cat] = {\"accuracy\": accuracy, \"precision_1\": precision_1, \n",
    "                         \"precision_0\": precision_0, \"recall_1\": recall_1, \n",
    "                         \"recall_0\": recall_0, \"f1_1\": f1_1, \"f1_0\": f1_0, \n",
    "                         \"mean\":mn,\"auc\": auc,\"confusion_matrix\":cm,\"class_report\":cr} \n",
    "    \n",
    "    cat_dict = pd.DataFrame(cat_dict)\n",
    "    return cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tm = shuffle(df)[-20000:]\n",
    "xgb = test_model(df_tm, XGBClassifier())\n",
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf = shuffle(df)[-20000:]\n",
    "rf = test_model(df_rf, RandomForestClassifier(max_features='auto',n_estimators=1000))\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gb = shuffle(df)[-20000:]\n",
    "gb = test_model(df_gb, GradientBoostingClassifier(n_estimators=500))\n",
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
