{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import (sent_tokenize, TreebankWordTokenizer, WhitespaceTokenizer)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB,MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report,confusion_matrix, precision_score, \n",
    "                             recall_score, f1_score, roc_curve, roc_auc_score, average_precision_score, \n",
    "                             precision_recall_curve, auc)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def sentence_tokenizer(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def whitespace_tokenizer(sentences):\n",
    "    listy = []\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    for i in list(range(0,len(sentences))):\n",
    "        tokenized = tokenizer.tokenize(sentences[i])\n",
    "        listy.append(tokenized)\n",
    "    return listy\n",
    "\n",
    "def polarity(sentences):\n",
    "    listy = []\n",
    "    for i in list(range(0,len(sentences))):\n",
    "        pol = TextBlob(sentences[i]).polarity\n",
    "        listy.append(pol)\n",
    "    return np.min(listy), np.max(listy), np.mean(listy),listy\n",
    "\n",
    "def stemmer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    listy = []\n",
    "    for word in TextBlob(text).words:\n",
    "        listy.append(stemmer.stem(word))\n",
    "    return listy\n",
    "\n",
    "def token_clean(text):\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
    "    text = text.lower().split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/toxictrain.pkl')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sum(axis=0,numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_Cl = df[df.rating == 0]\n",
    "df_To = df[df.toxic == 1]\n",
    "df_ST = df[df.severe_toxic == 1]\n",
    "df_Ob = df[df.obscene == 1]\n",
    "df_Th = df[df.threat == 1]\n",
    "df_In = df[df.insult == 1]\n",
    "df_IH = df[df.identity_hate == 1]\n",
    "print(df_Cl.shape, df_To.shape,df_ST.shape,df_Ob.shape,df_Th.shape,df_In.shape,df_IH.shape)\n",
    "\n",
    "df_STu = resample(df_ST, replace=True, n_samples=30000)\n",
    "df_Obu = resample(df_Ob, replace=True, n_samples=20000)\n",
    "df_Thu = resample(df_Th, replace=True, n_samples=20000)\n",
    "df_Inu = resample(df_In, replace=True, n_samples=20000)\n",
    "df_IHu = resample(df_IH, replace=True, n_samples=30000)\n",
    "\n",
    "df = pd.concat([df_Cl, df_STu, df_Obu, df_Thu, df_Inu, df_IHu])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.sum(axis=0,numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_t.sum(axis=0,numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df1 = shuffle(df)[-100000:]\n",
    "# # df1 = df[['comment_text_s','toxic']]\n",
    "# X = df1['comment_text_s']\n",
    "# y = df1['toxic']\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count_vect = CountVectorizer()\n",
    "# X_cv = count_vect.fit_transform(df1.comment_text_s)\n",
    "# X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_cv, y, test_size=0.3,random_state=42, stratify=y)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42, stratify=y)\n",
    "\n",
    "# k_range = list(range(1, 101))\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = MultinomialNB().fit(X_train_tfidf, y)\n",
    "# text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('clf', MultinomialNB())])\n",
    "# text_clf.fit(X_train, y_train)\n",
    "# y_pred = text_clf.predict(X_test)\n",
    "\n",
    "# print(np.mean(y_pred == y_test))\n",
    "# print(confusion_matrix(y_test,y_pred))\n",
    "# print(classification_report(y_test,y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "#                                            alpha=1e-3, random_state=42,\n",
    "#                                            max_iter=5, tol=None))])\n",
    "# text_clf.fit(X_train,y_train) \n",
    "# y_pred = text_clf.predict(X_test)\n",
    "# print(np.mean(y_pred == y_test))\n",
    "# print(confusion_matrix(y_test,y_pred))\n",
    "# print(classification_report(y_test,y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pipe = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('pipe', RandomForestClassifier(max_features='auto', n_estimators=1000))])\n",
    "# pipe.fit(X_train,y_train) \n",
    "# y_pred = pipe.predict(X_test)\n",
    "# print(np.mean(y_pred == y_test))\n",
    "# print(confusion_matrix(y_test,y_pred))\n",
    "# print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pipe = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('pipe', GradientBoostingClassifier(n_estimators=500))])\n",
    "# pipe.fit(X_train,y_train) \n",
    "# y_pred = pipe.predict(X_test)\n",
    "# print(np.mean(y_pred == y_test))\n",
    "# print(confusion_matrix(y_test,y_pred))\n",
    "# print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(dataframe, model):\n",
    "    categories = ['clean','toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "    cat_dict = {}\n",
    "    \n",
    "    \n",
    "    for cat in categories:\n",
    "        df = dataframe[dataframe[cat]==1]\n",
    "        X = df['comment_text_s']\n",
    "        y = pd.cut(df.rating, bins=7, labels=list(range(7)))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42, stratify=y)\n",
    "        X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "\n",
    "        pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                             ('tfidf', TfidfTransformer()),\n",
    "                             ('to_dense', DenseTransformer()),\n",
    "                             ('pipe', model)])\n",
    "        pipe.fit(X_train,y_train.ravel()) # added .ravel() due to feature names mismatch\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision_1 = precision_score(y_test, y_pred ,pos_label=1, average=None)\n",
    "        precision_0 = precision_score(y_test, y_pred ,pos_label=0, average=None)\n",
    "        recall_1 = recall_score(y_test, y_pred, pos_label=1, average=None)\n",
    "        recall_0 = recall_score(y_test, y_pred, pos_label=0, average=None)\n",
    "        f1_1 = f1_score(y_test, y_pred, pos_label=1, average=None)\n",
    "        f1_0 = f1_score(y_test, y_pred, pos_label=0, average=None)\n",
    "#         auc = roc_auc_score(y_test, y_pred)        \n",
    "        \n",
    "        mn = np.mean(y_pred == y_test)\n",
    "        cm = confusion_matrix(y_test,y_pred)\n",
    "        cr = classification_report(y_test,y_pred)\n",
    "        cat_dict[cat] = {\"accuracy\": accuracy, \"precision_1\": precision_1, \n",
    "                         \"precision_0\": precision_0, \"recall_1\": recall_1, \n",
    "                         \"recall_0\": recall_0, \"f1_1\": f1_1, \"f1_0\": f1_0, \n",
    "                         \"mean\":mn,\"confusion_matrix\":cm,\"class_report\":cr} # \"auc\": auc,\n",
    "    \n",
    "    cat_dict = pd.DataFrame(cat_dict)\n",
    "#     cat_dict = cat_dict.copy()\n",
    "    cat_dict['average'] = cat_dict.mean(numeric_only=True, axis=1)\n",
    "    return cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tm = shuffle(df)[-5000:]\n",
    "xgb = test_model(df_tm, XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.7,\n",
    "                                       gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=6,\n",
    "                                       min_child_weight=11, missing=-999, n_estimators=500, nthread=4,\n",
    "                                       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
    "                                       scale_pos_weight=1, seed=0, silent=1, subsample=0.8))\n",
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xgb = xgb.copy()\n",
    "# xgb['average'] = xgb.mean(numeric_only=True, axis=1)\n",
    "# xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rf = shuffle(df)[-5000:]\n",
    "rf = test_model(df_rf, RandomForestClassifier(max_features='auto',n_estimators=1000))\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_gb = shuffle(df)[-5000:]\n",
    "gb = test_model(df_gb, GradientBoostingClassifier(n_estimators=500))\n",
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_nbm = shuffle(df)[-5000:]\n",
    "# nbm = test_model(df_nbm, MultinomialNB(alpha=0))\n",
    "# nbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_XGB = shuffle(df)[-5000:]\n",
    "\n",
    "# categories = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "# cat_XGB = {}\n",
    "# X = df_XGB['comment_text_s']\n",
    "\n",
    "# for cat in categories:\n",
    "#     y = df_XGB[cat]\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42, stratify=y)\n",
    "#     X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "\n",
    "#     pipe = Pipeline([('vect', CountVectorizer()),\n",
    "#                          ('tfidf', TfidfTransformer()),\n",
    "#                          ('to_dense', DenseTransformer()),\n",
    "#                          ('pipe', XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.7,\n",
    "#                                    gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=6,\n",
    "#                                    min_child_weight=11, missing=-999, n_estimators=500, nthread=4,\n",
    "#                                    objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
    "#                                    scale_pos_weight=1, seed=0, silent=1, subsample=0.8))])\n",
    "#     pipe.fit(X_train,y_train.ravel()) # added .ravel() due to feature names mismatch\n",
    "#     y_pred = pipe.predict(X_test) \n",
    "#     mn = np.mean(y_pred == y_test)\n",
    "#     cm = confusion_matrix(y_test,y_pred)\n",
    "#     cr = classification_report(y_test,y_pred)\n",
    "#     cat_XGB[cat] = mn, cm, cr\n",
    "# cat_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cat_XGB_df = pd.DataFrame(cat_XGB).T\n",
    "# cat_XGB_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pipe = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('pipe', SVC(C=10,gamma=0.001,probability=True))])\n",
    "# pipe.fit(X_train,y_train) \n",
    "# y_pred = pipe.predict(X_test)\n",
    "# print(np.mean(y_pred == y_test))\n",
    "# print(confusion_matrix(y_test,y_pred))\n",
    "# print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\t==============================\n",
      "\tAccuracy: 0.801266666667\n",
      "\tAUC: 0.934563523665\n",
      "\n",
      "\n",
      "[[1585    0    1   34    2    0    0]\n",
      " [  27    6    0    1    0    0    0]\n",
      " [  86    0   71   10   24    0    0]\n",
      " [ 141    0    6  100   91    1    0]\n",
      " [  87    0    3   24  430    7    0]\n",
      " [  10    0    0    3   32  186    0]\n",
      " [   5    0    0    0    1    0   26]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.98      0.89      1622\n",
      "          1       1.00      0.18      0.30        34\n",
      "          2       0.88      0.37      0.52       191\n",
      "          3       0.58      0.29      0.39       339\n",
      "          4       0.74      0.78      0.76       551\n",
      "          5       0.96      0.81      0.88       231\n",
      "          6       1.00      0.81      0.90        32\n",
      "\n",
      "avg / total       0.79      0.80      0.78      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# Define models to test\n",
    "model_list = [['RandomForest', RandomForestClassifier(max_features='auto',n_estimators=1000)], \n",
    "                ['GradientBoost', GradientBoostingClassifier(n_estimators=500)],\n",
    "                ['XGBoost', XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.7,\n",
    "                       gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=6,\n",
    "                       min_child_weight=11, missing=-999, n_estimators=500, nthread=4,\n",
    "                       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
    "                       scale_pos_weight=1, seed=0, silent=1, subsample=0.8)]] \n",
    "\n",
    "# Calculate metrics for each model\n",
    "roc = {}\n",
    "results_dict = {}\n",
    "for model in model_list:\n",
    "    \n",
    "    df_A = shuffle(df)[-10000:]\n",
    "    X = df_A['comment_text_s']\n",
    "    y = pd.cut(df_A.rating, bins=7, labels=list(range(7)))\n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     y = mlb.fit_transform(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42, stratify=y)\n",
    "    \n",
    "    model_name = model[0]\n",
    "    model = model[1]\n",
    "    pipe = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()), ('pipe', model)])\n",
    "\n",
    "    accuracy = []\n",
    "    precision_1 = []\n",
    "    precision_0 = []\n",
    "    recall_1 = []\n",
    "    recall_0 = []\n",
    "    f1_1 = []\n",
    "    f1_0 = []\n",
    "#     auc = []\n",
    "        \n",
    "    # Perform K-Fold CV and calculate metrics for each fold\n",
    "    kf = KFold(5, random_state=42, shuffle=True) \n",
    "    for train_idx, test_idx in kf.split(X, y=y):\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        precision_1.append(precision_score(y_test, y_pred ,pos_label=1, average=None))\n",
    "        precision_0.append(precision_score(y_test, y_pred ,pos_label=0, average=None))\n",
    "        recall_1.append(recall_score(y_test, y_pred, pos_label=1, average=None))\n",
    "        recall_0.append(recall_score(y_test, y_pred, pos_label=0, average=None))\n",
    "        f1_1.append(f1_score(y_test, y_pred, pos_label=1, average=None))\n",
    "        f1_0.append(f1_score(y_test, y_pred, pos_label=0, average=None))\n",
    "#         auc.append(roc_auc_score(y_test, y_pred, average='macro'))\n",
    "        \n",
    "    # Calculate mean metric across K-folds\n",
    "    mean_accuracy = np.mean(accuracy)\n",
    "    mean_precision_1 = np.mean(precision_1)\n",
    "    mean_precision_0 = np.mean(precision_0)\n",
    "    mean_recall_1 = np.mean(recall_1)\n",
    "    mean_recall_0 = np.mean(recall_0)\n",
    "    mean_f1_1 = np.mean(f1_1)\n",
    "    mean_f1_0 = np.mean(f1_0)\n",
    "#     mean_auc = np.mean(auc)\n",
    "    \n",
    "    # Capture TPR and FPR from last fold for plotting\n",
    "    y_score = pipe.predict_proba(X_test)[:,1]\n",
    "#     roc[model_name] = roc_curve(y_test, y_score), mean_auc\n",
    "    results_dict[model_name] = {\"accuracy\": mean_accuracy, \"precision_s\": mean_precision_1, \n",
    "                                \"precision_f\": mean_precision_0, \"recall_s\": mean_recall_1, \n",
    "                                \"recall_f\": mean_recall_0, \"f1_s\": mean_f1_1, \"f1_f\": mean_f1_0} # \"auc\": mean_auc\n",
    "    \n",
    "    # Print formatted results\n",
    "    print(model)\n",
    "    print('\\t==============================')\n",
    "    print('\\tAccuracy:', mean_accuracy)\n",
    "    print('\\tAUC:', mean_auc)\n",
    "    print('\\n')\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve from the last K-Fold split\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot 50-50 Line\n",
    "ax.plot([0,1],[0,1], ls='--', color='k', label='50-50')\n",
    "\n",
    "# Plot Classifier ROC Curves\n",
    "for key, value in roc.items():\n",
    "    label = '{}, AUC: {}%'.format(key, round(100*value[1],1))\n",
    "    ax.plot(roc[key][0][0], roc[key][0][1], label=label)\n",
    "    \n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR')\n",
    "ax.set_title('ROC Curve - All Models',fontweight='bold',fontsize=15)\n",
    "ax.legend(loc='best')\n",
    "plt.savefig('../charts/toxic_roc_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rd = pd.DataFrame(results_dict).T\n",
    "rd = rd.apply(lambda x: round(100*x,1).astype(str) + \"%\")\n",
    "rd = rd.sort_values(['auc'],ascending=[False])\n",
    "rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
