{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import utils\n",
    "\n",
    "checkpoint = \"../data/checkpoints/w2v_3allwords.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/toxictrain.pkl')\n",
    "# df = df[df['rating']>0]\n",
    "documents = [''.join(r) for r in df.comment_text]\n",
    "text = ''.join(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = re.sub('[^A-Za-z0-9 ]+', '',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'hope', 'your', 'retarded', 'kids', 'get', 'anal', 'raped', 'and', 'murdered', 'for', 'having', 'such', 'a', 'fag', 'as', 'a', 'father', 'im', 'gonna', 'fuck', 'your', 'fat', 'wife', 'and', 'her', 'over', 'the', 'bridge', 'consider']\n"
     ]
    }
   ],
   "source": [
    "words = utils.preprocess(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 9910700\n",
      "Unique words: 39064\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding = 200 # Number of embedding features \n",
    "with train_graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of negative labels to sample\n",
    "n_sampled = 100\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab))\n",
    "    \n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                      labels, embed,\n",
    "                                      n_sampled, n_vocab)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ## From Thushan Ganegedara's implementation\n",
    "    valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "# !mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Iteration: 100 Avg. Training loss: 5.6529 0.1076 sec/batch\n",
      "Epoch 1/5 Iteration: 200 Avg. Training loss: 5.6899 0.1149 sec/batch\n",
      "Epoch 1/5 Iteration: 300 Avg. Training loss: 5.7117 0.1189 sec/batch\n",
      "Epoch 1/5 Iteration: 400 Avg. Training loss: 5.6352 0.1183 sec/batch\n",
      "Epoch 1/5 Iteration: 500 Avg. Training loss: 5.6288 0.1180 sec/batch\n",
      "Epoch 1/5 Iteration: 600 Avg. Training loss: 5.5946 0.1217 sec/batch\n",
      "Epoch 1/5 Iteration: 700 Avg. Training loss: 5.5619 0.1186 sec/batch\n",
      "Epoch 1/5 Iteration: 800 Avg. Training loss: 5.4720 0.1181 sec/batch\n",
      "Epoch 1/5 Iteration: 900 Avg. Training loss: 5.3971 0.1184 sec/batch\n",
      "Epoch 1/5 Iteration: 1000 Avg. Training loss: 5.2972 0.1182 sec/batch\n",
      "Nearest to an: sahih, infoboxes, chalukya, thatll, nearest, deleeted, bran, countrythe,\n",
      "Nearest to your: place, were, to, demand, cierekim, mistake, townshend, adviser,\n",
      "Nearest to then: way, potential, cares, himher, hear, uncertainties, diode, changed,\n",
      "Nearest to them: steam, nomenclature, freds, dances, codes, if, topics, elegant,\n",
      "Nearest to there: picture, talk, stringent, jadoon, widespread, boner, gaza, mortar,\n",
      "Nearest to people: included, novelists, determine, comfortable, active, mad, interruptions, original,\n",
      "Nearest to does: the, redirect, loosely, have, gregalton, chickens, arranged, community,\n",
      "Nearest to if: produce, you, revert, them, forgery, wwes, think, socket,\n",
      "Nearest to popular: oaks, ynet, unclear, gentile, clients, deduction, tagging, flame,\n",
      "Nearest to dead: perverse, reasonsyou, pashtun, lightly, childishness, practicality, forcible, accustomed,\n",
      "Nearest to dog: vikings, crazier, rdns, trig, voltaire, sexual, broadcasted, vary,\n",
      "Nearest to shown: somewhat, retake, write, bjaodn, gilliam, unpunished, 41, define,\n",
      "Nearest to album: id, rejoin, downloaded, ideally, missouri, rd232, reasoning, existing,\n",
      "Nearest to club: depression, observations, no, coding, chose, misrata, currency, proves,\n",
      "Nearest to body: kennel, silva, approximate, toro, yammer, wikiality, tracker, frame,\n",
      "Nearest to former: conspirators, nonesense, usertancred, excise, gratitude, killings, phaedriel, zhang,\n",
      "Epoch 1/5 Iteration: 1100 Avg. Training loss: 5.1914 0.1192 sec/batch\n",
      "Epoch 1/5 Iteration: 1200 Avg. Training loss: 5.0813 0.1175 sec/batch\n",
      "Epoch 1/5 Iteration: 1300 Avg. Training loss: 5.0278 0.1181 sec/batch\n",
      "Epoch 1/5 Iteration: 1400 Avg. Training loss: 4.9738 0.1180 sec/batch\n",
      "Epoch 1/5 Iteration: 1500 Avg. Training loss: 4.9238 0.1174 sec/batch\n",
      "Epoch 1/5 Iteration: 1600 Avg. Training loss: 4.8630 0.1202 sec/batch\n",
      "Epoch 1/5 Iteration: 1700 Avg. Training loss: 4.8423 0.1193 sec/batch\n",
      "Epoch 1/5 Iteration: 1800 Avg. Training loss: 4.8436 0.1182 sec/batch\n",
      "Epoch 1/5 Iteration: 1900 Avg. Training loss: 4.7859 0.1178 sec/batch\n",
      "Epoch 1/5 Iteration: 2000 Avg. Training loss: 4.7902 0.1176 sec/batch\n",
      "Nearest to an: began, thatll, deleeted, kings, sahih, chalukya, bran, countrythe,\n",
      "Nearest to your: place, cierekim, wikipediatutorialhow, mistake, demand, carolina, himthe, were,\n",
      "Nearest to then: himher, unsure, hear, cares, overthetop, potential, nose, clearcut,\n",
      "Nearest to them: nomenclature, steam, december, codes, freds, dances, lighting, topics,\n",
      "Nearest to there: picture, unsigned, stringent, theyll, talk, widespread, handling, boner,\n",
      "Nearest to people: novelists, included, comfortable, interruptions, english, active, utcthank, determine,\n",
      "Nearest to does: redirect, priority, loosely, unknowable, gregalton, 1641, the, bombarding,\n",
      "Nearest to if: produce, forgery, improvements, revert, overcome, resume, vandalize, achieving,\n",
      "Nearest to popular: ynet, unclear, oaks, gentile, clients, cultural, deduction, tagging,\n",
      "Nearest to dead: perverse, lightly, reasonsyou, pashtun, practicality, accustomed, troubled, pronouns,\n",
      "Nearest to dog: vikings, promotional, crazier, trig, sexual, voltaire, vary, rdns,\n",
      "Nearest to shown: somewhat, retake, define, laptop, 41, write, unpunished, regards,\n",
      "Nearest to album: id, rejoin, ideally, affect, downloaded, missouri, news, honoured,\n",
      "Nearest to club: chose, observations, depression, proves, requesting, importance, descriptive, coding,\n",
      "Nearest to body: kennel, approximate, silva, toro, frame, adult, constructively, plea,\n",
      "Nearest to former: conspirators, nonesense, killings, excise, gratitude, fair, zhang, proserb,\n",
      "Epoch 1/5 Iteration: 2100 Avg. Training loss: 4.7933 0.1197 sec/batch\n",
      "Epoch 2/5 Iteration: 2200 Avg. Training loss: 4.9703 0.0678 sec/batch\n",
      "Epoch 2/5 Iteration: 2300 Avg. Training loss: 4.7430 0.1132 sec/batch\n",
      "Epoch 2/5 Iteration: 2400 Avg. Training loss: 4.7070 0.1169 sec/batch\n",
      "Epoch 2/5 Iteration: 2500 Avg. Training loss: 4.6772 0.1188 sec/batch\n",
      "Epoch 2/5 Iteration: 2600 Avg. Training loss: 4.6562 0.1182 sec/batch\n",
      "Epoch 2/5 Iteration: 2700 Avg. Training loss: 4.6531 0.1228 sec/batch\n",
      "Epoch 2/5 Iteration: 2800 Avg. Training loss: 4.6371 0.1176 sec/batch\n",
      "Epoch 2/5 Iteration: 2900 Avg. Training loss: 4.6632 0.1188 sec/batch\n",
      "Epoch 2/5 Iteration: 3000 Avg. Training loss: 4.6200 0.1186 sec/batch\n",
      "Nearest to an: thatll, deleeted, began, bran, sahih, countrythe, where, kings,\n",
      "Nearest to your: place, cierekim, wikipediatutorialhow, himthe, produce, demand, mistake, carolina,\n",
      "Nearest to then: sukecchi, clearcut, ithe, overthetop, unsure, nose, uncertainties, 1958,\n",
      "Nearest to them: nomenclature, steam, dances, misunderstandingi, freds, december, lighting, codes,\n",
      "Nearest to there: stringent, theyll, unsigned, picture, jadoon, handling, widespread, boner,\n",
      "Nearest to people: novelists, comfortable, included, interruptions, pisses, mad, utcthank, nhl,\n",
      "Nearest to does: redirect, gregalton, unknowable, priority, loosely, pie, 1641, bombarding,\n",
      "Nearest to if: produce, you, overcome, forgery, 1919, monopoly, wwes, revert,\n",
      "Nearest to popular: ynet, oaks, unclear, gentile, consortium, deduction, clients, cultural,\n",
      "Nearest to dead: perverse, pashtun, practicality, accustomed, reasonsyou, pronouns, homophobic, moores,\n",
      "Nearest to dog: vikings, sexual, deployment, voltaire, trig, rdns, vary, informally,\n",
      "Nearest to shown: retake, somewhat, bjaodn, gilliam, laptop, define, 41, f16,\n",
      "Nearest to album: id, rejoin, ideally, affect, downloaded, missouri, honoured, rd232,\n",
      "Nearest to club: requesting, importance, indicate, nobility, misrata, observations, proves, spiff,\n",
      "Nearest to body: kennel, approximate, silva, frame, toro, wikiality, adult, tracker,\n",
      "Nearest to former: conspirators, nonesense, killings, gratitude, excise, chickens, usertancred, zhang,\n",
      "Epoch 2/5 Iteration: 3100 Avg. Training loss: 4.6200 0.1198 sec/batch\n",
      "Epoch 2/5 Iteration: 3200 Avg. Training loss: 4.5922 0.1189 sec/batch\n",
      "Epoch 2/5 Iteration: 3300 Avg. Training loss: 4.6064 0.1187 sec/batch\n",
      "Epoch 2/5 Iteration: 3400 Avg. Training loss: 4.5761 0.1183 sec/batch\n",
      "Epoch 2/5 Iteration: 3500 Avg. Training loss: 4.5944 0.1223 sec/batch\n",
      "Epoch 2/5 Iteration: 3600 Avg. Training loss: 4.5778 0.1183 sec/batch\n",
      "Epoch 2/5 Iteration: 3700 Avg. Training loss: 4.5899 0.1226 sec/batch\n",
      "Epoch 2/5 Iteration: 3800 Avg. Training loss: 4.5839 0.1204 sec/batch\n",
      "Epoch 2/5 Iteration: 3900 Avg. Training loss: 4.5873 0.1189 sec/batch\n",
      "Epoch 2/5 Iteration: 4000 Avg. Training loss: 4.6092 0.1184 sec/batch\n",
      "Nearest to an: bran, deleeted, stiff, requesting, countrythe, where, thanksthis, sahih,\n",
      "Nearest to your: place, himthe, cierekim, wikipediatutorialhow, fontsize85, mistake, produce, to,\n",
      "Nearest to then: sukecchi, logged, overthetop, unsure, ithe, clearcut, uncertainties, nose,\n",
      "Nearest to them: nomenclature, steam, dances, misunderstandingi, assassinations, somaliland, freds, december,\n",
      "Nearest to there: stringent, theyll, picture, unsigned, jadoon, handling, boner, dislikes,\n",
      "Nearest to people: novelists, included, pisses, comfortable, interruptions, nhl, gutenberg, mad,\n",
      "Nearest to does: gregalton, redirect, pie, priority, unknowable, scrutiny, bombarding, loosely,\n",
      "Nearest to if: you, produce, 1919, overcome, incoherent, vandalize, forgery, enjoy,\n",
      "Nearest to popular: ynet, oaks, gentile, unclear, cultural, deduction, consortium, clients,\n",
      "Nearest to dead: perverse, pashtun, practicality, reasonsyou, childishness, crewer, pronouns, accustomed,\n",
      "Nearest to dog: vikings, deployment, trig, crazier, sexual, voltaire, rdns, informally,\n",
      "Nearest to shown: retake, bjaodn, 41, somewhat, laptop, gilliam, f16, write,\n",
      "Nearest to album: id, rejoin, ideally, affect, honoured, rd232, missouri, downloaded,\n",
      "Nearest to club: requesting, indicate, importance, misrata, cyberstalking, nobility, spiff, guidelinesfor,\n",
      "Nearest to body: kennel, silva, approximate, toro, wikiality, foreignlanguage, eichenwalds, adult,\n",
      "Nearest to former: conspirators, nonesense, killings, excise, chickens, usertancred, proserb, zhang,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 Iteration: 4100 Avg. Training loss: 4.5721 0.1202 sec/batch\n",
      "Epoch 2/5 Iteration: 4200 Avg. Training loss: 4.5876 0.1191 sec/batch\n",
      "Epoch 3/5 Iteration: 4300 Avg. Training loss: 4.5774 0.0320 sec/batch\n",
      "Epoch 3/5 Iteration: 4400 Avg. Training loss: 4.5273 0.1111 sec/batch\n",
      "Epoch 3/5 Iteration: 4500 Avg. Training loss: 4.4829 0.1164 sec/batch\n",
      "Epoch 3/5 Iteration: 4600 Avg. Training loss: 4.4995 0.1184 sec/batch\n",
      "Epoch 3/5 Iteration: 4700 Avg. Training loss: 4.5307 0.1237 sec/batch\n",
      "Epoch 3/5 Iteration: 4800 Avg. Training loss: 4.4802 0.1190 sec/batch\n",
      "Epoch 3/5 Iteration: 4900 Avg. Training loss: 4.5033 0.1185 sec/batch\n",
      "Epoch 3/5 Iteration: 5000 Avg. Training loss: 4.5016 0.1185 sec/batch\n",
      "Nearest to an: bran, where, stiff, thanksthis, deleeted, requesting, enwikipedia, gordon,\n",
      "Nearest to your: place, fontsize85, himthe, wikipediatutorialhow, cierekim, produce, to, mistake,\n",
      "Nearest to then: overthetop, sukecchi, logged, unsure, ithe, pagestutorialarticle, clearcut, flair,\n",
      "Nearest to them: dances, misunderstandingi, steam, nomenclature, assassinations, somaliland, lighting, autoblocks,\n",
      "Nearest to there: theyll, stringent, picture, jadoon, widespread, unsigned, dislikes, inferences,\n",
      "Nearest to people: novelists, included, pisses, interruptions, nhl, comfortable, industries, gutenberg,\n",
      "Nearest to does: gregalton, redirect, priority, unknowable, pie, scrutiny, bombarding, 1641,\n",
      "Nearest to if: you, produce, vandalize, incoherent, revert, wwes, pike, this,\n",
      "Nearest to popular: ynet, oaks, gentile, unclear, cultural, clients, consortium, sofa,\n",
      "Nearest to dead: perverse, practicality, pashtun, childishness, reasonsyou, pronouns, straws, crewer,\n",
      "Nearest to dog: vikings, rdns, voltaire, deployment, crazier, trig, sexual, upshot,\n",
      "Nearest to shown: retake, bjaodn, 41, defintion, f16, somewhat, write, laptop,\n",
      "Nearest to album: id, rejoin, ideally, affect, honoured, nymex, missouri, downloaded,\n",
      "Nearest to club: requesting, indicate, importance, misrata, cyberstalking, guidelinesfor, nobility, spiff,\n",
      "Nearest to body: kennel, silva, approximate, wikiality, yammer, toro, adult, eichenwalds,\n",
      "Nearest to former: nonesense, conspirators, killings, usertancred, excise, zhang, chickens, gratitude,\n",
      "Epoch 3/5 Iteration: 5100 Avg. Training loss: 4.4710 0.1194 sec/batch\n",
      "Epoch 3/5 Iteration: 5200 Avg. Training loss: 4.5028 0.1193 sec/batch\n",
      "Epoch 3/5 Iteration: 5300 Avg. Training loss: 4.4849 0.1182 sec/batch\n",
      "Epoch 3/5 Iteration: 5400 Avg. Training loss: 4.5062 0.1181 sec/batch\n",
      "Epoch 3/5 Iteration: 5500 Avg. Training loss: 4.4735 0.1181 sec/batch\n",
      "Epoch 3/5 Iteration: 5600 Avg. Training loss: 4.4922 0.1181 sec/batch\n",
      "Epoch 3/5 Iteration: 5700 Avg. Training loss: 4.4860 0.1219 sec/batch\n",
      "Epoch 3/5 Iteration: 5800 Avg. Training loss: 4.4678 0.1182 sec/batch\n",
      "Epoch 3/5 Iteration: 5900 Avg. Training loss: 4.4733 0.1171 sec/batch\n",
      "Epoch 3/5 Iteration: 6000 Avg. Training loss: 4.4687 0.1182 sec/batch\n",
      "Nearest to an: bran, thanksthis, where, requesting, deleeted, stiff, enwikipedia, gordon,\n",
      "Nearest to your: fontsize85, place, wikipediatutorialhow, cierekim, mistake, himthe, produce, to,\n",
      "Nearest to then: logged, sukecchi, overthetop, unsure, files, specify, pagestutorialarticle, himher,\n",
      "Nearest to them: dances, misunderstandingi, steam, nomenclature, assassinations, somaliland, aga, thirty,\n",
      "Nearest to there: stringent, theyll, jadoon, picture, assholery, inferences, dislikes, minority,\n",
      "Nearest to people: novelists, included, interruptions, industries, nhl, pisses, comfortable, active,\n",
      "Nearest to does: gregalton, priority, unknowable, pie, scrutiny, redirect, bombarding, 1641,\n",
      "Nearest to if: you, revert, produce, vandalize, this, incoherent, enjoy, thank,\n",
      "Nearest to popular: ynet, oaks, gentile, unclear, cultural, clients, consortium, sofa,\n",
      "Nearest to dead: practicality, perverse, childishness, reasonsyou, crewer, pashtun, pronouns, straws,\n",
      "Nearest to dog: vikings, rdns, deployment, voltaire, trig, sexual, crazier, upshot,\n",
      "Nearest to shown: retake, bjaodn, laptop, 41, f16, penthouse, ketchup, write,\n",
      "Nearest to album: id, rejoin, ideally, honoured, missouri, affect, nymex, rd232,\n",
      "Nearest to club: requesting, indicate, importance, misrata, cyberstalking, generally, guidelinesfor, spiff,\n",
      "Nearest to body: silva, kennel, approximate, wikiality, yammer, toro, eichenwalds, adult,\n",
      "Nearest to former: nonesense, conspirators, killings, usertancred, zhang, chickens, excise, proserb,\n",
      "Epoch 3/5 Iteration: 6100 Avg. Training loss: 4.4786 0.1197 sec/batch\n",
      "Epoch 3/5 Iteration: 6200 Avg. Training loss: 4.4761 0.1186 sec/batch\n",
      "Epoch 3/5 Iteration: 6300 Avg. Training loss: 4.4688 0.1182 sec/batch\n",
      "Epoch 3/5 Iteration: 6400 Avg. Training loss: 4.4497 0.1186 sec/batch\n",
      "Epoch 4/5 Iteration: 6500 Avg. Training loss: 4.2585 0.1014 sec/batch\n",
      "Epoch 4/5 Iteration: 6600 Avg. Training loss: 4.3557 0.1144 sec/batch\n",
      "Epoch 4/5 Iteration: 6700 Avg. Training loss: 4.4367 0.1227 sec/batch\n",
      "Epoch 4/5 Iteration: 6800 Avg. Training loss: 4.4101 0.1183 sec/batch\n",
      "Epoch 4/5 Iteration: 6900 Avg. Training loss: 4.3877 0.1183 sec/batch\n",
      "Epoch 4/5 Iteration: 7000 Avg. Training loss: 4.4058 0.1184 sec/batch\n",
      "Nearest to an: bran, where, enwikipedia, gordon, requesting, stiff, postpone, thanksthis,\n",
      "Nearest to your: fontsize85, place, wikipediatutorialhow, mistake, cierekim, to, produce, himthe,\n",
      "Nearest to then: logged, sukecchi, files, specify, overthetop, unsure, pagestutorialarticle, tagsfair,\n",
      "Nearest to them: dances, misunderstandingi, steam, aga, if, assassinations, wikipediafair, somaliland,\n",
      "Nearest to there: stringent, theyll, dislikes, jadoon, picture, so, minority, chillum,\n",
      "Nearest to people: novelists, included, pisses, interruptions, active, comfortable, nhl, ancient,\n",
      "Nearest to does: priority, scrutiny, unknowable, gregalton, pie, bombarding, redirect, insufficient,\n",
      "Nearest to if: you, this, vandalize, incoherent, revert, produce, helpfulintroductionthe, them,\n",
      "Nearest to popular: ynet, oaks, gentile, clients, sofa, unclear, mangalore, cultural,\n",
      "Nearest to dead: practicality, crewer, straws, childishness, pronouns, perverse, authenticate, reasonsyou,\n",
      "Nearest to dog: vikings, rdns, voltaire, deployment, sexual, crazier, trig, soccer,\n",
      "Nearest to shown: retake, bjaodn, f16, write, penthouse, 41, defintion, laptop,\n",
      "Nearest to album: id, rejoin, ideally, missouri, affect, nymex, hishertheir, honoured,\n",
      "Nearest to club: requesting, indicate, importance, placed, subject, guidelinesfor, generally, misrata,\n",
      "Nearest to body: kennel, silva, approximate, wikiality, yammer, toro, adult, eichenwalds,\n",
      "Nearest to former: nonesense, conspirators, killings, usertancred, zhang, actully, seattle, proserb,\n",
      "Epoch 4/5 Iteration: 7100 Avg. Training loss: 4.3888 0.1202 sec/batch\n",
      "Epoch 4/5 Iteration: 7200 Avg. Training loss: 4.4248 0.1194 sec/batch\n",
      "Epoch 4/5 Iteration: 7300 Avg. Training loss: 4.4041 0.1180 sec/batch\n",
      "Epoch 4/5 Iteration: 7400 Avg. Training loss: 4.4031 0.1195 sec/batch\n",
      "Epoch 4/5 Iteration: 7500 Avg. Training loss: 4.4207 0.1187 sec/batch\n",
      "Epoch 4/5 Iteration: 7600 Avg. Training loss: 4.4088 0.1188 sec/batch\n",
      "Epoch 4/5 Iteration: 7700 Avg. Training loss: 4.3721 0.1211 sec/batch\n",
      "Epoch 4/5 Iteration: 7800 Avg. Training loss: 4.4065 0.1196 sec/batch\n",
      "Epoch 4/5 Iteration: 7900 Avg. Training loss: 4.4169 0.1185 sec/batch\n",
      "Epoch 4/5 Iteration: 8000 Avg. Training loss: 4.3953 0.1180 sec/batch\n",
      "Nearest to an: enwikipedia, bran, thanksthis, requesting, stiff, gordon, themes, postpone,\n",
      "Nearest to your: place, fontsize85, mistake, to, wikipediatutorialhow, cierekim, produce, questionshere,\n",
      "Nearest to then: logged, specify, files, tagsfair, sukecchi, unsure, overthetop, pagestutorialarticle,\n",
      "Nearest to them: dances, misunderstandingi, aga, steam, thirty, wikipediafair, assassinations, assistance,\n",
      "Nearest to there: stringent, theyll, minority, so, jadoon, inferences, dislikes, chillum,\n",
      "Nearest to people: included, novelists, active, industries, interruptions, pisses, comfortable, nhl,\n",
      "Nearest to does: unknowable, gregalton, priority, scrutiny, pie, bombarding, deleter, 1641,\n",
      "Nearest to if: you, this, vandalize, includingthe, produce, helpfulintroductionthe, revert, incoherent,\n",
      "Nearest to popular: ynet, gentile, oaks, clients, sofa, mangalore, victors, remount,\n",
      "Nearest to dead: practicality, crewer, childishness, pronouns, straws, authenticate, reasonsyou, avril,\n",
      "Nearest to dog: vikings, rdns, voltaire, deployment, sexual, trig, crazier, falsified,\n",
      "Nearest to shown: retake, bjaodn, 41, f16, laptop, ketchup, penthouse, write,\n",
      "Nearest to album: id, rejoin, ideally, hishertheir, missouri, affect, nymex, discography,\n",
      "Nearest to club: requesting, indicate, importance, notable, subject, speedily, significance, included,\n",
      "Nearest to body: silva, kennel, approximate, wikiality, yammer, toro, eichenwalds, adult,\n",
      "Nearest to former: nonesense, conspirators, killings, usertancred, seattle, zhang, actully, 1930,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 Iteration: 8100 Avg. Training loss: 4.3862 0.1200 sec/batch\n",
      "Epoch 4/5 Iteration: 8200 Avg. Training loss: 4.4305 0.1183 sec/batch\n",
      "Epoch 4/5 Iteration: 8300 Avg. Training loss: 4.3990 0.1187 sec/batch\n",
      "Epoch 4/5 Iteration: 8400 Avg. Training loss: 4.4044 0.1188 sec/batch\n",
      "Epoch 4/5 Iteration: 8500 Avg. Training loss: 4.3880 0.1186 sec/batch\n",
      "Epoch 5/5 Iteration: 8600 Avg. Training loss: 4.1293 0.0626 sec/batch\n",
      "Epoch 5/5 Iteration: 8700 Avg. Training loss: 4.2099 0.1153 sec/batch\n",
      "Epoch 5/5 Iteration: 8800 Avg. Training loss: 4.3053 0.1191 sec/batch\n",
      "Epoch 5/5 Iteration: 8900 Avg. Training loss: 4.3504 0.1189 sec/batch\n",
      "Epoch 5/5 Iteration: 9000 Avg. Training loss: 4.3320 0.1186 sec/batch\n",
      "Nearest to an: enwikipedia, gordon, requesting, bran, stiff, postpone, not, where,\n",
      "Nearest to your: fontsize85, place, to, mistake, cierekim, you, wikipediatutorialhow, questionshere,\n",
      "Nearest to then: logged, specify, files, overthetop, tagsfair, sukecchi, mind, pagestutorialarticle,\n",
      "Nearest to them: dances, misunderstandingi, aga, steam, thirty, assistance, if, outpost,\n",
      "Nearest to there: dislikes, so, stringent, minority, colby, theyll, picture, widespread,\n",
      "Nearest to people: included, novelists, ancient, interruptions, pisses, active, industries, lgagnon,\n",
      "Nearest to does: scrutiny, unknowable, priority, gregalton, pie, loosely, bombarding, have,\n",
      "Nearest to if: you, this, vandalize, the, incoherent, revert, for, on,\n",
      "Nearest to popular: ynet, gentile, oaks, clients, mangalore, sofa, victors, remount,\n",
      "Nearest to dead: crewer, practicality, childishness, pronouns, straws, authenticate, avril, reasonsyou,\n",
      "Nearest to dog: vikings, rdns, sexual, soccer, deployment, falsified, upshot, voltaire,\n",
      "Nearest to shown: retake, bjaodn, f16, 41, defintion, aim, laptop, penthouse,\n",
      "Nearest to album: id, rejoin, discography, ideally, affect, hishertheir, missouri, nymex,\n",
      "Nearest to club: requesting, indicate, importance, subject, spiff, notable, assert, misrata,\n",
      "Nearest to body: kennel, silva, approximate, yammer, wikiality, toro, eichenwalds, applying,\n",
      "Nearest to former: nonesense, conspirators, killings, usertancred, seattle, kobres, kingdom, takrur,\n",
      "Epoch 5/5 Iteration: 9100 Avg. Training loss: 4.3320 0.1196 sec/batch\n",
      "Epoch 5/5 Iteration: 9200 Avg. Training loss: 4.3585 0.1222 sec/batch\n",
      "Epoch 5/5 Iteration: 9300 Avg. Training loss: 4.3355 0.1200 sec/batch\n",
      "Epoch 5/5 Iteration: 9400 Avg. Training loss: 4.3397 0.1245 sec/batch\n",
      "Epoch 5/5 Iteration: 9500 Avg. Training loss: 4.3625 0.1337 sec/batch\n",
      "Epoch 5/5 Iteration: 9600 Avg. Training loss: 4.3510 0.1271 sec/batch\n",
      "Epoch 5/5 Iteration: 9700 Avg. Training loss: 4.3429 0.1319 sec/batch\n",
      "Epoch 5/5 Iteration: 9800 Avg. Training loss: 4.3000 0.1209 sec/batch\n",
      "Epoch 5/5 Iteration: 9900 Avg. Training loss: 4.3448 0.1171 sec/batch\n",
      "Epoch 5/5 Iteration: 10000 Avg. Training loss: 4.3301 0.1175 sec/batch\n",
      "Nearest to an: enwikipedia, requesting, bran, group, not, stiff, gordon, postpone,\n",
      "Nearest to your: to, place, cierekim, fontsize85, mistake, questionshere, wikipediatutorialhow, produce,\n",
      "Nearest to then: logged, specify, files, tagsfair, terms, mind, overthetop, located,\n",
      "Nearest to them: dances, misunderstandingi, aga, steam, thirty, if, assistance, outpost,\n",
      "Nearest to there: so, dislikes, stringent, colby, minority, theyll, newer, ul,\n",
      "Nearest to people: included, novelists, interruptions, ancient, active, lgagnon, group, industries,\n",
      "Nearest to does: scrutiny, unknowable, gregalton, pie, particularyou, bombarding, priority, 1641,\n",
      "Nearest to if: you, this, the, helpfulintroductionthe, on, vandalize, for, revert,\n",
      "Nearest to popular: ynet, gentile, oaks, clients, victors, mangalore, unclear, closedminded,\n",
      "Nearest to dead: practicality, crewer, childishness, straws, pronouns, authenticate, avril, actioni,\n",
      "Nearest to dog: vikings, rdns, sexual, soccer, upshot, deployment, trig, falsified,\n",
      "Nearest to shown: retake, bjaodn, 41, f16, ketchup, bdsm, egalitarianism, defintion,\n",
      "Nearest to album: id, discography, rejoin, ideally, hishertheir, songs, singles, missouri,\n",
      "Nearest to club: indicate, requesting, importance, subject, notable, assert, fc, why,\n",
      "Nearest to body: kennel, silva, approximate, eichenwalds, yammer, wikiality, applying, toro,\n",
      "Nearest to former: nonesense, conspirators, usertancred, killings, seattle, kobres, kingdom, ic,\n",
      "Epoch 5/5 Iteration: 10100 Avg. Training loss: 4.3487 0.1188 sec/batch\n",
      "Epoch 5/5 Iteration: 10200 Avg. Training loss: 4.2957 0.1173 sec/batch\n",
      "Epoch 5/5 Iteration: 10300 Avg. Training loss: 4.3274 0.1180 sec/batch\n",
      "Epoch 5/5 Iteration: 10400 Avg. Training loss: 4.3387 0.1179 sec/batch\n",
      "Epoch 5/5 Iteration: 10500 Avg. Training loss: 4.3122 0.1172 sec/batch\n",
      "Epoch 5/5 Iteration: 10600 Avg. Training loss: 4.3082 0.1175 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 1000\n",
    "window_size = 10\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            if iteration % 1000 == 0:\n",
    "                # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = int_to_vocab[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = int_to_vocab[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, checkpoint)\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't load save_path when it is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-70aa2f05ad76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_graph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0membed_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.4/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1660\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1662\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1663\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't load save_path when it is None."
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(checkpoint))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_words = 500\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "ax.axis([-4.0,4.0, -4.0,4.0])\n",
    "ax = fig.gca()\n",
    "ax.set_autoscale_on(False)\n",
    "\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
