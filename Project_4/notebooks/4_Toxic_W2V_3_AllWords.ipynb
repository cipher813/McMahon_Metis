{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianmcmahon/anaconda3/envs/tensorflow1.4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import utils\n",
    "\n",
    "checkpoint = \"checkpoints/w2v_3allwords.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/toxictrain.pkl')\n",
    "# df = df[df['rating']>0]\n",
    "documents = [''.join(r) for r in df.comment_text]\n",
    "text = ''.join(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = re.sub('[^A-Za-z0-9 ]+', '',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'hope', 'your', 'retarded', 'kids', 'get', 'anal', 'raped', 'and', 'murdered', 'for', 'having', 'such', 'a', 'fag', 'as', 'a', 'father', 'im', 'gonna', 'fuck', 'your', 'fat', 'wife', 'and', 'her', 'over', 'the', 'bridge', 'consider']\n"
     ]
    }
   ],
   "source": [
    "words = utils.preprocess(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 9910700\n",
      "Unique words: 39064\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding = 200 # Number of embedding features \n",
    "with train_graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of negative labels to sample\n",
    "n_sampled = 100\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab))\n",
    "    \n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                      labels, embed,\n",
    "                                      n_sampled, n_vocab)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ## From Thushan Ganegedara's implementation\n",
    "    valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "# !mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Iteration: 100 Avg. Training loss: 5.7186 0.3666 sec/batch\n",
      "Epoch 1/50 Iteration: 200 Avg. Training loss: 5.6822 0.3798 sec/batch\n",
      "Epoch 1/50 Iteration: 300 Avg. Training loss: 5.7193 0.4782 sec/batch\n",
      "Epoch 1/50 Iteration: 400 Avg. Training loss: 5.6446 0.4700 sec/batch\n",
      "Epoch 1/50 Iteration: 500 Avg. Training loss: 5.6233 0.4069 sec/batch\n",
      "Epoch 1/50 Iteration: 600 Avg. Training loss: 5.6214 0.4122 sec/batch\n",
      "Epoch 1/50 Iteration: 700 Avg. Training loss: 5.5528 0.3816 sec/batch\n",
      "Epoch 1/50 Iteration: 800 Avg. Training loss: 5.4687 0.3896 sec/batch\n",
      "Epoch 1/50 Iteration: 900 Avg. Training loss: 5.3900 0.3952 sec/batch\n",
      "Epoch 1/50 Iteration: 1000 Avg. Training loss: 5.2638 0.3935 sec/batch\n",
      "Nearest to time: astonished, youplease, decides, biggest, edit, complies, porch, copywritten,\n",
      "Nearest to like: paras, punjabis, reportedly, frowned, opposing, fails, meet, hardcoded,\n",
      "Nearest to your: themwhat, focuses, where, user, disagrees, facility, kof, clear,\n",
      "Nearest to me: exaggeration, hawkings, switch, finalised, deletion, werent, blackberry, or,\n",
      "Nearest to how: tar, mig, watchdogs, hardwired, ruggero, promising, administratori, not,\n",
      "Nearest to about: verbally, marino, counted, notified, suggesting, violent, stock, fabricated,\n",
      "Nearest to been: certify, several, algorithm, creation, consider, vegetarian, decisions, caleb,\n",
      "Nearest to by: coding, 221, closeness, hope, theft, crummy, samba, somebody,\n",
      "Nearest to club: snatch, trish, obliged, offends, platforms, that, corporation, finasteride,\n",
      "Nearest to controversial: fragments, albeit, hrvatska, itll, camille, crc, deleate, proponents,\n",
      "Nearest to reader: condensing, disruptive, mehello, lighthearted, rockwell, embroiled, 0036, choice,\n",
      "Nearest to couldnt: biassed, sponsoring, usaf, prospective, flight, fires, berezovsky, patagonia,\n",
      "Nearest to study: hours, spiritually, shunned, wasthe, adequate, palestine, shortsighted, shamans,\n",
      "Nearest to gone: turley, induction, molest, ugle, pitching, conventionally, fatal, emigrant,\n",
      "Nearest to april: creating, digits, httprexcurrynetpledge2html, joplin, broadcasts, exemple, sigma, uploading,\n",
      "Nearest to basis: ballet, hath, 1619, meantime, sandboxes, trains, woodrow, massachusetts,\n",
      "Epoch 1/50 Iteration: 1100 Avg. Training loss: 5.2005 0.3546 sec/batch\n",
      "Epoch 1/50 Iteration: 1200 Avg. Training loss: 5.0711 0.3992 sec/batch\n",
      "Epoch 1/50 Iteration: 1300 Avg. Training loss: 5.0263 0.4005 sec/batch\n",
      "Epoch 1/50 Iteration: 1400 Avg. Training loss: 4.9657 0.4116 sec/batch\n",
      "Epoch 1/50 Iteration: 1500 Avg. Training loss: 4.9518 0.3344 sec/batch\n",
      "Epoch 1/50 Iteration: 1600 Avg. Training loss: 4.9004 0.3317 sec/batch\n",
      "Epoch 1/50 Iteration: 1700 Avg. Training loss: 4.8427 0.3853 sec/batch\n",
      "Epoch 1/50 Iteration: 1800 Avg. Training loss: 4.8479 0.4151 sec/batch\n",
      "Epoch 1/50 Iteration: 1900 Avg. Training loss: 4.8167 0.3674 sec/batch\n",
      "Epoch 1/50 Iteration: 2000 Avg. Training loss: 4.7810 0.3279 sec/batch\n",
      "Nearest to time: youplease, astonished, complies, decides, porch, hoaxing, alphabets, alkhwarizmi,\n",
      "Nearest to like: paras, punjabis, frowned, reportedly, encyclopaedia, opposing, fails, meet,\n",
      "Nearest to your: themwhat, focuses, user, where, disagrees, legality, facility, html,\n",
      "Nearest to me: exaggeration, hawkings, werent, finalised, switch, happening, windy, conceal,\n",
      "Nearest to how: promising, mig, watchdogs, administratori, hardwired, ruggero, mongoloid, exceptions,\n",
      "Nearest to about: verbally, counted, notified, marino, fabricated, nobility, statistic, infant,\n",
      "Nearest to been: certify, algorithm, creation, ltd, vegetarian, iota, details, transcribed,\n",
      "Nearest to by: coding, 221, theft, closeness, somebody, crummy, animosity, hannity,\n",
      "Nearest to club: snatch, subject, trish, obliged, offends, platforms, corporation, reflecting,\n",
      "Nearest to controversial: albeit, fragments, itll, peninsula, proponents, genealogy, complaining, exhibit,\n",
      "Nearest to reader: disruptive, condensing, choice, appropriately, plagiarize, lighthearted, institutional, prerequisite,\n",
      "Nearest to couldnt: biassed, meeting, flight, fires, prospective, germanys, usaf, involved,\n",
      "Nearest to study: hours, adequate, shunned, wasthe, distinguish, palestine, shamans, tagi,\n",
      "Nearest to gone: induction, fun, wpor, ugle, molest, suggest, developing, cookies,\n",
      "Nearest to april: digits, creating, broadcasts, exemple, httprexcurrynetpledge2html, joplin, uploading, ordering,\n",
      "Nearest to basis: ballet, meantime, hath, achievement, massachusetts, trains, sandboxes, totally,\n",
      "Epoch 1/50 Iteration: 2100 Avg. Training loss: 4.7734 0.3718 sec/batch\n",
      "Epoch 2/50 Iteration: 2200 Avg. Training loss: 4.9744 0.1817 sec/batch\n",
      "Epoch 2/50 Iteration: 2300 Avg. Training loss: 4.7650 0.3290 sec/batch\n",
      "Epoch 2/50 Iteration: 2400 Avg. Training loss: 4.6929 0.3562 sec/batch\n",
      "Epoch 2/50 Iteration: 2500 Avg. Training loss: 4.6519 0.4334 sec/batch\n",
      "Epoch 2/50 Iteration: 2600 Avg. Training loss: 4.6669 0.4072 sec/batch\n",
      "Epoch 2/50 Iteration: 2700 Avg. Training loss: 4.6435 0.3361 sec/batch\n",
      "Epoch 2/50 Iteration: 2800 Avg. Training loss: 4.6264 0.3599 sec/batch\n",
      "Epoch 2/50 Iteration: 2900 Avg. Training loss: 4.6554 0.3604 sec/batch\n",
      "Epoch 2/50 Iteration: 3000 Avg. Training loss: 4.6323 0.3357 sec/batch\n",
      "Nearest to time: porch, youplease, astonished, hoaxing, complies, bedroom, decides, alphabets,\n",
      "Nearest to like: punjabis, paras, reportedly, frowned, 1884, cosmology, existential, encyclopaedia,\n",
      "Nearest to your: themwhat, focuses, automatically, iar, where, kof, html, backgroundcolor,\n",
      "Nearest to me: exaggeration, hawkings, finalised, unhinged, conceal, werent, blackberry, windy,\n",
      "Nearest to how: administratori, mig, promising, watchdogs, hardwired, mongoloid, farcical, exceptions,\n",
      "Nearest to about: verbally, statistic, nobility, fabricated, counted, marino, disagreements, notified,\n",
      "Nearest to been: certify, ltd, creation, algorithm, iota, sth, transcribed, wpown,\n",
      "Nearest to by: 221, coding, crummy, somebody, theft, closeness, animosity, niemti,\n",
      "Nearest to club: snatch, subject, trish, reflecting, importance, that, offends, corporation,\n",
      "Nearest to controversial: albeit, fragments, peninsula, hrvatska, camille, deleate, proponents, itll,\n",
      "Nearest to reader: disruptive, condensing, appropriately, choice, feeding, mehello, embroiled, lighthearted,\n",
      "Nearest to couldnt: biassed, prospective, fires, flight, meeting, usaf, transclusions, filter,\n",
      "Nearest to study: shunned, adequate, hours, wasthe, spiritually, tagi, shortsighted, distinguish,\n",
      "Nearest to gone: ugle, induction, molest, pitching, turley, fun, blather, messing,\n",
      "Nearest to april: digits, creating, broadcasts, exemple, httprexcurrynetpledge2html, joplin, silesia, sigma,\n",
      "Nearest to basis: ballet, meantime, sandboxes, hath, trains, 1619, massachusetts, achievement,\n",
      "Epoch 2/50 Iteration: 3100 Avg. Training loss: 4.6290 0.3396 sec/batch\n",
      "Epoch 2/50 Iteration: 3200 Avg. Training loss: 4.6113 0.3658 sec/batch\n",
      "Epoch 2/50 Iteration: 3300 Avg. Training loss: 4.6269 0.3141 sec/batch\n",
      "Epoch 2/50 Iteration: 3400 Avg. Training loss: 4.5787 0.3075 sec/batch\n",
      "Epoch 2/50 Iteration: 3500 Avg. Training loss: 4.5915 0.3104 sec/batch\n",
      "Epoch 2/50 Iteration: 3600 Avg. Training loss: 4.6156 0.3109 sec/batch\n",
      "Epoch 2/50 Iteration: 3700 Avg. Training loss: 4.5667 0.3153 sec/batch\n",
      "Epoch 2/50 Iteration: 3800 Avg. Training loss: 4.5845 0.2946 sec/batch\n",
      "Epoch 2/50 Iteration: 3900 Avg. Training loss: 4.5653 0.3130 sec/batch\n",
      "Epoch 2/50 Iteration: 4000 Avg. Training loss: 4.5986 0.3073 sec/batch\n",
      "Nearest to time: porch, youplease, hoaxing, bedroom, astonished, complies, alphabets, decides,\n",
      "Nearest to like: punjabis, reportedly, paras, 1884, existential, vinnycee, frowned, cosmology,\n",
      "Nearest to your: themwhat, automatically, focuses, backgroundcolor, welcomehello, iar, html, user,\n",
      "Nearest to me: hawkings, finalised, exaggeration, wayback, unhinged, wpmos, talk, bubbling,\n",
      "Nearest to how: administratori, mig, watchdogs, promising, schlafly, exceptions, mongoloid, unproved,\n",
      "Nearest to about: verbally, statistic, fabricated, marino, nobility, bori, duff, disagreements,\n",
      "Nearest to been: creation, certify, ltd, sth, iota, algorithm, consider, wpown,\n",
      "Nearest to by: 221, coding, crummy, closeness, niemti, animosity, somebody, theft,\n",
      "Nearest to club: snatch, subject, trish, importance, that, corporation, reflecting, 0157,\n",
      "Nearest to controversial: fragments, albeit, camille, peninsula, hrvatska, deleate, categorizes, crc,\n",
      "Nearest to reader: disruptive, condensing, appropriately, mehello, plagiarize, embroiled, lightly, feeding,\n",
      "Nearest to couldnt: biassed, prospective, meeting, fires, usaf, filter, flight, transclusions,\n",
      "Nearest to study: shunned, adequate, tagi, spiritually, wasthe, distinguish, hours, dalton,\n",
      "Nearest to gone: ugle, pitching, induction, molest, raise, turley, messing, 1052,\n",
      "Nearest to april: digits, broadcasts, exemple, creating, httprexcurrynetpledge2html, joplin, silesia, editorship,\n",
      "Nearest to basis: ballet, sandboxes, trains, achievement, replete, hath, distasteful, meantime,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 Iteration: 4100 Avg. Training loss: 4.5747 0.3559 sec/batch\n",
      "Epoch 2/50 Iteration: 4200 Avg. Training loss: 4.5665 0.3004 sec/batch\n",
      "Epoch 3/50 Iteration: 4300 Avg. Training loss: 4.5516 0.0784 sec/batch\n",
      "Epoch 3/50 Iteration: 4400 Avg. Training loss: 4.5420 0.2853 sec/batch\n",
      "Epoch 3/50 Iteration: 4500 Avg. Training loss: 4.4922 0.3133 sec/batch\n",
      "Epoch 3/50 Iteration: 4600 Avg. Training loss: 4.5144 0.3194 sec/batch\n",
      "Epoch 3/50 Iteration: 4700 Avg. Training loss: 4.5119 0.3428 sec/batch\n",
      "Epoch 3/50 Iteration: 4800 Avg. Training loss: 4.4807 0.3419 sec/batch\n",
      "Epoch 3/50 Iteration: 4900 Avg. Training loss: 4.5081 0.2929 sec/batch\n",
      "Epoch 3/50 Iteration: 5000 Avg. Training loss: 4.4909 0.2877 sec/batch\n",
      "Nearest to time: porch, youplease, bedroom, hoaxing, astonished, complies, decides, alphabets,\n",
      "Nearest to like: punjabis, reportedly, paras, vinnycee, 1884, existential, frowned, cosmology,\n",
      "Nearest to your: automatically, themwhat, user, html, welcomehello, backgroundcolor, wikipedian, iar,\n",
      "Nearest to me: hawkings, finalised, talk, unhinged, wayback, bubbling, puzzles, wpmos,\n",
      "Nearest to how: administratori, stylebackgroundcolorf5fffastylewidth, watchdogs, mig, border0, exceptions, crossposted, schlafly,\n",
      "Nearest to about: verbally, statistic, bori, nobility, fabricated, disagreements, duff, pirates,\n",
      "Nearest to been: creation, certify, sth, ltd, iota, consider, algorithm, transcribed,\n",
      "Nearest to by: 221, crummy, coding, niemti, animosity, hannity, closeness, somebody,\n",
      "Nearest to club: snatch, subject, trish, importance, that, tolerated, corporation, 0157,\n",
      "Nearest to controversial: fragments, deleate, albeit, categorizes, peninsula, camille, hrvatska, crc,\n",
      "Nearest to reader: mehello, disruptive, condensing, appropriately, embroiled, lightly, plagiarize, lighthearted,\n",
      "Nearest to couldnt: biassed, prospective, meeting, fires, filter, flight, eminems, butchers,\n",
      "Nearest to study: shunned, adequate, spiritually, wasthe, tagi, distinguish, shortsighted, dalton,\n",
      "Nearest to gone: ugle, induction, pitching, raise, turley, molest, messing, 1052,\n",
      "Nearest to april: digits, broadcasts, exemple, joplin, creating, httprexcurrynetpledge2html, silesia, editorship,\n",
      "Nearest to basis: ballet, distasteful, achievement, trains, meantime, sandboxes, massachusetts, replete,\n",
      "Epoch 3/50 Iteration: 5100 Avg. Training loss: 4.4907 0.3419 sec/batch\n",
      "Epoch 3/50 Iteration: 5200 Avg. Training loss: 4.4921 0.3022 sec/batch\n",
      "Epoch 3/50 Iteration: 5300 Avg. Training loss: 4.4889 0.3351 sec/batch\n",
      "Epoch 3/50 Iteration: 5400 Avg. Training loss: 4.4899 0.2663 sec/batch\n",
      "Epoch 3/50 Iteration: 5500 Avg. Training loss: 4.4704 0.2795 sec/batch\n",
      "Epoch 3/50 Iteration: 5600 Avg. Training loss: 4.4587 0.2870 sec/batch\n",
      "Epoch 3/50 Iteration: 5700 Avg. Training loss: 4.4756 0.2409 sec/batch\n",
      "Epoch 3/50 Iteration: 5800 Avg. Training loss: 4.4829 0.2631 sec/batch\n",
      "Epoch 3/50 Iteration: 5900 Avg. Training loss: 4.4742 0.3350 sec/batch\n",
      "Epoch 3/50 Iteration: 6000 Avg. Training loss: 4.4631 0.2996 sec/batch\n",
      "Nearest to time: porch, youplease, bedroom, hoaxing, f15, complies, astonished, copywritten,\n",
      "Nearest to like: punjabis, vinnycee, reportedly, paras, existential, 1884, wishmanual, frowned,\n",
      "Nearest to your: automatically, themwhat, welcomehello, user, backgroundcolor, overrated, html, helpme,\n",
      "Nearest to me: hawkings, finalised, bubbling, talk, puzzles, wayback, conceal, haste,\n",
      "Nearest to how: administratori, stylebackgroundcolorf5fffastylewidth, watchdogs, promising, crossposted, border0, themhow, mig,\n",
      "Nearest to about: verbally, bori, statistic, fabricated, pirates, noisy, disagreements, brochures,\n",
      "Nearest to been: creation, sth, certify, ltd, consider, iota, on, medvedev,\n",
      "Nearest to by: 221, coding, crummy, niemti, somebody, theft, closeness, multilicense,\n",
      "Nearest to club: snatch, subject, importance, that, trish, group, tolerated, corporation,\n",
      "Nearest to controversial: deleate, categorizes, fragments, albeit, peninsula, camille, hrvatska, itll,\n",
      "Nearest to reader: mehello, condensing, appropriately, disruptive, lightly, embroiled, plagiarize, curvature,\n",
      "Nearest to couldnt: biassed, prospective, meeting, butchers, fires, explication, filter, xl,\n",
      "Nearest to study: shunned, wasthe, distinguish, adequate, spiritually, tagi, shortsighted, services,\n",
      "Nearest to gone: ugle, raise, induction, turley, pitching, molest, 1052, messing,\n",
      "Nearest to april: digits, broadcasts, exemple, joplin, creating, editorship, silesia, charged,\n",
      "Nearest to basis: ballet, achievement, distasteful, trains, meantime, litmus, sandboxes, replete,\n",
      "Epoch 3/50 Iteration: 6100 Avg. Training loss: 4.4821 0.2282 sec/batch\n",
      "Epoch 3/50 Iteration: 6200 Avg. Training loss: 4.4719 0.2259 sec/batch\n",
      "Epoch 3/50 Iteration: 6300 Avg. Training loss: 4.4571 0.2228 sec/batch\n",
      "Epoch 3/50 Iteration: 6400 Avg. Training loss: 4.4405 0.2448 sec/batch\n",
      "Epoch 4/50 Iteration: 6500 Avg. Training loss: 4.2376 0.2215 sec/batch\n",
      "Epoch 4/50 Iteration: 6600 Avg. Training loss: 4.3867 0.2332 sec/batch\n",
      "Epoch 4/50 Iteration: 6700 Avg. Training loss: 4.4244 0.2642 sec/batch\n",
      "Epoch 4/50 Iteration: 6800 Avg. Training loss: 4.4008 0.2593 sec/batch\n",
      "Epoch 4/50 Iteration: 6900 Avg. Training loss: 4.4020 0.2703 sec/batch\n",
      "Epoch 4/50 Iteration: 7000 Avg. Training loss: 4.3983 0.2698 sec/batch\n",
      "Nearest to time: youplease, porch, bedroom, hoaxing, astonished, copywritten, complies, decides,\n",
      "Nearest to like: vinnycee, punjabis, existential, reportedly, 1884, paras, frowned, wishmanual,\n",
      "Nearest to your: automatically, themwhat, welcomehello, html, backgroundcolor, verticalaligntopcolor000000fontsize, overrated, user,\n",
      "Nearest to me: i, puzzles, finalised, hawkings, unhinged, werent, egg, wpcivil,\n",
      "Nearest to how: administratori, crossposted, watchdogs, promising, mig, unproved, stylebackgroundcolorf5fffastylewidth, patrols,\n",
      "Nearest to about: statistic, bori, verbally, pirates, fabricated, duff, extremism, brochures,\n",
      "Nearest to been: creation, certify, sth, ltd, consider, transcribed, iota, on,\n",
      "Nearest to by: 221, coding, crummy, niemti, roe, wan, transclusion, multilicense,\n",
      "Nearest to club: subject, snatch, importance, that, group, trish, corporation, speedily,\n",
      "Nearest to controversial: categorizes, peninsula, deleate, albeit, fragments, camille, hrvatska, mebut,\n",
      "Nearest to reader: appropriately, condensing, mehello, disruptive, plagiarize, embroiled, humility, feeding,\n",
      "Nearest to couldnt: biassed, prospective, butchers, meeting, eminems, fires, xl, filter,\n",
      "Nearest to study: shunned, spiritually, wasthe, adequate, distinguish, tagi, services, churchs,\n",
      "Nearest to gone: ugle, raise, pitching, induction, turley, 1052, molest, show,\n",
      "Nearest to april: digits, broadcasts, exemple, creating, joplin, silesia, charged, defunct,\n",
      "Nearest to basis: ballet, achievement, distasteful, litmus, meantime, sandboxes, cyclothymia, tolerance,\n",
      "Epoch 4/50 Iteration: 7100 Avg. Training loss: 4.3952 0.2674 sec/batch\n",
      "Epoch 4/50 Iteration: 7200 Avg. Training loss: 4.3968 0.2596 sec/batch\n",
      "Epoch 4/50 Iteration: 7300 Avg. Training loss: 4.4081 0.2834 sec/batch\n",
      "Epoch 4/50 Iteration: 7400 Avg. Training loss: 4.4204 0.2768 sec/batch\n",
      "Epoch 4/50 Iteration: 7500 Avg. Training loss: 4.3984 0.2674 sec/batch\n",
      "Epoch 4/50 Iteration: 7600 Avg. Training loss: 4.3944 0.2870 sec/batch\n",
      "Epoch 4/50 Iteration: 7700 Avg. Training loss: 4.3686 0.2696 sec/batch\n",
      "Epoch 4/50 Iteration: 7800 Avg. Training loss: 4.3968 0.3038 sec/batch\n",
      "Epoch 4/50 Iteration: 7900 Avg. Training loss: 4.4038 0.2539 sec/batch\n",
      "Epoch 4/50 Iteration: 8000 Avg. Training loss: 4.3972 0.2627 sec/batch\n",
      "Nearest to time: youplease, porch, bedroom, hoaxing, copywritten, sudoghost, complies, f15,\n",
      "Nearest to like: vinnycee, punjabis, wishmanual, existential, reportedly, paras, extremly, 1884,\n",
      "Nearest to your: automatically, welcomehello, themwhat, verticalaligntopcolor000000fontsize, backgroundcolor, prorussian, helpme, wikipedian,\n",
      "Nearest to me: finalised, hawkings, puzzles, i, egg, wpcivil, unhinged, conceal,\n",
      "Nearest to how: administratori, border0, mentored, stylebackgroundcolorf5fffastylewidth, getting, themhow, crossposted, unproved,\n",
      "Nearest to about: bori, statistic, verbally, pirates, encyclopedia, talksee, extremism, fgc,\n",
      "Nearest to been: creation, certify, sth, ltd, consider, bl, medvedev, featured,\n",
      "Nearest to by: 221, coding, crummy, multilicense, roe, somebody, wan, phantom,\n",
      "Nearest to club: subject, snatch, importance, group, trish, corporation, that, speedily,\n",
      "Nearest to controversial: categorizes, deleate, camille, albeit, fragments, peninsula, hrvatska, mebut,\n",
      "Nearest to reader: appropriately, mehello, condensing, plagiarize, curvature, embroiled, wikipediastub, disruptive,\n",
      "Nearest to couldnt: biassed, prospective, butchers, eminems, meeting, filter, xl, explication,\n",
      "Nearest to study: shunned, adequate, spiritually, wasthe, distinguish, tagi, services, palestine,\n",
      "Nearest to gone: raise, ugle, 1052, induction, turley, pitching, molest, show,\n",
      "Nearest to april: digits, broadcasts, exemple, defunct, creating, charged, silesia, mohamed,\n",
      "Nearest to basis: ballet, achievement, distasteful, litmus, replete, tolerance, sandboxes, tangible,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 Iteration: 8100 Avg. Training loss: 4.3877 0.2612 sec/batch\n",
      "Epoch 4/50 Iteration: 8200 Avg. Training loss: 4.4232 0.2545 sec/batch\n",
      "Epoch 4/50 Iteration: 8300 Avg. Training loss: 4.3731 0.2477 sec/batch\n",
      "Epoch 4/50 Iteration: 8400 Avg. Training loss: 4.3922 0.2454 sec/batch\n",
      "Epoch 4/50 Iteration: 8500 Avg. Training loss: 4.3950 0.2490 sec/batch\n",
      "Epoch 5/50 Iteration: 8600 Avg. Training loss: 4.1242 0.1224 sec/batch\n",
      "Epoch 5/50 Iteration: 8700 Avg. Training loss: 4.2387 0.2229 sec/batch\n",
      "Epoch 5/50 Iteration: 8800 Avg. Training loss: 4.3347 0.2360 sec/batch\n",
      "Epoch 5/50 Iteration: 8900 Avg. Training loss: 4.3437 0.2749 sec/batch\n",
      "Epoch 5/50 Iteration: 9000 Avg. Training loss: 4.3163 0.2727 sec/batch\n",
      "Nearest to time: youplease, porch, bedroom, hoaxing, sudoghost, pilsudski, decides, copywritten,\n",
      "Nearest to like: vinnycee, punjabis, existential, wishmanual, reportedly, extremly, 1884, frowned,\n",
      "Nearest to your: automatically, themwhat, welcomehello, verticalaligntopcolor000000fontsize, everyone, tallyho, wikipedian, overrated,\n",
      "Nearest to me: i, puzzles, finalised, unhinged, egg, conceal, wpcivil, werent,\n",
      "Nearest to how: administratori, getting, border0, themhow, mentored, crossposted, stylebackgroundcolorf5fffastylewidth, promising,\n",
      "Nearest to about: bori, pirates, statistic, encyclopedia, extremism, posthardcore, duff, not,\n",
      "Nearest to been: creation, certify, sth, ltd, bl, iota, attempted, featured,\n",
      "Nearest to by: 221, coding, crummy, monet, somebody, multilicense, niemti, wan,\n",
      "Nearest to club: subject, snatch, importance, group, trish, corporation, speedily, tolerated,\n",
      "Nearest to controversial: categorizes, deleate, albeit, camille, peninsula, fragments, mebut, input,\n",
      "Nearest to reader: appropriately, mehello, plagiarize, condensing, wikipediastub, curvature, embroiled, feeding,\n",
      "Nearest to couldnt: biassed, butchers, prospective, xl, explication, meeting, eminems, filter,\n",
      "Nearest to study: shunned, adequate, spiritually, distinguish, services, wasthe, tagi, churchs,\n",
      "Nearest to gone: raise, ugle, 1052, show, turley, pitching, fun, molest,\n",
      "Nearest to april: digits, broadcasts, exemple, defunct, creating, november, charged, mohamed,\n",
      "Nearest to basis: ballet, achievement, litmus, distasteful, meantime, sandboxes, brokaws, tolerance,\n",
      "Epoch 5/50 Iteration: 9100 Avg. Training loss: 4.3439 0.2602 sec/batch\n",
      "Epoch 5/50 Iteration: 9200 Avg. Training loss: 4.3549 0.2700 sec/batch\n",
      "Epoch 5/50 Iteration: 9300 Avg. Training loss: 4.3317 0.3225 sec/batch\n",
      "Epoch 5/50 Iteration: 9400 Avg. Training loss: 4.3264 0.2946 sec/batch\n",
      "Epoch 5/50 Iteration: 9500 Avg. Training loss: 4.3517 0.2289 sec/batch\n",
      "Epoch 5/50 Iteration: 9600 Avg. Training loss: 4.3277 0.2644 sec/batch\n",
      "Epoch 5/50 Iteration: 9700 Avg. Training loss: 4.3438 0.2803 sec/batch\n",
      "Epoch 5/50 Iteration: 9800 Avg. Training loss: 4.3295 0.2724 sec/batch\n",
      "Epoch 5/50 Iteration: 9900 Avg. Training loss: 4.3273 0.2734 sec/batch\n",
      "Epoch 5/50 Iteration: 10000 Avg. Training loss: 4.3153 0.2748 sec/batch\n",
      "Nearest to time: youplease, porch, bedroom, scrubbing, pilsudski, hoaxing, wong, f15,\n",
      "Nearest to like: vinnycee, wishmanual, punjabis, existential, periodically, frowned, stylei, reportedly,\n",
      "Nearest to your: automatically, welcomehello, themwhat, tallyho, wikipedian, 2246, verticalaligntopcolor000000fontsize, backgroundcolor,\n",
      "Nearest to me: finalised, unhinged, conceal, puzzles, egg, i, bubbling, hawkings,\n",
      "Nearest to how: administratori, getting, border0, mentored, themhow, crossposted, stylebackgroundcolorf5fffastylewidth, involvementhappy,\n",
      "Nearest to about: bori, encyclopedia, statistic, not, pirates, posthardcore, extremism, verbally,\n",
      "Nearest to been: creation, sth, certify, ltd, payments, bl, consider, iota,\n",
      "Nearest to by: 221, coding, multilicense, monet, transclusion, wan, somebody, roe,\n",
      "Nearest to club: subject, importance, snatch, group, rugby, corporation, trish, speedily,\n",
      "Nearest to controversial: categorizes, deleate, peninsula, albeit, fragments, camille, mebut, hovind,\n",
      "Nearest to reader: wikipediastub, mehello, appropriately, plagiarize, curvature, condensing, feeding, nonmainstream,\n",
      "Nearest to couldnt: biassed, butchers, prospective, xl, explication, lennart, npr, filter,\n",
      "Nearest to study: shunned, adequate, wasthe, churchs, spiritually, services, distinguish, tagi,\n",
      "Nearest to gone: raise, pitching, 1052, show, ugle, turley, rfas, molest,\n",
      "Nearest to april: digits, broadcasts, defunct, november, exemple, 9, 2007, mohamed,\n",
      "Nearest to basis: ballet, achievement, litmus, distasteful, tolerance, tangible, meantime, brokaws,\n",
      "Epoch 5/50 Iteration: 10100 Avg. Training loss: 4.3274 0.3196 sec/batch\n",
      "Epoch 5/50 Iteration: 10200 Avg. Training loss: 4.3234 0.2918 sec/batch\n",
      "Epoch 5/50 Iteration: 10300 Avg. Training loss: 4.3289 0.3147 sec/batch\n",
      "Epoch 5/50 Iteration: 10400 Avg. Training loss: 4.3286 0.2918 sec/batch\n",
      "Epoch 5/50 Iteration: 10500 Avg. Training loss: 4.3266 0.2554 sec/batch\n",
      "Epoch 5/50 Iteration: 10600 Avg. Training loss: 4.3369 0.2730 sec/batch\n",
      "Epoch 6/50 Iteration: 10700 Avg. Training loss: 4.0904 0.0626 sec/batch\n",
      "Epoch 6/50 Iteration: 10800 Avg. Training loss: 4.0635 0.2853 sec/batch\n",
      "Epoch 6/50 Iteration: 10900 Avg. Training loss: 4.2299 0.3327 sec/batch\n",
      "Epoch 6/50 Iteration: 11000 Avg. Training loss: 4.2866 0.3000 sec/batch\n",
      "Nearest to time: porch, bedroom, youplease, sudoghost, wong, pilsudski, truculent, scrubbing,\n",
      "Nearest to like: vinnycee, punjabis, wishmanual, existential, periodically, extremly, reportedly, frowned,\n",
      "Nearest to your: themwhat, everyone, automatically, welcomehello, tallyho, watch, cheat, verticalaligntopcolor000000fontsize,\n",
      "Nearest to me: i, finalised, unhinged, conceal, puzzles, egg, cant, werent,\n",
      "Nearest to how: administratori, crossposted, themhow, getting, mentored, border0, adjudicate, patrols,\n",
      "Nearest to about: bori, statistic, encyclopedia, not, posthardcore, pirates, extremism, wrath,\n",
      "Nearest to been: creation, certify, ltd, sth, bl, requesting, tumbledown, datestamp,\n",
      "Nearest to by: 221, coding, transclusion, monet, multilicense, wan, niemti, animosity,\n",
      "Nearest to club: subject, importance, group, snatch, rugby, speedily, company, tolerated,\n",
      "Nearest to controversial: categorizes, deleate, hovind, peninsula, remarks, albeit, fragments, mebut,\n",
      "Nearest to reader: wikipediastub, appropriately, curvature, mehello, plagiarize, lightly, feeding, headmaster,\n",
      "Nearest to couldnt: biassed, butchers, lennart, xl, prospective, explication, eminems, filter,\n",
      "Nearest to study: shunned, spiritually, adequate, services, wasthe, tagi, churchs, distinguish,\n",
      "Nearest to gone: raise, show, 1052, turley, pitching, molest, ugle, fun,\n",
      "Nearest to april: digits, broadcasts, november, defunct, 2007, exemple, 9, mohamed,\n",
      "Nearest to basis: ballet, distasteful, achievement, litmus, tolerance, disagree, wprfpp, nonadmin,\n",
      "Epoch 6/50 Iteration: 11100 Avg. Training loss: 4.3154 0.3103 sec/batch\n",
      "Epoch 6/50 Iteration: 11200 Avg. Training loss: 4.2571 0.2769 sec/batch\n",
      "Epoch 6/50 Iteration: 11300 Avg. Training loss: 4.3055 0.2896 sec/batch\n",
      "Epoch 6/50 Iteration: 11400 Avg. Training loss: 4.2958 0.3001 sec/batch\n",
      "Epoch 6/50 Iteration: 11500 Avg. Training loss: 4.2725 0.2621 sec/batch\n",
      "Epoch 6/50 Iteration: 11600 Avg. Training loss: 4.2760 0.2678 sec/batch\n",
      "Epoch 6/50 Iteration: 11700 Avg. Training loss: 4.3151 0.2820 sec/batch\n",
      "Epoch 6/50 Iteration: 11800 Avg. Training loss: 4.2912 0.3283 sec/batch\n",
      "Epoch 6/50 Iteration: 11900 Avg. Training loss: 4.2579 0.2976 sec/batch\n",
      "Epoch 6/50 Iteration: 12000 Avg. Training loss: 4.2598 0.3510 sec/batch\n",
      "Nearest to time: youplease, porch, scrubbing, bedroom, truculent, pilsudski, wong, sudoghost,\n",
      "Nearest to like: vinnycee, wishmanual, punjabis, existential, periodically, extremly, reportedly, welcomehello,\n",
      "Nearest to your: automatically, welcomehello, tallyho, wikipedian, backgroundcolor, 2246, themwhat, verticalaligntopcolor000000fontsize,\n",
      "Nearest to me: finalised, unhinged, conceal, egg, puzzles, bubbling, exaggeration, insteadif,\n",
      "Nearest to how: administratori, getting, border0, mentored, themhow, 1547, watchdogs, stylebackgroundcolorf5fffastylewidth,\n",
      "Nearest to about: encyclopedia, bori, not, pirates, statistic, talksee, extremism, duff,\n",
      "Nearest to been: creation, sth, ltd, certify, murders, bl, requesting, payments,\n",
      "Nearest to by: 221, multilicense, coding, monet, transclusion, roe, haramein, niemti,\n",
      "Nearest to club: subject, importance, group, rugby, snatch, speedily, company, indicate,\n",
      "Nearest to controversial: categorizes, hovind, deleate, albeit, peninsula, remarks, mebut, fragments,\n",
      "Nearest to reader: wikipediastub, appropriately, mehello, plagiarize, lightly, curvature, flexibility, humility,\n",
      "Nearest to couldnt: biassed, butchers, prospective, explication, lennart, npr, xl, eminems,\n",
      "Nearest to study: shunned, wasthe, adequate, spiritually, churchs, tagi, services, customs,\n",
      "Nearest to gone: raise, 1052, turley, show, pitching, molest, rfas, fun,\n",
      "Nearest to april: digits, broadcasts, november, defunct, 2007, 9, exemple, meetup,\n",
      "Nearest to basis: ballet, distasteful, achievement, litmus, tolerance, mantra, anthrax, disregards,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 Iteration: 12100 Avg. Training loss: 4.2711 0.3437 sec/batch\n",
      "Epoch 6/50 Iteration: 12200 Avg. Training loss: 4.2895 0.2952 sec/batch\n",
      "Epoch 6/50 Iteration: 12300 Avg. Training loss: 4.2751 0.3052 sec/batch\n",
      "Epoch 6/50 Iteration: 12400 Avg. Training loss: 4.2656 0.3148 sec/batch\n",
      "Epoch 6/50 Iteration: 12500 Avg. Training loss: 4.2891 0.3246 sec/batch\n",
      "Epoch 6/50 Iteration: 12600 Avg. Training loss: 4.2767 0.3145 sec/batch\n",
      "Epoch 6/50 Iteration: 12700 Avg. Training loss: 4.2843 0.3169 sec/batch\n",
      "Epoch 6/50 Iteration: 12800 Avg. Training loss: 4.2565 0.3780 sec/batch\n",
      "Epoch 7/50 Iteration: 12900 Avg. Training loss: 3.8048 0.2198 sec/batch\n",
      "Epoch 7/50 Iteration: 13000 Avg. Training loss: 4.1258 0.2471 sec/batch\n",
      "Nearest to time: youplease, truculent, bedroom, porch, wong, sudoghost, scrubbing, vandals,\n",
      "Nearest to like: vinnycee, punjabis, wishmanual, existential, roughshod, mumble, really, stay,\n",
      "Nearest to your: everyone, themwhat, automatically, bradbury, welcomehello, truth, yopu, cheat,\n",
      "Nearest to me: i, unhinged, puzzles, cant, conceal, finalised, egg, computeri,\n",
      "Nearest to how: administratori, getting, themhow, crossposted, like, head, 1547, patrols,\n",
      "Nearest to about: not, bori, encyclopedia, posthardcore, talksee, wrath, statistic, pirates,\n",
      "Nearest to been: creation, murders, sth, ltd, bl, certify, requesting, tumbledown,\n",
      "Nearest to by: 221, monet, coding, transclusion, haramein, multilicense, niemti, criterias,\n",
      "Nearest to club: subject, importance, rugby, group, snatch, tolerated, trish, speedily,\n",
      "Nearest to controversial: hovind, categorizes, remarks, deleate, albeit, mebut, peninsula, showcased,\n",
      "Nearest to reader: wikipediastub, lightly, curvature, mehello, appropriately, feeding, humility, flexibility,\n",
      "Nearest to couldnt: biassed, butchers, xl, prospective, explication, lennart, eminems, npr,\n",
      "Nearest to study: shunned, wasthe, spiritually, adequate, services, sensitivity, tagi, churchs,\n",
      "Nearest to gone: raise, 1052, show, molest, turley, pitching, fun, rfas,\n",
      "Nearest to april: digits, broadcasts, november, defunct, 2007, 9, exemple, meetup,\n",
      "Nearest to basis: ballet, distasteful, achievement, litmus, mantra, tolerance, disregards, costume,\n",
      "Epoch 7/50 Iteration: 13100 Avg. Training loss: 4.2614 0.2521 sec/batch\n",
      "Epoch 7/50 Iteration: 13200 Avg. Training loss: 4.2515 0.2415 sec/batch\n",
      "Epoch 7/50 Iteration: 13300 Avg. Training loss: 4.2236 0.2536 sec/batch\n",
      "Epoch 7/50 Iteration: 13400 Avg. Training loss: 4.2362 0.2433 sec/batch\n",
      "Epoch 7/50 Iteration: 13500 Avg. Training loss: 4.2544 0.2495 sec/batch\n",
      "Epoch 7/50 Iteration: 13600 Avg. Training loss: 4.2422 0.2798 sec/batch\n",
      "Epoch 7/50 Iteration: 13700 Avg. Training loss: 4.2666 0.3976 sec/batch\n",
      "Epoch 7/50 Iteration: 13800 Avg. Training loss: 4.2481 0.3649 sec/batch\n",
      "Epoch 7/50 Iteration: 13900 Avg. Training loss: 4.2577 0.3292 sec/batch\n",
      "Epoch 7/50 Iteration: 14000 Avg. Training loss: 4.2245 0.3081 sec/batch\n",
      "Nearest to time: youplease, scrubbing, sudoghost, porch, pilsudski, bedroom, truculent, wong,\n",
      "Nearest to like: vinnycee, punjabis, wishmanual, existential, welcomehello, roughshod, periodically, stay,\n",
      "Nearest to your: welcomehello, automatically, for, backgroundcolor, yopu, wikipedian, five, tallyho,\n",
      "Nearest to me: i, conceal, finalised, unhinged, puzzles, egg, insteadif, cc,\n",
      "Nearest to how: getting, administratori, themhow, mentored, border0, patrols, imagesintuitive, 1547,\n",
      "Nearest to about: encyclopedia, not, bori, pirates, talksee, extremism, statistic, duff,\n",
      "Nearest to been: creation, murders, sth, ltd, bl, certify, requesting, on,\n",
      "Nearest to by: 221, monet, multilicense, coding, wan, haramein, transclusion, vickers,\n",
      "Nearest to club: subject, importance, rugby, group, snatch, indicate, company, trish,\n",
      "Nearest to controversial: hovind, remarks, categorizes, deleate, albeit, peninsula, mebut, sherrod,\n",
      "Nearest to reader: wikipediastub, mehello, lightly, appropriately, curvature, humility, flexibility, feeding,\n",
      "Nearest to couldnt: prospective, butchers, biassed, xl, explication, npr, lennart, eminems,\n",
      "Nearest to study: shunned, wasthe, spiritually, adequate, patients, customs, services, tagi,\n",
      "Nearest to gone: raise, 1052, rfas, pitching, turley, molest, show, hex,\n",
      "Nearest to april: digits, november, broadcasts, defunct, 2007, 9, exemple, meetup,\n",
      "Nearest to basis: ballet, distasteful, achievement, litmus, tolerance, mantra, accomplishes, disregards,\n",
      "Epoch 7/50 Iteration: 14100 Avg. Training loss: 4.2086 0.3023 sec/batch\n",
      "Epoch 7/50 Iteration: 14200 Avg. Training loss: 4.2308 0.2736 sec/batch\n",
      "Epoch 7/50 Iteration: 14300 Avg. Training loss: 4.2457 0.3104 sec/batch\n",
      "Epoch 7/50 Iteration: 14400 Avg. Training loss: 4.2452 0.3306 sec/batch\n",
      "Epoch 7/50 Iteration: 14500 Avg. Training loss: 4.2099 0.3506 sec/batch\n",
      "Epoch 7/50 Iteration: 14600 Avg. Training loss: 4.2572 0.3308 sec/batch\n",
      "Epoch 7/50 Iteration: 14700 Avg. Training loss: 4.2241 0.3068 sec/batch\n",
      "Epoch 7/50 Iteration: 14800 Avg. Training loss: 4.2265 0.3060 sec/batch\n",
      "Epoch 7/50 Iteration: 14900 Avg. Training loss: 4.2431 0.3552 sec/batch\n",
      "Epoch 8/50 Iteration: 15000 Avg. Training loss: 3.8069 0.1656 sec/batch\n",
      "Nearest to time: truculent, bedroom, wong, sudoghost, pilsudski, youplease, scrubbing, porch,\n",
      "Nearest to like: wishmanual, punjabis, existential, vinnycee, roughshod, welcomehello, stay, periodically,\n",
      "Nearest to your: automatically, welcomehello, yopu, cheat, bradbury, everyone, themwhat, verticalaligntopcolor000000fontsize,\n",
      "Nearest to me: i, conceal, computeri, unhinged, puzzles, egg, finalised, cant,\n",
      "Nearest to how: getting, administratori, themhow, mentored, border0, imagesintuitive, edit, patrols,\n",
      "Nearest to about: not, encyclopedia, bori, wrath, talksee, pirates, posthardcore, statistic,\n",
      "Nearest to been: creation, sth, murders, requesting, on, bl, ltd, tumbledown,\n",
      "Nearest to by: 221, monet, coding, wan, multilicense, registrant, animosity, commencement,\n",
      "Nearest to club: subject, rugby, importance, group, indicate, trish, tolerated, company,\n",
      "Nearest to controversial: hovind, remarks, categorizes, deleate, albeit, mebut, sherrod, problematic,\n",
      "Nearest to reader: wikipediastub, lightly, feeding, curvature, appropriately, humility, mehello, damac,\n",
      "Nearest to couldnt: prospective, butchers, biassed, xl, explication, npr, lennart, pj,\n",
      "Nearest to study: shunned, spiritually, wasthe, patients, adequate, services, customs, sensitivity,\n",
      "Nearest to gone: raise, 1052, molest, rfas, turley, hex, pitching, scibaby,\n",
      "Nearest to april: digits, november, broadcasts, defunct, 2007, 9, meetup, june,\n",
      "Nearest to basis: ballet, distasteful, achievement, litmus, mantra, tolerance, disregards, nonadmin,\n",
      "Epoch 8/50 Iteration: 15100 Avg. Training loss: 4.0628 0.3290 sec/batch\n",
      "Epoch 8/50 Iteration: 15200 Avg. Training loss: 4.1584 0.3522 sec/batch\n",
      "Epoch 8/50 Iteration: 15300 Avg. Training loss: 4.2069 0.3886 sec/batch\n",
      "Epoch 8/50 Iteration: 15400 Avg. Training loss: 4.2121 0.5818 sec/batch\n",
      "Epoch 8/50 Iteration: 15500 Avg. Training loss: 4.1852 0.5086 sec/batch\n",
      "Epoch 8/50 Iteration: 16300 Avg. Training loss: 4.2150 0.6355 sec/batch\n",
      "Epoch 8/50 Iteration: 16400 Avg. Training loss: 4.2199 0.5385 sec/batch\n",
      "Epoch 8/50 Iteration: 16500 Avg. Training loss: 4.2188 0.5784 sec/batch\n",
      "Epoch 8/50 Iteration: 16600 Avg. Training loss: 4.1969 0.5257 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 1000\n",
    "window_size = 10\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            if iteration % 1000 == 0:\n",
    "                # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = int_to_vocab[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = int_to_vocab[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"../data/checkpoints/w2v_3allwords.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('../data/checkpoints'))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viz_words = 500\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "ax.axis([-200.0,200.0, -200.0,200.0])\n",
    "ax = fig.gca()\n",
    "ax.set_autoscale_on(False)\n",
    "\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)\n",
    "plt.savefig('w2v_3_AllWords.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
