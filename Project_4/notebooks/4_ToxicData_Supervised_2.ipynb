{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianmcmahon/anaconda3/envs/tensorflow1.4/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import (sent_tokenize, TreebankWordTokenizer, WhitespaceTokenizer)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB,MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report,confusion_matrix, precision_score, \n",
    "                             recall_score, f1_score, roc_curve, roc_auc_score, average_precision_score, \n",
    "                             precision_recall_curve, auc)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def sentence_tokenizer(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "def whitespace_tokenizer(sentences):\n",
    "    listy = []\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    for i in list(range(0,len(sentences))):\n",
    "        tokenized = tokenizer.tokenize(sentences[i])\n",
    "        listy.append(tokenized)\n",
    "    return listy\n",
    "\n",
    "def polarity(sentences):\n",
    "    listy = []\n",
    "    for i in list(range(0,len(sentences))):\n",
    "        pol = TextBlob(sentences[i]).polarity\n",
    "        listy.append(pol)\n",
    "    return np.min(listy), np.max(listy), np.mean(listy),listy\n",
    "\n",
    "def stemmer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    listy = []\n",
    "    for word in TextBlob(text).words:\n",
    "        listy.append(stemmer.stem(word))\n",
    "    return listy\n",
    "\n",
    "def token_clean(text):\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
    "    text = text.lower().split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>...</th>\n",
       "      <th>sent_token</th>\n",
       "      <th>polarity_sentence</th>\n",
       "      <th>polarity_comment</th>\n",
       "      <th>polarity_comment_s</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>char_count_s</th>\n",
       "      <th>polarity_min</th>\n",
       "      <th>polarity_max</th>\n",
       "      <th>polarity_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3a4c7758fad18de3</th>\n",
       "      <td>3a4c7758fad18de3</td>\n",
       "      <td>, I hope your retarded kids get anal raped and...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ToSTObThInIH</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>[, I hope your retarded kids get anal raped an...</td>\n",
       "      <td>(-0.6, 1.0, -0.018750000000000003, [-0.4, -0.4...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>94</td>\n",
       "      <td>494</td>\n",
       "      <td>494</td>\n",
       "      <td>-0.6000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-0.018750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24d2b50726b67167</th>\n",
       "      <td>24d2b50726b67167</td>\n",
       "      <td>I am going to murder ZimZalaBim ST47 for being...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ToSTObThInIH</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>[I am going to murder ZimZalaBim ST47 for bein...</td>\n",
       "      <td>(-1.0, -1.0, -1.0, [-1.0])</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c586b7a2fd575b13</th>\n",
       "      <td>c586b7a2fd575b13</td>\n",
       "      <td>Shut up you asswipe, we don't care. I'll decap...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ToSTObThInIH</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>[Shut up you asswipe, we don't care., I'll dec...</td>\n",
       "      <td>(-0.2, 0.0, -0.07555555555555557, [0.0, -0.177...</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>24</td>\n",
       "      <td>3890</td>\n",
       "      <td>1000</td>\n",
       "      <td>-0.2000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.075556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77d84b1321c22d9a</th>\n",
       "      <td>77d84b1321c22d9a</td>\n",
       "      <td>LGBT \\n\\nyou little fuck , are you a fag , tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ToSTObThInIH</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>[LGBT \\n\\nyou little fuck , are you a fag , th...</td>\n",
       "      <td>(-0.17750000000000005, 0.0, -0.088750000000000...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56</td>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "      <td>-0.1775</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.088750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368c10281978876</th>\n",
       "      <td>1368c10281978876</td>\n",
       "      <td>You're a stupid cunt \\n\\nFuck you dumb arse, y...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ToSTObThInIH</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>[You're a stupid cunt \\n\\nFuck you dumb arse, ...</td>\n",
       "      <td>(-0.41250000000000003, -0.41250000000000003, -...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>-0.4125</td>\n",
       "      <td>-0.4125</td>\n",
       "      <td>-0.412500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  \\\n",
       "idx                                  \n",
       "3a4c7758fad18de3  3a4c7758fad18de3   \n",
       "24d2b50726b67167  24d2b50726b67167   \n",
       "c586b7a2fd575b13  c586b7a2fd575b13   \n",
       "77d84b1321c22d9a  77d84b1321c22d9a   \n",
       "1368c10281978876  1368c10281978876   \n",
       "\n",
       "                                                       comment_text  toxic  \\\n",
       "idx                                                                          \n",
       "3a4c7758fad18de3  , I hope your retarded kids get anal raped and...      1   \n",
       "24d2b50726b67167  I am going to murder ZimZalaBim ST47 for being...      1   \n",
       "c586b7a2fd575b13  Shut up you asswipe, we don't care. I'll decap...      1   \n",
       "77d84b1321c22d9a  LGBT \\n\\nyou little fuck , are you a fag , tha...      1   \n",
       "1368c10281978876  You're a stupid cunt \\n\\nFuck you dumb arse, y...      1   \n",
       "\n",
       "                  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "idx                                                                      \n",
       "3a4c7758fad18de3             1        1       1       1              1   \n",
       "24d2b50726b67167             1        1       1       1              1   \n",
       "c586b7a2fd575b13             1        1       1       1              1   \n",
       "77d84b1321c22d9a             1        1       1       1              1   \n",
       "1368c10281978876             1        1       1       1              1   \n",
       "\n",
       "                      category  rating      ...        \\\n",
       "idx                                         ...         \n",
       "3a4c7758fad18de3  ToSTObThInIH       6      ...         \n",
       "24d2b50726b67167  ToSTObThInIH       6      ...         \n",
       "c586b7a2fd575b13  ToSTObThInIH       6      ...         \n",
       "77d84b1321c22d9a  ToSTObThInIH       6      ...         \n",
       "1368c10281978876  ToSTObThInIH       6      ...         \n",
       "\n",
       "                                                         sent_token  \\\n",
       "idx                                                                   \n",
       "3a4c7758fad18de3  [, I hope your retarded kids get anal raped an...   \n",
       "24d2b50726b67167  [I am going to murder ZimZalaBim ST47 for bein...   \n",
       "c586b7a2fd575b13  [Shut up you asswipe, we don't care., I'll dec...   \n",
       "77d84b1321c22d9a  [LGBT \\n\\nyou little fuck , are you a fag , th...   \n",
       "1368c10281978876  [You're a stupid cunt \\n\\nFuck you dumb arse, ...   \n",
       "\n",
       "                                                  polarity_sentence  \\\n",
       "idx                                                                   \n",
       "3a4c7758fad18de3  (-0.6, 1.0, -0.018750000000000003, [-0.4, -0.4...   \n",
       "24d2b50726b67167                         (-1.0, -1.0, -1.0, [-1.0])   \n",
       "c586b7a2fd575b13  (-0.2, 0.0, -0.07555555555555557, [0.0, -0.177...   \n",
       "77d84b1321c22d9a  (-0.17750000000000005, 0.0, -0.088750000000000...   \n",
       "1368c10281978876  (-0.41250000000000003, -0.41250000000000003, -...   \n",
       "\n",
       "                 polarity_comment polarity_comment_s word_count char_count  \\\n",
       "idx                                                                          \n",
       "3a4c7758fad18de3             0.00               0.00         94        494   \n",
       "24d2b50726b67167             0.00               0.00         12         68   \n",
       "c586b7a2fd575b13            -0.25              -0.25         24       3890   \n",
       "77d84b1321c22d9a             0.00               0.00         56        280   \n",
       "1368c10281978876             0.00               0.00         59        278   \n",
       "\n",
       "                  char_count_s  polarity_min  polarity_max  polarity_mean  \n",
       "idx                                                                        \n",
       "3a4c7758fad18de3           494       -0.6000        1.0000      -0.018750  \n",
       "24d2b50726b67167            68       -1.0000       -1.0000      -1.000000  \n",
       "c586b7a2fd575b13          1000       -0.2000        0.0000      -0.075556  \n",
       "77d84b1321c22d9a           280       -0.1775        0.0000      -0.088750  \n",
       "1368c10281978876           278       -0.4125       -0.4125      -0.412500  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('../data/toxictrain.pkl')\n",
    "print(df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic                 1.529400e+04\n",
       "severe_toxic          1.595000e+03\n",
       "obscene               8.449000e+03\n",
       "threat                4.780000e+02\n",
       "insult                7.877000e+03\n",
       "identity_hate         1.405000e+03\n",
       "rating                3.509800e+04\n",
       "clean                 1.433460e+05\n",
       "polarity_comment      2.065111e+03\n",
       "polarity_comment_s    1.902005e+03\n",
       "word_count            1.055252e+07\n",
       "char_count            6.288266e+07\n",
       "char_count_s          5.053248e+07\n",
       "polarity_min         -1.595756e+04\n",
       "polarity_max          3.604874e+04\n",
       "polarity_mean         7.919310e+03\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sum(axis=0,numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
       "       'insult', 'identity_hate', 'category', 'rating', 'clean',\n",
       "       'comment_text_s', 'comment_text_f', 'token_clean', 'sent_token',\n",
       "       'polarity_sentence', 'polarity_comment', 'polarity_comment_s',\n",
       "       'word_count', 'char_count', 'char_count_s', 'polarity_min',\n",
       "       'polarity_max', 'polarity_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143346, 24) (15294, 24) (1595, 24) (8449, 24) (478, 24) (7877, 24) (1405, 24)\n",
      "(255346, 24)\n"
     ]
    }
   ],
   "source": [
    "df_Cl = df[df.rating == 0]\n",
    "df_To = df[df.toxic == 1]\n",
    "df_ST = df[df.severe_toxic == 1]\n",
    "df_Ob = df[df.obscene == 1]\n",
    "df_Th = df[df.threat == 1]\n",
    "df_In = df[df.insult == 1]\n",
    "df_IH = df[df.identity_hate == 1]\n",
    "print(df_Cl.shape, df_To.shape,df_ST.shape,df_Ob.shape,df_Th.shape,df_In.shape,df_IH.shape)\n",
    "\n",
    "df_STu = resample(df_ST, replace=True, n_samples=24000)\n",
    "df_Obu = resample(df_Ob, replace=True, n_samples=20000)\n",
    "df_Thu = resample(df_Th, replace=True, n_samples=24000)\n",
    "df_Inu = resample(df_In, replace=True, n_samples=20000)\n",
    "df_IHu = resample(df_IH, replace=True, n_samples=24000)\n",
    "\n",
    "df = pd.concat([df_Cl, df_STu, df_Obu, df_Thu, df_Inu, df_IHu])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic                 1.063100e+05\n",
       "severe_toxic          4.210500e+04\n",
       "obscene               9.122900e+04\n",
       "threat                2.872900e+04\n",
       "insult                9.062100e+04\n",
       "identity_hate         3.904400e+04\n",
       "rating                3.980380e+05\n",
       "clean                 1.433460e+05\n",
       "polarity_comment      2.732151e+03\n",
       "polarity_comment_s    2.527520e+03\n",
       "word_count            1.602646e+07\n",
       "char_count            9.508330e+07\n",
       "char_count_s          7.060831e+07\n",
       "polarity_min         -4.494172e+04\n",
       "polarity_max          3.705951e+04\n",
       "polarity_mean        -5.361416e+03\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sum(axis=0,numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,) (100000,)\n"
     ]
    }
   ],
   "source": [
    "df1 = shuffle(df)[-100000:]\n",
    "# df1 = df[['comment_text_s','toxic']]\n",
    "X = df1['comment_text_s']\n",
    "y = df1['toxic']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('pipe', RandomForestClassifier(max_features='auto', n_estimators=1000))])\n",
    "pipe.fit(X_train,y_train) \n",
    "y_pred = pipe.predict(X_test)\n",
    "print(pipe.predict_proba())\n",
    "print(np.mean(y_pred == y_test))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(data,model):\n",
    "    model_dict = {}\n",
    "    categories = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "    for cat in categories:\n",
    "        X = data['comment_text']\n",
    "        y = data[cat]\n",
    "        \n",
    "        pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('pipe', model)])\n",
    "        pipe.fit(X_train,y_train) \n",
    "        y_pred = pipe.predict(X_test)\n",
    "        pp = pipe.predict_proba()\n",
    "        auc = roc_auc_score(y_test, y_pred) \n",
    "        mn = np.mean(y_pred == y_test)\n",
    "        cm = confusion_matrix(y_test,y_pred)\n",
    "        cr = classification_report(y_test,y_pred)   \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        p_1 = precision_score(y_test, y_pred ,pos_label=1)\n",
    "        p_0 = precision_score(y_test, y_pred ,pos_label=0)\n",
    "        r_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "        r_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "        f1_1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "        f1_0 = f1_score(y_test, y_pred, pos_label=0)\n",
    "        model_dict[cat] = {\"Probability\":pp, \"AUC\":auc, \"Mean\":mn,\"Accuracy\"acc,\n",
    "                           \"Precision_1\"p_1,\"Precision_0\":p_0,\"Recall_1\":r_1,\n",
    "                           \"Recal_0\":r_0,\"F1_1\":f1_1,\"F1_0\":f1_0} \n",
    "        print(cm, cr)\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(max_features='auto', n_estimators=1000)\n",
    "run_model(df,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(dataframe, model):\n",
    "    categories = ['clean','toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "    cat_dict = {}\n",
    "    \n",
    "    \n",
    "    for cat in categories:\n",
    "        df = dataframe[dataframe[cat]==1]\n",
    "        X = df['comment_text_s']\n",
    "        y = pd.cut(df.rating, bins=7, labels=list(range(7)))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42, stratify=y)\n",
    "        X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "\n",
    "        pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                             ('tfidf', TfidfTransformer()),\n",
    "                             ('to_dense', DenseTransformer()),\n",
    "                             ('pipe', model)])\n",
    "        pipe.fit(X_train,y_train.ravel()) # added .ravel() due to feature names mismatch\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision_1 = precision_score(y_test, y_pred ,pos_label=1, average=None)\n",
    "        precision_0 = precision_score(y_test, y_pred ,pos_label=0, average=None)\n",
    "        recall_1 = recall_score(y_test, y_pred, pos_label=1, average=None)\n",
    "        recall_0 = recall_score(y_test, y_pred, pos_label=0, average=None)\n",
    "        f1_1 = f1_score(y_test, y_pred, pos_label=1, average=None)\n",
    "        f1_0 = f1_score(y_test, y_pred, pos_label=0, average=None)\n",
    "        auc = roc_auc_score(y_test, y_pred)        \n",
    "        \n",
    "        mn = np.mean(y_pred == y_test)\n",
    "        cm = confusion_matrix(y_test,y_pred)\n",
    "        cr = classification_report(y_test,y_pred)\n",
    "        cat_dict[cat] = {\"accuracy\": accuracy, \"precision_1\": precision_1, \n",
    "                         \"precision_0\": precision_0, \"recall_1\": recall_1, \n",
    "                         \"recall_0\": recall_0, \"f1_1\": f1_1, \"f1_0\": f1_0, \n",
    "                         \"mean\":mn,\"confusion_matrix\":cm,\"class_report\":cr} # \"auc\": auc,\n",
    "    \n",
    "    cat_dict = pd.DataFrame(cat_dict)\n",
    "#     cat_dict = cat_dict.copy()\n",
    "    cat_dict['average'] = cat_dict.mean(numeric_only=True, axis=1)\n",
    "    return cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tm = shuffle(df)[-5000:]\n",
    "xgb = test_model(df_tm, XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.7,\n",
    "                                       gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=6,\n",
    "                                       min_child_weight=11, missing=-999, n_estimators=500, nthread=4,\n",
    "                                       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
    "                                       scale_pos_weight=1, seed=0, silent=1, subsample=0.8))\n",
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xgb = xgb.copy()\n",
    "# xgb['average'] = xgb.mean(numeric_only=True, axis=1)\n",
    "# xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_rf = shuffle(df)[-5000:]\n",
    "rf = test_model(df_rf, RandomForestClassifier(max_features='auto',n_estimators=1000))\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_gb = shuffle(df)[-5000:]\n",
    "gb = test_model(df_gb, GradientBoostingClassifier(n_estimators=500))\n",
    "gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_nbm = shuffle(df)[-5000:]\n",
    "# nbm = test_model(df_nbm, MultinomialNB(alpha=0))\n",
    "# nbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_XGB = shuffle(df)[-5000:]\n",
    "\n",
    "# categories = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "# cat_XGB = {}\n",
    "# X = df_XGB['comment_text_s']\n",
    "\n",
    "# for cat in categories:\n",
    "#     y = df_XGB[cat]\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42, stratify=y)\n",
    "#     X_train, X_test = np.array(X_train), np.array(X_test)\n",
    "\n",
    "#     pipe = Pipeline([('vect', CountVectorizer()),\n",
    "#                          ('tfidf', TfidfTransformer()),\n",
    "#                          ('to_dense', DenseTransformer()),\n",
    "#                          ('pipe', XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.7,\n",
    "#                                    gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=6,\n",
    "#                                    min_child_weight=11, missing=-999, n_estimators=500, nthread=4,\n",
    "#                                    objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
    "#                                    scale_pos_weight=1, seed=0, silent=1, subsample=0.8))])\n",
    "#     pipe.fit(X_train,y_train.ravel()) # added .ravel() due to feature names mismatch\n",
    "#     y_pred = pipe.predict(X_test) \n",
    "#     mn = np.mean(y_pred == y_test)\n",
    "#     cm = confusion_matrix(y_test,y_pred)\n",
    "#     cr = classification_report(y_test,y_pred)\n",
    "#     cat_XGB[cat] = mn, cm, cr\n",
    "# cat_XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cat_XGB_df = pd.DataFrame(cat_XGB).T\n",
    "# cat_XGB_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pipe = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('pipe', SVC(C=10,gamma=0.001,probability=True))])\n",
    "# pipe.fit(X_train,y_train) \n",
    "# y_pred = pipe.predict(X_test)\n",
    "# print(np.mean(y_pred == y_test))\n",
    "# print(confusion_matrix(y_test,y_pred))\n",
    "# print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\t==============================\n",
      "\tAccuracy: 0.801266666667\n",
      "\tAUC: 0.934563523665\n",
      "\n",
      "\n",
      "[[1585    0    1   34    2    0    0]\n",
      " [  27    6    0    1    0    0    0]\n",
      " [  86    0   71   10   24    0    0]\n",
      " [ 141    0    6  100   91    1    0]\n",
      " [  87    0    3   24  430    7    0]\n",
      " [  10    0    0    3   32  186    0]\n",
      " [   5    0    0    0    1    0   26]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.98      0.89      1622\n",
      "          1       1.00      0.18      0.30        34\n",
      "          2       0.88      0.37      0.52       191\n",
      "          3       0.58      0.29      0.39       339\n",
      "          4       0.74      0.78      0.76       551\n",
      "          5       0.96      0.81      0.88       231\n",
      "          6       1.00      0.81      0.90        32\n",
      "\n",
      "avg / total       0.79      0.80      0.78      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# Define models to test\n",
    "model_list = [['RandomForest', RandomForestClassifier(max_features='auto',n_estimators=1000)], \n",
    "                ['GradientBoost', GradientBoostingClassifier(n_estimators=500)],\n",
    "                ['XGBoost', XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.7,\n",
    "                       gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=6,\n",
    "                       min_child_weight=11, missing=-999, n_estimators=500, nthread=4,\n",
    "                       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
    "                       scale_pos_weight=1, seed=0, silent=1, subsample=0.8)]] \n",
    "\n",
    "# Calculate metrics for each model\n",
    "roc = {}\n",
    "results_dict = {}\n",
    "for model in model_list:\n",
    "    \n",
    "    df_A = shuffle(df)[-10000:]\n",
    "    X = df_A['comment_text_s']\n",
    "    y = pd.cut(df_A.rating, bins=7, labels=list(range(7)))\n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     y = mlb.fit_transform(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42, stratify=y)\n",
    "    \n",
    "    model_name = model[0]\n",
    "    model = model[1]\n",
    "    pipe = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),\n",
    "                     ('to_dense', DenseTransformer()), ('pipe', model)])\n",
    "\n",
    "    accuracy = []\n",
    "    precision_1 = []\n",
    "    precision_0 = []\n",
    "    recall_1 = []\n",
    "    recall_0 = []\n",
    "    f1_1 = []\n",
    "    f1_0 = []\n",
    "#     auc = []\n",
    "        \n",
    "    # Perform K-Fold CV and calculate metrics for each fold\n",
    "    kf = KFold(5, random_state=42, shuffle=True) \n",
    "    for train_idx, test_idx in kf.split(X, y=y):\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        precision_1.append(precision_score(y_test, y_pred ,pos_label=1, average=None))\n",
    "        precision_0.append(precision_score(y_test, y_pred ,pos_label=0, average=None))\n",
    "        recall_1.append(recall_score(y_test, y_pred, pos_label=1, average=None))\n",
    "        recall_0.append(recall_score(y_test, y_pred, pos_label=0, average=None))\n",
    "        f1_1.append(f1_score(y_test, y_pred, pos_label=1, average=None))\n",
    "        f1_0.append(f1_score(y_test, y_pred, pos_label=0, average=None))\n",
    "#         auc.append(roc_auc_score(y_test, y_pred, average='macro'))\n",
    "        \n",
    "    # Calculate mean metric across K-folds\n",
    "    mean_accuracy = np.mean(accuracy)\n",
    "    mean_precision_1 = np.mean(precision_1)\n",
    "    mean_precision_0 = np.mean(precision_0)\n",
    "    mean_recall_1 = np.mean(recall_1)\n",
    "    mean_recall_0 = np.mean(recall_0)\n",
    "    mean_f1_1 = np.mean(f1_1)\n",
    "    mean_f1_0 = np.mean(f1_0)\n",
    "#     mean_auc = np.mean(auc)\n",
    "    \n",
    "    # Capture TPR and FPR from last fold for plotting\n",
    "    y_score = pipe.predict_proba(X_test)[:,1]\n",
    "#     roc[model_name] = roc_curve(y_test, y_score), mean_auc\n",
    "    results_dict[model_name] = {\"accuracy\": mean_accuracy, \"precision_s\": mean_precision_1, \n",
    "                                \"precision_f\": mean_precision_0, \"recall_s\": mean_recall_1, \n",
    "                                \"recall_f\": mean_recall_0, \"f1_s\": mean_f1_1, \"f1_f\": mean_f1_0} # \"auc\": mean_auc\n",
    "    \n",
    "    # Print formatted results\n",
    "    print(model)\n",
    "    print('\\t==============================')\n",
    "    print('\\tAccuracy:', mean_accuracy)\n",
    "    print('\\tAUC:', mean_auc)\n",
    "    print('\\n')\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curve from the last K-Fold split\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot 50-50 Line\n",
    "ax.plot([0,1],[0,1], ls='--', color='k', label='50-50')\n",
    "\n",
    "# Plot Classifier ROC Curves\n",
    "for key, value in roc.items():\n",
    "    label = '{}, AUC: {}%'.format(key, round(100*value[1],1))\n",
    "    ax.plot(roc[key][0][0], roc[key][0][1], label=label)\n",
    "    \n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR')\n",
    "ax.set_title('ROC Curve - All Models',fontweight='bold',fontsize=15)\n",
    "ax.legend(loc='best')\n",
    "plt.savefig('../charts/toxic_roc_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rd = pd.DataFrame(results_dict).T\n",
    "rd = rd.apply(lambda x: round(100*x,1).astype(str) + \"%\")\n",
    "rd = rd.sort_values(['auc'],ascending=[False])\n",
    "rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
