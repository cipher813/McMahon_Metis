{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Generator LSTM Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cipher000/anaconda3/envs/magenta/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \n",
      "Music21 v.4 is the last version that will support Python 2.\n",
      "Please start using Python 3 instead.\n",
      "\n",
      "Set music21.environment.UserSettings()['warnings'] = 0\n",
      "to disable this message.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from IPython.display import Audio\n",
    "from intervaltree import Interval, IntervalTree\n",
    "\n",
    "# from scipy.fftpack import fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_file = '../data/notes'\n",
    "midi_files = '../data/Bach/*.mid'\n",
    "# midi_files = \"../midi_songs/*.mid\"\n",
    "# MusicNet_file = '/media/cipher000/DATA/Dropbox/Programming/MusicNet/Dataset/musicnet.npz'\n",
    "\n",
    "sequence_length = 100 # the lstm will predict the next note based on the last set of notes heard\n",
    "node1 = 512\n",
    "node2 = 256\n",
    "drop = 0.3\n",
    "epochs = 10 # 200\n",
    "batch_size = 64\n",
    "notes_generated = 500\n",
    "\n",
    "fs = 44100      # samples/second\n",
    "d = 2048        # input dimensions\n",
    "m = 128         # number of notes\n",
    "features = 0    # first element of (X,Y) data tuple\n",
    "labels = 1      # second element of (X,Y) data tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Midi Files from MusicNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
    "    notes = []\n",
    "    notes_dict = {}\n",
    "    cnt = 0\n",
    "\n",
    "#     for file in train_data.files:\n",
    "    for file in glob.glob(midi_files):\n",
    "        print(file)\n",
    "        notes_per_file = []\n",
    "#         print(\"Type: {} Length: {} Contents: {}\".format(type(file),len(file),file))\n",
    "#         print(\"Y Type: {} Length: {} Contents: {}\".format(type(Y),len(Y),Y))\n",
    "\n",
    "        midi = converter.parse(file)\n",
    "#         print(type(midi))\n",
    "#         notes_to_parse = None\n",
    "\n",
    "        parts = instrument.partitionByInstrument(midi)\n",
    "#         print(type(parts))\n",
    "        if parts: # file has instrument parts\n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "        else: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "        \n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "                notes_per_file.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                to_append = '.'.join(str(n) for n in element.normalOrder)\n",
    "                notes.append(to_append)\n",
    "                notes_per_file.append(to_append)\n",
    "        notes_dict[file] = notes_per_file\n",
    "        cnt +=1\n",
    "    with open(notes_file, 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "    print(\"{} midi files and {} notes\".format(cnt,type(notes)))\n",
    "    return notes,cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Bach/B0304.mid\n",
      "../data/Bach/B0101.mid\n",
      "../data/Bach/B0102.mid\n",
      "../data/Bach/B0103.mid\n",
      "../data/Bach/B0104.mid\n",
      "../data/Bach/B0105.mid\n",
      "../data/Bach/B0106.mid\n",
      "../data/Bach/B0201.mid\n",
      "../data/Bach/B0202.mid\n",
      "../data/Bach/B0203.mid\n",
      "../data/Bach/B0204.mid\n",
      "../data/Bach/B0205.mid\n",
      "../data/Bach/B0206.mid\n",
      "../data/Bach/B0301.mid\n",
      "../data/Bach/B0302.mid\n",
      "../data/Bach/B0303.mid\n",
      "../data/Bach/B0305.mid\n",
      "../data/Bach/B0306.mid\n",
      "../data/Bach/B0401.mid\n",
      "../data/Bach/B0402.mid\n",
      "../data/Bach/B0403.mid\n",
      "../data/Bach/B0404.mid\n",
      "../data/Bach/B0405.mid\n",
      "../data/Bach/B0406.mid\n",
      "../data/Bach/B0407.mid\n",
      "../data/Bach/B0407a.mid\n",
      "../data/Bach/B0408.mid\n",
      "../data/Bach/B0501.mid\n",
      "../data/Bach/B0502.mid\n",
      "../data/Bach/B0503.mid\n",
      "../data/Bach/B0504.mid\n",
      "../data/Bach/B0505.mid\n",
      "../data/Bach/B0506.mid\n",
      "../data/Bach/B0507.mid\n",
      "../data/Bach/B0601.mid\n",
      "../data/Bach/B0602.mid\n",
      "../data/Bach/B0603.mid\n",
      "../data/Bach/B0604.mid\n",
      "../data/Bach/B0605.mid\n",
      "../data/Bach/B0606.mid\n",
      "40 midi files and <type 'list'> notes\n"
     ]
    }
   ],
   "source": [
    "notes, cnt = get_notes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17956"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "     # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "#         print(\"Sequence in: {}\".format(sequence_in))\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "#         print(\"Sequence out: {}\".format(sequence_out))\n",
    "        input_append = [note_to_int[char] for char in sequence_in]\n",
    "        network_input.append(input_append)\n",
    "#         print(\"Network input: {}\".format(input_append))\n",
    "        output_append = note_to_int[sequence_out]\n",
    "        network_output.append(output_append)\n",
    "#         print(\"Network output: {}\".format(output_append))\n",
    "\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "    print(\"Number of Patterns: {}\".format(n_patterns))\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    print(\"Network input reshaped: {}\".format(network_input.shape))\n",
    "    \n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    print(\"Network input over float n_vocab: {}\".format(network_input.shape))\n",
    "\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "    print(\"Network output: {}\".format(network_output.shape))\n",
    "\n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Patterns: 17856\n",
      "Network input reshaped: (17856, 100, 1)\n",
      "Network input over float n_vocab: (17856, 100, 1)\n",
      "Network output: (17856, 89)\n"
     ]
    }
   ],
   "source": [
    "n_vocab = len(set(notes))\n",
    "\n",
    "network_input, network_output = prepare_sequences(notes, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab,weights_file=None):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(node1,input_shape=(network_input.shape[1], network_input.shape[2]),return_sequences=True))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(LSTM(node1, return_sequences=True))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(LSTM(node1))\n",
    "    model.add(Dense(node2))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    # Load the weights to each node\n",
    "    try:\n",
    "        model.load_weights(weights_file)\n",
    "        print(\"Weights file loaded: {}\".format(weights_file))\n",
    "    except Exception as e:\n",
    "        print(\"Training mode (No weights file)\")\n",
    "#         print(e)\n",
    "        pass\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mode (No weights file)\n"
     ]
    }
   ],
   "source": [
    "model = create_network(network_input, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, network_input, network_output):\n",
    "    \"\"\" train the neural network \"\"\"\n",
    "\n",
    "    filepath = \"../weights/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "    weights_file = filepath\n",
    "    print(\"Weights File Update to {}\".format(weights_file))\n",
    "#     print(filepath)\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    model.fit(network_input, network_output, epochs=epochs, batch_size=batch_size, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights File Update to weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\n",
      "Epoch 1/10\n",
      "17856/17856 [==============================] - 2688s 151ms/step - loss: 3.5590\n",
      "Epoch 2/10\n",
      "17856/17856 [==============================] - 911s 51ms/step - loss: 3.4361\n",
      "Epoch 3/10\n",
      "17856/17856 [==============================] - 891s 50ms/step - loss: 3.4259\n",
      "Epoch 4/10\n",
      "17856/17856 [==============================] - 884s 49ms/step - loss: 3.4251\n",
      "Epoch 5/10\n",
      "17856/17856 [==============================] - 899s 50ms/step - loss: 3.4209\n",
      "Epoch 6/10\n",
      "17856/17856 [==============================] - 921s 52ms/step - loss: 3.4203\n",
      "Epoch 7/10\n",
      "17856/17856 [==============================] - 956s 54ms/step - loss: 3.4189\n",
      "Epoch 8/10\n",
      "17856/17856 [==============================] - 905s 51ms/step - loss: 3.4328\n",
      "Epoch 9/10\n",
      "17856/17856 [==============================] - 901s 50ms/step - loss: 3.4192\n",
      "Epoch 10/10\n",
      "17856/17856 [==============================] - 875s 49ms/step - loss: 3.4174\n"
     ]
    }
   ],
   "source": [
    "train(model, network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Create Midi File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = '../weights/weights.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of notes: 17956\n",
      "Length of Pitch Names: 89\n",
      "Length of N Vocab: 89\n"
     ]
    }
   ],
   "source": [
    "#load the notes used to train the model\n",
    "with open(notes_file, 'rb') as filepath:\n",
    "    notes = pickle.load(filepath)\n",
    "print(\"Length of notes: {}\".format(len(notes)))\n",
    "# Get all pitch names\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "print(\"Length of Pitch Names: {}\".format(len(pitchnames)))\n",
    "\n",
    "# Get all pitch names\n",
    "n_vocab = len(set(notes))\n",
    "print(\"Length of N Vocab: {}\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, pitchnames, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "#     sequence_length = 100\n",
    "    network_input = []\n",
    "    output = []\n",
    "    \n",
    "    # map between notes and integers and back\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    print(\"Note to Integer Dictionary Length: {}\".format(len(note_to_int)))\n",
    "\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        output.append(note_to_int[sequence_out])\n",
    "#         print(\"Preparation {}: {}\".format(i,note_to_int[sequence_out]))\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "    print(\"N Patterns Length: {}\".format(n_patterns))\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    normalized_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "    print(\"Network Input Shape: {}  Normalized Input Shape: {}\".format(len(network_input), len(normalized_input)))\n",
    "\n",
    "    return (network_input, normalized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note to Integer Dictionary Length: 89\n",
      "N Patterns Length: 17856\n",
      "Network Input Shape: 17856  Normalized Input Shape: 17856\n"
     ]
    }
   ],
   "source": [
    "network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mode (No weights file)\n"
     ]
    }
   ],
   "source": [
    "model = create_network(normalized_input, n_vocab,weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "    print(\"Start: {}\".format(start))\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    print(\"Int to Note Length: {}\".format(len(int_to_note)))\n",
    "    \n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "#     print(\"Pattern: {}  Prediction Output: {}\".format(pattern, prediction_output))\n",
    "\n",
    "    # generate 500 notes\n",
    "    for note_index in range(notes_generated):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "#         print(\"Pattern: {}\".format(pattern))\n",
    "#     print(\"Prediction Output: {}\".format(prediction_output))\n",
    "    print(\"Notes generated: {}\".format(notes_generated))\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 16309\n",
      "Int to Note Length: 89\n",
      "Notes generated: 500\n"
     ]
    }
   ],
   "source": [
    "prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "#     print(\"Prediction Output: {}\".format(prediction_output))\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "#             print(\"Pattern: {} New Chord: {}\".format(pattern, new_chord))\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "#             print(\"Pattern: {} New Note: {}\".format(pattern, new_note))\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    print(\"Midi Stream: {}\".format(midi_stream))\n",
    "    output_file = '../output/{}{}.mid'.format(weights_file,epochs)\n",
    "\n",
    "    midi_stream.write('midi', fp=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Midi Stream: <music21.stream.Stream 0x7fcd99235150>\n"
     ]
    }
   ],
   "source": [
    "create_midi(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model adapted from Towards Data Science blog [here](https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5)\n",
    "with accompanying \"Classical Piano Composer\" github repo [here](https://github.com/Skuldur/Classical-Piano-Composer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download MusicNet from http://homes.cs.washington.edu/~thickstn/musicnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
