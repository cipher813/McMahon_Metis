{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music VEA\n",
    "[Magenta](https://github.com/tensorflow/magenta/tree/master/magenta/models/music_vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cipher000/anaconda3/envs/magenta/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### music_vae_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xb but this version of numpy is 0xa",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xb but this version of numpy is 0xa"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid run directory: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c8b703ff4f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m   \u001b[0mconsole_entry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-c8b703ff4f92>\u001b[0m in \u001b[0;36mconsole_entry_point\u001b[0;34m()\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconsole_entry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cipher000/anaconda3/envs/magenta/lib/python2.7/site-packages/tensorflow/python/platform/app.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c8b703ff4f92>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_argv)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munused_argv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m   \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONFIG_MAP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c8b703ff4f92>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(config_map, tf_file_reader, file_reader)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \"\"\"\n\u001b[1;32m    257\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid run directory: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m   \u001b[0mrun_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m   \u001b[0mtrain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid run directory: None"
     ]
    }
   ],
   "source": [
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"MusicVAE training script.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "# internal imports\n",
    "import tensorflow as tf\n",
    "\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae import data\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    'master', '',\n",
    "    'The TensorFlow master to use.')\n",
    "flags.DEFINE_string(\n",
    "    'examples_path', None,\n",
    "    'Path to a TFRecord file of NoteSequence examples. Overrides the config.')\n",
    "flags.DEFINE_string(\n",
    "    'run_dir', None,\n",
    "    'Path where checkpoints and summary events will be located during '\n",
    "    'training and evaluation. Separate subdirectories `train` and `eval` '\n",
    "    'will be created within this directory.')\n",
    "flags.DEFINE_integer(\n",
    "    'num_steps', 200000,\n",
    "    'Number of training steps or `None` for infinite.')\n",
    "flags.DEFINE_integer(\n",
    "    'eval_num_batches', None,\n",
    "    'Number of batches to use during evaluation or `None` for all batches '\n",
    "    'in the data source.')\n",
    "flags.DEFINE_integer(\n",
    "    'checkpoints_to_keep', 100,\n",
    "    'Maximum number of checkpoints to keep in `train` mode or 0 for infinite.')\n",
    "flags.DEFINE_integer(\n",
    "    'keep_checkpoint_every_n_hours', 1,\n",
    "    'In addition to checkpoints_to_keep, keep a checkpoint every N hours.')\n",
    "flags.DEFINE_string(\n",
    "    'mode', 'train',\n",
    "    'Which mode to use (`train` or `eval`).')\n",
    "flags.DEFINE_string(\n",
    "    'config', '',\n",
    "    'The name of the config to use.')\n",
    "flags.DEFINE_string(\n",
    "    'hparams', '',\n",
    "    'A comma-separated list of `name=value` hyperparameter values to merge '\n",
    "    'with those in the config.')\n",
    "flags.DEFINE_integer(\n",
    "    'task', 0,\n",
    "    'The task number for this worker.')\n",
    "flags.DEFINE_integer(\n",
    "    'num_ps_tasks', 0,\n",
    "    'The number of parameter server tasks.')\n",
    "flags.DEFINE_integer(\n",
    "    'num_sync_workers', 0,\n",
    "    'The number of synchronized workers.')\n",
    "flags.DEFINE_integer(\n",
    "    'num_data_threads', 4,\n",
    "    'The number of data preprocessing threads.')\n",
    "flags.DEFINE_integer(\n",
    "    'prefetch_size', 4,\n",
    "    'How many batches to prefetch at the end of the data pipeline.')\n",
    "flags.DEFINE_string(\n",
    "    'eval_dir_suffix', '',\n",
    "    'Suffix to add to eval output directory.')\n",
    "flags.DEFINE_string(\n",
    "    'log', 'INFO',\n",
    "    'The threshold for what messages will be logged: '\n",
    "    'DEBUG, INFO, WARN, ERROR, or FATAL.')\n",
    "\n",
    "\n",
    "# Should not be called from within the graph to avoid redundant summaries.\n",
    "def _trial_summary(hparams, examples_path, output_dir):\n",
    "  \"\"\"Writes a tensorboard text summary of the trial.\"\"\"\n",
    "\n",
    "  examples_path_summary = tf.summary.text(\n",
    "      'examples_path', tf.constant(examples_path, name='examples_path'),\n",
    "      collections=[])\n",
    "\n",
    "  hparams_dict = hparams.values()\n",
    "\n",
    "  # Create a markdown table from hparams.\n",
    "  header = '| Key | Value |\\n| :--- | :--- |\\n'\n",
    "  keys = sorted(hparams_dict.keys())\n",
    "  lines = ['| %s | %s |' % (key, str(hparams_dict[key])) for key in keys]\n",
    "  hparams_table = header + '\\n'.join(lines) + '\\n'\n",
    "\n",
    "  hparam_summary = tf.summary.text(\n",
    "      'hparams', tf.constant(hparams_table, name='hparams'), collections=[])\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(output_dir, graph=sess.graph)\n",
    "    writer.add_summary(examples_path_summary.eval())\n",
    "    writer.add_summary(hparam_summary.eval())\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def _get_input_tensors(dataset, config):\n",
    "  \"\"\"Get input tensors from dataset.\"\"\"\n",
    "  batch_size = config.hparams.batch_size\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  (input_sequence, output_sequence, control_sequence,\n",
    "   sequence_length) = iterator.get_next()\n",
    "  input_sequence.set_shape(\n",
    "      [batch_size, None, config.data_converter.input_depth])\n",
    "  output_sequence.set_shape(\n",
    "      [batch_size, None, config.data_converter.output_depth])\n",
    "  if not config.data_converter.control_depth:\n",
    "    control_sequence = None\n",
    "  else:\n",
    "    control_sequence.set_shape(\n",
    "        [batch_size, None, config.data_converter.control_depth])\n",
    "  sequence_length.set_shape([batch_size] + sequence_length.shape[1:].as_list())\n",
    "\n",
    "  return {\n",
    "      'input_sequence': input_sequence,\n",
    "      'output_sequence': output_sequence,\n",
    "      'control_sequence': control_sequence,\n",
    "      'sequence_length': sequence_length\n",
    "  }\n",
    "\n",
    "\n",
    "def train(train_dir,\n",
    "          config,\n",
    "          dataset,\n",
    "          checkpoints_to_keep=5,\n",
    "          keep_checkpoint_every_n_hours=1,\n",
    "          num_steps=None,\n",
    "          master='',\n",
    "          num_sync_workers=0,\n",
    "          num_ps_tasks=0,\n",
    "          task=0):\n",
    "  \"\"\"Train loop.\"\"\"\n",
    "  tf.gfile.MakeDirs(train_dir)\n",
    "  is_chief = (task == 0)\n",
    "  if is_chief:\n",
    "    _trial_summary(config.hparams, config.train_examples_path, train_dir)\n",
    "  with tf.Graph().as_default():\n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "        num_ps_tasks, merge_devices=True)):\n",
    "\n",
    "      model = config.model\n",
    "      model.build(config.hparams,\n",
    "                  config.data_converter.output_depth,\n",
    "                  is_training=True)\n",
    "\n",
    "      optimizer = model.train(**_get_input_tensors(dataset, config))\n",
    "\n",
    "      hooks = []\n",
    "      if num_sync_workers:\n",
    "        optimizer = tf.train.SyncReplicasOptimizer(\n",
    "            optimizer,\n",
    "            num_sync_workers)\n",
    "        hooks.append(optimizer.make_session_run_hook(is_chief))\n",
    "\n",
    "      grads, var_list = zip(*optimizer.compute_gradients(model.loss))\n",
    "      global_norm = tf.global_norm(grads)\n",
    "      tf.summary.scalar('global_norm', global_norm)\n",
    "\n",
    "      if config.hparams.clip_mode == 'value':\n",
    "        g = config.hparams.grad_clip\n",
    "        clipped_grads = [tf.clip_by_value(grad, -g, g) for grad in grads]\n",
    "      elif config.hparams.clip_mode == 'global_norm':\n",
    "        clipped_grads = tf.cond(\n",
    "            global_norm < config.hparams.grad_norm_clip_to_zero,\n",
    "            lambda: tf.clip_by_global_norm(  # pylint:disable=g-long-lambda\n",
    "                grads, config.hparams.grad_clip, use_norm=global_norm)[0],\n",
    "            lambda: [tf.zeros(tf.shape(g)) for g in grads])\n",
    "      else:\n",
    "        raise ValueError(\n",
    "            'Unknown clip_mode: {}'.format(config.hparams.clip_mode))\n",
    "      train_op = optimizer.apply_gradients(\n",
    "          zip(clipped_grads, var_list), global_step=model.global_step,\n",
    "          name='train_step')\n",
    "\n",
    "      logging_dict = {'global_step': model.global_step,\n",
    "                      'loss': model.loss}\n",
    "\n",
    "      hooks.append(tf.train.LoggingTensorHook(logging_dict, every_n_iter=100))\n",
    "      if num_steps:\n",
    "        hooks.append(tf.train.StopAtStepHook(last_step=num_steps))\n",
    "\n",
    "      scaffold = tf.train.Scaffold(\n",
    "          saver=tf.train.Saver(\n",
    "              max_to_keep=checkpoints_to_keep,\n",
    "              keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours))\n",
    "      tf.contrib.training.train(\n",
    "          train_op=train_op,\n",
    "          logdir=train_dir,\n",
    "          scaffold=scaffold,\n",
    "          hooks=hooks,\n",
    "          save_checkpoint_secs=60,\n",
    "          master=master,\n",
    "          is_chief=is_chief)\n",
    "\n",
    "\n",
    "def evaluate(train_dir,\n",
    "             eval_dir,\n",
    "             config,\n",
    "             dataset,\n",
    "             num_batches,\n",
    "             master=''):\n",
    "  \"\"\"Evaluate the model repeatedly.\"\"\"\n",
    "  tf.gfile.MakeDirs(eval_dir)\n",
    "\n",
    "  _trial_summary(config.hparams, config.eval_examples_path, eval_dir)\n",
    "  with tf.Graph().as_default():\n",
    "    model = config.model\n",
    "    model.build(config.hparams,\n",
    "                config.data_converter.output_depth,\n",
    "                is_training=False)\n",
    "\n",
    "    eval_op = model.eval(\n",
    "        **_get_input_tensors(dataset.take(num_batches), config))\n",
    "\n",
    "    hooks = [\n",
    "        tf.contrib.training.StopAfterNEvalsHook(num_batches),\n",
    "        tf.contrib.training.SummaryAtEndHook(eval_dir)]\n",
    "    tf.contrib.training.evaluate_repeatedly(\n",
    "        train_dir,\n",
    "        eval_ops=eval_op,\n",
    "        hooks=hooks,\n",
    "        eval_interval_secs=60,\n",
    "        master=master)\n",
    "\n",
    "\n",
    "def run(config_map,\n",
    "        tf_file_reader=tf.data.TFRecordDataset,\n",
    "        file_reader=tf.python_io.tf_record_iterator):\n",
    "  \"\"\"Load model params, save config file and start trainer.\n",
    "\n",
    "  Args:\n",
    "    config_map: Dictionary mapping configuration name to Config object.\n",
    "    tf_file_reader: The tf.data.Dataset class to use for reading files.\n",
    "    file_reader: The Python reader to use for reading files.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if required flags are missing or invalid.\n",
    "  \"\"\"\n",
    "  if not FLAGS.run_dir:\n",
    "    raise ValueError('Invalid run directory: %s' % FLAGS.run_dir)\n",
    "  run_dir = os.path.expanduser(FLAGS.run_dir)\n",
    "  train_dir = os.path.join(run_dir, 'train')\n",
    "\n",
    "  if FLAGS.mode not in ['train', 'eval']:\n",
    "    raise ValueError('Invalid mode: %s' % FLAGS.mode)\n",
    "\n",
    "  if FLAGS.config not in config_map:\n",
    "    raise ValueError('Invalid config: %s' % FLAGS.config)\n",
    "  config = config_map[FLAGS.config]\n",
    "  if FLAGS.hparams:\n",
    "    config.hparams.parse(FLAGS.hparams)\n",
    "  config_update_map = {}\n",
    "  if FLAGS.examples_path:\n",
    "    config_update_map['%s_examples_path' % FLAGS.mode] = FLAGS.examples_path\n",
    "  config = configs.update_config(config, config_update_map)\n",
    "  if FLAGS.num_sync_workers:\n",
    "    config.hparams.batch_size //= FLAGS.num_sync_workers\n",
    "\n",
    "  if FLAGS.mode == 'train':\n",
    "    is_training = True\n",
    "  elif FLAGS.mode == 'eval':\n",
    "    is_training = False\n",
    "  else:\n",
    "    raise ValueError('Invalid mode: {}'.format(FLAGS.mode))\n",
    "\n",
    "  dataset = data.get_dataset(\n",
    "      config,\n",
    "      tf_file_reader=tf_file_reader,\n",
    "      num_threads=FLAGS.num_data_threads,\n",
    "      prefetch_size=FLAGS.prefetch_size,\n",
    "      is_training=is_training)\n",
    "\n",
    "  if is_training:\n",
    "    train(\n",
    "        train_dir,\n",
    "        config=config,\n",
    "        dataset=dataset,\n",
    "        checkpoints_to_keep=FLAGS.checkpoints_to_keep,\n",
    "        keep_checkpoint_every_n_hours=FLAGS.keep_checkpoint_every_n_hours,\n",
    "        num_steps=FLAGS.num_steps,\n",
    "        master=FLAGS.master,\n",
    "        num_sync_workers=FLAGS.num_sync_workers,\n",
    "        num_ps_tasks=FLAGS.num_ps_tasks,\n",
    "        task=FLAGS.task)\n",
    "  else:\n",
    "    num_batches = FLAGS.eval_num_batches or data.count_examples(\n",
    "        config.eval_examples_path,\n",
    "        config.data_converter,\n",
    "        file_reader) // config.hparams.batch_size\n",
    "    eval_dir = os.path.join(run_dir, 'eval' + FLAGS.eval_dir_suffix)\n",
    "    evaluate(\n",
    "        train_dir,\n",
    "        eval_dir,\n",
    "        config=config,\n",
    "        dataset=dataset,\n",
    "        num_batches=num_batches,\n",
    "        master=FLAGS.master)\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "  tf.logging.set_verbosity(FLAGS.log)\n",
    "  run(configs.CONFIG_MAP)\n",
    "\n",
    "\n",
    "def console_entry_point():\n",
    "  tf.app.run(main)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  console_entry_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Configurations for MusicVAE models.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "\n",
    "from magenta.common import merge_hparams\n",
    "from magenta.models.music_vae import data\n",
    "from magenta.models.music_vae import lstm_models\n",
    "from magenta.models.music_vae.base_model import MusicVAE\n",
    "from tensorflow.contrib.training import HParams\n",
    "\n",
    "\n",
    "class Config(collections.namedtuple(\n",
    "    'Config',\n",
    "    ['model', 'hparams', 'note_sequence_augmenter', 'data_converter',\n",
    "     'train_examples_path', 'eval_examples_path'])):\n",
    "\n",
    "  def values(self):\n",
    "    return self._asdict()\n",
    "\n",
    "\n",
    "def update_config(config, update_dict):\n",
    "  config_dict = config.values()\n",
    "  config_dict.update(update_dict)\n",
    "  return Config(**config_dict)\n",
    "\n",
    "\n",
    "CONFIG_MAP = {}\n",
    "\n",
    "\n",
    "# Melody\n",
    "CONFIG_MAP['cat-mel_2bar_small'] = Config(\n",
    "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n",
    "                   lstm_models.CategoricalLstmDecoder()),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=512,\n",
    "            max_seq_len=32,  # 2 bars w/ 16 steps per bar\n",
    "            z_size=256,\n",
    "            enc_rnn_size=[512],\n",
    "            dec_rnn_size=[256, 256],\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=data.OneHotMelodyConverter(\n",
    "        valid_programs=data.MEL_PROGRAMS,\n",
    "        skip_polyphony=False,\n",
    "        max_bars=100,  # Truncate long melodies before slicing.\n",
    "        slice_bars=2,\n",
    "        steps_per_quarter=4),\n",
    "    train_examples_path=None,\n",
    "    eval_examples_path=None,\n",
    ")\n",
    "\n",
    "CONFIG_MAP['cat-mel_2bar_big'] = Config(\n",
    "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n",
    "                   lstm_models.CategoricalLstmDecoder()),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=512,\n",
    "            max_seq_len=32,  # 2 bars w/ 16 steps per bar\n",
    "            z_size=512,\n",
    "            enc_rnn_size=[2048],\n",
    "            dec_rnn_size=[2048, 2048, 2048],\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=data.OneHotMelodyConverter(\n",
    "        valid_programs=data.MEL_PROGRAMS,\n",
    "        skip_polyphony=False,\n",
    "        max_bars=100,  # Truncate long melodies before slicing.\n",
    "        slice_bars=2,\n",
    "        steps_per_quarter=4),\n",
    "    train_examples_path=None,\n",
    "    eval_examples_path=None,\n",
    ")\n",
    "\n",
    "# Drums\n",
    "CONFIG_MAP['cat-drums_2bar_small'] = Config(\n",
    "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n",
    "                   lstm_models.CategoricalLstmDecoder()),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=512,\n",
    "            max_seq_len=32,  # 2 bars w/ 16 steps per bar\n",
    "            z_size=256,\n",
    "            enc_rnn_size=[512],\n",
    "            dec_rnn_size=[256, 256],\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=data.DrumsConverter(\n",
    "        max_bars=100,  # Truncate long drum sequences before slicing.\n",
    "        slice_bars=2,\n",
    "        steps_per_quarter=4,\n",
    "        roll_input=True),\n",
    "    train_examples_path=None,\n",
    "    eval_examples_path=None,\n",
    ")\n",
    "\n",
    "CONFIG_MAP['cat-drums_2bar_big'] = Config(\n",
    "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n",
    "                   lstm_models.CategoricalLstmDecoder()),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=512,\n",
    "            max_seq_len=32,  # 2 bars w/ 16 steps per bar\n",
    "            z_size=512,\n",
    "            enc_rnn_size=[2048],\n",
    "            dec_rnn_size=[2048, 2048, 2048],\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=data.DrumsConverter(\n",
    "        max_bars=100,  # Truncate long drum sequences before slicing.\n",
    "        slice_bars=2,\n",
    "        steps_per_quarter=4,\n",
    "        roll_input=True),\n",
    "    train_examples_path=None,\n",
    "    eval_examples_path=None,\n",
    ")\n",
    "\n",
    "CONFIG_MAP['nade-drums_2bar_reduced'] = Config(\n",
    "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n",
    "                   lstm_models.MultiLabelRnnNadeDecoder()),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=512,\n",
    "            max_seq_len=32,  # 2 bars w/ 16 steps per bar\n",
    "            z_size=256,\n",
    "            enc_rnn_size=[1024],\n",
    "            dec_rnn_size=[512, 512],\n",
    "            nade_num_hidden=128,\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=data.DrumsConverter(\n",
    "        max_bars=100,  # Truncate long drum sequences before slicing.\n",
    "        slice_bars=2,\n",
    "        steps_per_quarter=4,\n",
    "        roll_input=True,\n",
    "        roll_output=True),\n",
    "    train_examples_path=None,\n",
    "    eval_examples_path=None,\n",
    ")\n",
    "\n",
    "CONFIG_MAP['nade-drums_2bar_full'] = Config(\n",
    "    model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n",
    "                   lstm_models.MultiLabelRnnNadeDecoder()),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=512,\n",
    "            max_seq_len=32,  # 2 bars w/ 16 steps per bar\n",
    "            z_size=256,\n",
    "            enc_rnn_size=[1024],\n",
    "            dec_rnn_size=[512, 512],\n",
    "            nade_num_hidden=128,\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=data.DrumsConverter(\n",
    "        max_bars=100,  # Truncate long drum sequences before slicing.\n",
    "        pitch_classes=data.FULL_DRUM_PITCH_CLASSES,\n",
    "        slice_bars=2,\n",
    "        steps_per_quarter=4,\n",
    "        roll_input=True,\n",
    "        roll_output=True),\n",
    "    train_examples_path=None,\n",
    "    eval_examples_path=None,\n",
    ")\n",
    "\n",
    "# Trio Models\n",
    "trio_16bar_converter = data.TrioConverter(\n",
    "    steps_per_quarter=4,\n",
    "    slice_bars=16,\n",
    "    gap_bars=2)\n",
    "\n",
    "CONFIG_MAP['flat-trio_16bar'] = Config(\n",
    "    model=MusicVAE(\n",
    "        lstm_models.BidirectionalLstmEncoder(),\n",
    "        lstm_models.MultiOutCategoricalLstmDecoder(\n",
    "            output_depths=[\n",
    "                90,  # melody\n",
    "                90,  # bass\n",
    "                512,  # drums\n",
    "            ])),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=256,\n",
    "            max_seq_len=256,\n",
    "            z_size=512,\n",
    "            enc_rnn_size=[2048, 2048],\n",
    "            dec_rnn_size=[2048, 2048, 2048],\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=trio_16bar_converter,\n",
    "    train_examples_path=None,\n",
    "    eval_examples_path=None,\n",
    ")\n",
    "\n",
    "CONFIG_MAP['hierdec-trio_16bar'] = Config(\n",
    "    model=MusicVAE(\n",
    "        lstm_models.BidirectionalLstmEncoder(),\n",
    "        lstm_models.HierarchicalLstmDecoder(\n",
    "            lstm_models.SplitMultiOutLstmDecoder(\n",
    "                core_decoders=[\n",
    "                    lstm_models.CategoricalLstmDecoder(),\n",
    "                    lstm_models.CategoricalLstmDecoder(),\n",
    "                    lstm_models.CategoricalLstmDecoder()],\n",
    "                output_depths=[\n",
    "                    90,  # melody\n",
    "                    90,  # bass\n",
    "                    512,  # drums\n",
    "                ]),\n",
    "            level_lengths=[16, 16],\n",
    "            disable_autoregression=True)),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=256,\n",
    "            max_seq_len=256,\n",
    "            z_size=512,\n",
    "            enc_rnn_size=[2048, 2048],\n",
    "            dec_rnn_size=[1024, 1024],\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=trio_16bar_converter,\n",
    "    train_examples_path=None,\n",
    "    eval_examples_path=None,\n",
    ")\n",
    "\n",
    "# 16-bar Melody Models\n",
    "mel_16bar_converter = data.OneHotMelodyConverter(\n",
    "    skip_polyphony=False,\n",
    "    max_bars=100,  # Truncate long melodies before slicing.\n",
    "    slice_bars=16,\n",
    "    steps_per_quarter=4)\n",
    "\n",
    "CONFIG_MAP['flat-mel_16bar'] = Config(\n",
    "    model=MusicVAE(\n",
    "        lstm_models.BidirectionalLstmEncoder(),\n",
    "        lstm_models.CategoricalLstmDecoder()),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=512,\n",
    "            max_seq_len=256,\n",
    "            z_size=512,\n",
    "            enc_rnn_size=[2048, 2048],\n",
    "            dec_rnn_size=[2048, 2048, 2048],\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=mel_16bar_converter,\n",
    "    train_examples_path=None,\n",
    "    eval_examples_path=None,\n",
    ")\n",
    "\n",
    "CONFIG_MAP['hierdec-mel_16bar'] = Config(\n",
    "    model=MusicVAE(\n",
    "        lstm_models.BidirectionalLstmEncoder(),\n",
    "        lstm_models.HierarchicalLstmDecoder(\n",
    "            lstm_models.CategoricalLstmDecoder(),\n",
    "            level_lengths=[16, 16],\n",
    "            disable_autoregression=True)),\n",
    "    hparams=merge_hparams(\n",
    "        lstm_models.get_default_hparams(),\n",
    "        HParams(\n",
    "            batch_size=512,\n",
    "            max_seq_len=256,\n",
    "            z_size=512,\n",
    "            enc_rnn_size=[2048, 2048],\n",
    "            dec_rnn_size=[1024, 1024],\n",
    "        )),\n",
    "    note_sequence_augmenter=None,\n",
    "    data_converter=mel_16bar_converter,\n",
    "    train_examples_path=None,\n",
    "    eval_examples_path=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"MusicVAE data library.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import abc\n",
    "import collections\n",
    "import copy\n",
    "import functools\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "# internal imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import magenta.music as mm\n",
    "from magenta.music import chord_symbols_lib\n",
    "from magenta.music import chords_lib\n",
    "from magenta.music import drums_encoder_decoder\n",
    "from magenta.music import sequences_lib\n",
    "from magenta.protobuf import music_pb2\n",
    "\n",
    "PIANO_MIN_MIDI_PITCH = 21\n",
    "PIANO_MAX_MIDI_PITCH = 108\n",
    "MIN_MIDI_PITCH = 0\n",
    "MAX_MIDI_PITCH = 127\n",
    "MIDI_PITCHES = 128\n",
    "\n",
    "MAX_INSTRUMENT_NUMBER = 127\n",
    "\n",
    "MEL_PROGRAMS = range(0, 32)  # piano, chromatic percussion, organ, guitar\n",
    "BASS_PROGRAMS = range(32, 40)\n",
    "ELECTRIC_BASS_PROGRAM = 33\n",
    "\n",
    "REDUCED_DRUM_PITCH_CLASSES = drums_encoder_decoder.DEFAULT_DRUM_TYPE_PITCHES\n",
    "FULL_DRUM_PITCH_CLASSES = [  # 61 classes\n",
    "    [p] for c in drums_encoder_decoder.DEFAULT_DRUM_TYPE_PITCHES for p in c]\n",
    "\n",
    "OUTPUT_VELOCITY = 80\n",
    "\n",
    "CHORD_SYMBOL = music_pb2.NoteSequence.TextAnnotation.CHORD_SYMBOL\n",
    "\n",
    "\n",
    "def _maybe_pad_seqs(seqs, dtype):\n",
    "  \"\"\"Pads sequences to match the longest and returns as a numpy array.\"\"\"\n",
    "  if not len(seqs):  # pylint:disable=g-explicit-length-test\n",
    "    return np.zeros((0, 0, 0), dtype)\n",
    "  lengths = [len(s) for s in seqs]\n",
    "  if len(set(lengths)) == 1:\n",
    "    return np.array(seqs, dtype)\n",
    "  else:\n",
    "    length = max(lengths)\n",
    "    return (np.array([np.pad(s, [(0, length - len(s)), (0, 0)], mode='constant')\n",
    "                      for s in seqs], dtype))\n",
    "\n",
    "\n",
    "def _extract_instrument(note_sequence, instrument):\n",
    "  extracted_ns = copy.copy(note_sequence)\n",
    "  del extracted_ns.notes[:]\n",
    "  extracted_ns.notes.extend(\n",
    "      n for n in note_sequence.notes if n.instrument == instrument)\n",
    "  return extracted_ns\n",
    "\n",
    "\n",
    "def np_onehot(indices, depth, dtype=np.bool):\n",
    "  \"\"\"Converts 1D array of indices to a one-hot 2D array with given depth.\"\"\"\n",
    "  onehot_seq = np.zeros((len(indices), depth), dtype=dtype)\n",
    "  onehot_seq[np.arange(len(indices)), indices] = 1.0\n",
    "  return onehot_seq\n",
    "\n",
    "\n",
    "class NoteSequenceAugmenter(object):\n",
    "  \"\"\"Class for augmenting NoteSequences.\n",
    "\n",
    "  Args:\n",
    "    transpose_range: A tuple containing the inclusive, integer range of\n",
    "        transpose amounts to sample from. If None, no transposition is applied.\n",
    "    stretch_range: A tuple containing the inclusive, float range of stretch\n",
    "        amounts to sample from.\n",
    "  Returns:\n",
    "    The augmented NoteSequence.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, transpose_range=None, stretch_range=None):\n",
    "    self._transpose_range = transpose_range\n",
    "    self._stretch_range = stretch_range\n",
    "\n",
    "  def augment(self, note_sequence):\n",
    "    \"\"\"Python implementation that augments the NoteSequence.\"\"\"\n",
    "    trans_amt = (random.randint(*self._transpose_range)\n",
    "                 if self._transpose_range else 0)\n",
    "    stretch_factor = (random.uniform(*self._stretch_range)\n",
    "                      if self._stretch_range else 1.0)\n",
    "    augmented_ns = copy.deepcopy(note_sequence)\n",
    "    del augmented_ns.notes[:]\n",
    "    for note in note_sequence.notes:\n",
    "      aug_pitch = note.pitch\n",
    "      if not note.is_drum:\n",
    "        aug_pitch += trans_amt\n",
    "      if MIN_MIDI_PITCH <= aug_pitch <= MAX_MIDI_PITCH:\n",
    "        augmented_ns.notes.add().CopyFrom(note)\n",
    "        augmented_ns.notes[-1].pitch = aug_pitch\n",
    "\n",
    "    for ta in augmented_ns.text_annotations:\n",
    "      if ta.annotation_type == CHORD_SYMBOL and ta.text != mm.NO_CHORD:\n",
    "        try:\n",
    "          figure = chord_symbols_lib.transpose_chord_symbol(ta.text, trans_amt)\n",
    "        except chord_symbols_lib.ChordSymbolException:\n",
    "          tf.logging.warning('Unable to transpose chord symbol: %s', ta.text)\n",
    "          figure = mm.NO_CHORD\n",
    "        ta.text = figure\n",
    "\n",
    "    augmented_ns = sequences_lib.stretch_note_sequence(\n",
    "        augmented_ns, stretch_factor)\n",
    "    return augmented_ns\n",
    "\n",
    "  def tf_augment(self, note_sequence_scalar):\n",
    "    \"\"\"TF op that augments the NoteSequence.\"\"\"\n",
    "    def _augment_str(note_sequence_str):\n",
    "      note_sequence = music_pb2.NoteSequence.FromString(note_sequence_str)\n",
    "      augmented_ns = self.augment(note_sequence)\n",
    "      return [augmented_ns.SerializeToString()]\n",
    "\n",
    "    augmented_note_sequence_scalar = tf.py_func(\n",
    "        _augment_str,\n",
    "        [note_sequence_scalar],\n",
    "        tf.string,\n",
    "        stateful=False,\n",
    "        name='augment')\n",
    "    augmented_note_sequence_scalar.set_shape(())\n",
    "    return augmented_note_sequence_scalar\n",
    "\n",
    "\n",
    "class ConverterTensors(collections.namedtuple(\n",
    "    'ConverterTensors', ['inputs', 'outputs', 'controls', 'lengths'])):\n",
    "  \"\"\"Tuple of tensors output by `to_tensors` method in converters.\n",
    "\n",
    "  Attributes:\n",
    "    inputs: Input tensors to feed to the encoder.\n",
    "    outputs: Output tensors to feed to the decoder.\n",
    "    controls: (Optional) tensors to use as controls for both encoding and\n",
    "        decoding.\n",
    "    lengths: Length of each input/output/control sequence.\n",
    "  \"\"\"\n",
    "\n",
    "  def __new__(cls, inputs=None, outputs=None, controls=None, lengths=None):\n",
    "    if inputs is None:\n",
    "      inputs = []\n",
    "    if outputs is None:\n",
    "      outputs = []\n",
    "    if lengths is None:\n",
    "      lengths = [len(i) for i in inputs]\n",
    "    if not controls:\n",
    "      controls = [np.zeros([l, 0]) for l in lengths]\n",
    "    return super(ConverterTensors, cls).__new__(\n",
    "        cls, inputs, outputs, controls, lengths)\n",
    "\n",
    "\n",
    "class BaseConverter(object):\n",
    "  \"\"\"Base class for data converters between items and tensors.\n",
    "\n",
    "  Inheriting classes must implement the following abstract methods:\n",
    "    -`_to_tensors`\n",
    "    -`_to_items`\n",
    "  \"\"\"\n",
    "\n",
    "  __metaclass__ = abc.ABCMeta\n",
    "\n",
    "  def __init__(self, input_depth, input_dtype, output_depth, output_dtype,\n",
    "               control_depth=0, control_dtype=np.bool, end_token=None,\n",
    "               max_tensors_per_item=None,\n",
    "               str_to_item_fn=lambda s: s, length_shape=()):\n",
    "    \"\"\"Initializes BaseConverter.\n",
    "\n",
    "    Args:\n",
    "      input_depth: Depth of final dimension of input (encoder) tensors.\n",
    "      input_dtype: DType of input (encoder) tensors.\n",
    "      output_depth: Depth of final dimension of output (decoder) tensors.\n",
    "      output_dtype: DType of output (decoder) tensors.\n",
    "      control_depth: Depth of final dimension of control tensors, or zero if not\n",
    "          conditioning on control tensors.\n",
    "      control_dtype: DType of control tensors.\n",
    "      end_token: Optional end token.\n",
    "      max_tensors_per_item: The maximum number of outputs to return for each\n",
    "          input.\n",
    "      str_to_item_fn: Callable to convert raw string input into an item for\n",
    "          conversion.\n",
    "      length_shape: Shape of length returned by `to_tensor`.\n",
    "    \"\"\"\n",
    "    self._input_depth = input_depth\n",
    "    self._input_dtype = input_dtype\n",
    "    self._output_depth = output_depth\n",
    "    self._output_dtype = output_dtype\n",
    "    self._control_depth = control_depth\n",
    "    self._control_dtype = control_dtype\n",
    "    self._end_token = end_token\n",
    "    self._max_tensors_per_input = max_tensors_per_item\n",
    "    self._str_to_item_fn = str_to_item_fn\n",
    "    self._is_training = False\n",
    "    self._length_shape = length_shape\n",
    "\n",
    "  @property\n",
    "  def is_training(self):\n",
    "    return self._is_training\n",
    "\n",
    "  @property\n",
    "  def str_to_item_fn(self):\n",
    "    return self._str_to_item_fn\n",
    "\n",
    "  @is_training.setter\n",
    "  def is_training(self, value):\n",
    "    self._is_training = value\n",
    "\n",
    "  @property\n",
    "  def max_tensors_per_item(self):\n",
    "    return self._max_tensors_per_input\n",
    "\n",
    "  @max_tensors_per_item.setter\n",
    "  def max_tensors_per_item(self, value):\n",
    "    self._max_tensors_per_input = value\n",
    "\n",
    "  @property\n",
    "  def end_token(self):\n",
    "    \"\"\"End token, or None.\"\"\"\n",
    "    return self._end_token\n",
    "\n",
    "  @property\n",
    "  def input_depth(self):\n",
    "    \"\"\"Dimension of inputs (to encoder) at each timestep of the sequence.\"\"\"\n",
    "    return self._input_depth\n",
    "\n",
    "  @property\n",
    "  def input_dtype(self):\n",
    "    \"\"\"DType of inputs (to encoder).\"\"\"\n",
    "    return self._input_dtype\n",
    "\n",
    "  @property\n",
    "  def output_depth(self):\n",
    "    \"\"\"Dimension of outputs (from decoder) at each timestep of the sequence.\"\"\"\n",
    "    return self._output_depth\n",
    "\n",
    "  @property\n",
    "  def output_dtype(self):\n",
    "    \"\"\"DType of outputs (from decoder).\"\"\"\n",
    "    return self._output_dtype\n",
    "\n",
    "  @property\n",
    "  def control_depth(self):\n",
    "    \"\"\"Dimension of control inputs at each timestep of the sequence.\"\"\"\n",
    "    return self._control_depth\n",
    "\n",
    "  @property\n",
    "  def control_dtype(self):\n",
    "    \"\"\"DType of control inputs.\"\"\"\n",
    "    return self._control_dtype\n",
    "\n",
    "  @property\n",
    "  def length_shape(self):\n",
    "    \"\"\"Shape of length returned by `to_tensor`.\"\"\"\n",
    "    return self._length_shape\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _to_tensors(self, item):\n",
    "    \"\"\"Implementation that converts item into encoder/decoder tensors.\n",
    "\n",
    "    Args:\n",
    "     item: Item to convert.\n",
    "\n",
    "    Returns:\n",
    "      A ConverterTensors struct containing encoder inputs, decoder outputs,\n",
    "      (optional) control tensors used for both encoding and decoding, and\n",
    "      sequence lengths.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _to_items(self, samples, controls=None):\n",
    "    \"\"\"Implementation that decodes model samples into list of items.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def _maybe_sample_outputs(self, outputs):\n",
    "    \"\"\"If should limit outputs, returns up to limit (randomly if training).\"\"\"\n",
    "    if (not self.max_tensors_per_item or\n",
    "        len(outputs) <= self.max_tensors_per_item):\n",
    "      return outputs\n",
    "    if self.is_training:\n",
    "      indices = set(np.random.choice(\n",
    "          len(outputs), size=self.max_tensors_per_item, replace=False))\n",
    "      return [outputs[i] for i in indices]\n",
    "    else:\n",
    "      return outputs[:self.max_tensors_per_item]\n",
    "\n",
    "  def to_tensors(self, item):\n",
    "    \"\"\"Python method that converts `item` into list of tensors.\"\"\"\n",
    "    tensors = self._to_tensors(item)\n",
    "    sampled_results = self._maybe_sample_outputs(list(zip(*tensors)))\n",
    "    return (ConverterTensors(*zip(*sampled_results))\n",
    "            if sampled_results else ConverterTensors())\n",
    "\n",
    "  def _combine_to_tensor_results(self, to_tensor_results):\n",
    "    \"\"\"Combines the results of multiple to_tensors calls into one result.\"\"\"\n",
    "    results = []\n",
    "    for result in to_tensor_results:\n",
    "      results.extend(zip(*result))\n",
    "    sampled_results = self._maybe_sample_outputs(results)\n",
    "    return (ConverterTensors(*zip(*sampled_results))\n",
    "            if sampled_results else ConverterTensors())\n",
    "\n",
    "  def to_items(self, samples, controls=None):\n",
    "    \"\"\"Python method that decodes samples into list of items.\"\"\"\n",
    "    if controls is None:\n",
    "      return self._to_items(samples)\n",
    "    else:\n",
    "      return self._to_items(samples, controls)\n",
    "\n",
    "  def tf_to_tensors(self, item_scalar):\n",
    "    \"\"\"TensorFlow op that converts item into output tensors.\n",
    "\n",
    "    Sequences will be padded to match the length of the longest.\n",
    "\n",
    "    Args:\n",
    "      item_scalar: A scalar of type tf.String containing the raw item to be\n",
    "          converted to tensors.\n",
    "\n",
    "    Returns:\n",
    "      inputs: A Tensor, shaped [num encoded seqs, max(lengths), input_depth],\n",
    "          containing the padded input encodings.\n",
    "      outputs: A Tensor, shaped [num encoded seqs, max(lengths), output_depth],\n",
    "          containing the padded output encodings resulting from the input.\n",
    "      controls: A Tensor, shaped\n",
    "          [num encoded seqs, max(lengths), control_depth], containing the padded\n",
    "          control encodings.\n",
    "      lengths: A tf.int32 Tensor, shaped [num encoded seqs], containing the\n",
    "        unpadded lengths of the tensor sequences resulting from the input.\n",
    "    \"\"\"\n",
    "    def _convert_and_pad(item_str):\n",
    "      item = self.str_to_item_fn(item_str)\n",
    "      tensors = self.to_tensors(item)\n",
    "      inputs = _maybe_pad_seqs(tensors.inputs, self.input_dtype)\n",
    "      outputs = _maybe_pad_seqs(tensors.outputs, self.output_dtype)\n",
    "      controls = _maybe_pad_seqs(tensors.controls, self.control_dtype)\n",
    "      return inputs, outputs, controls, np.array(tensors.lengths, np.int32)\n",
    "    inputs, outputs, controls, lengths = tf.py_func(\n",
    "        _convert_and_pad,\n",
    "        [item_scalar],\n",
    "        [self.input_dtype, self.output_dtype, self.control_dtype, tf.int32],\n",
    "        stateful=False,\n",
    "        name='convert_and_pad')\n",
    "    inputs.set_shape([None, None, self.input_depth])\n",
    "    outputs.set_shape([None, None, self.output_depth])\n",
    "    controls.set_shape([None, None, self.control_depth])\n",
    "    lengths.set_shape([None] + list(self.length_shape))\n",
    "    return inputs, outputs, controls, lengths\n",
    "\n",
    "\n",
    "def preprocess_notesequence(note_sequence, presplit_on_time_changes):\n",
    "  \"\"\"Preprocesses a single NoteSequence, resulting in multiple sequences.\"\"\"\n",
    "  if presplit_on_time_changes:\n",
    "    note_sequences = sequences_lib.split_note_sequence_on_time_changes(\n",
    "        note_sequence)\n",
    "  else:\n",
    "    note_sequences = [note_sequence]\n",
    "\n",
    "  return note_sequences\n",
    "\n",
    "\n",
    "class BaseNoteSequenceConverter(BaseConverter):\n",
    "  \"\"\"Base class for NoteSequence data converters.\n",
    "\n",
    "  Inheriting classes must implement the following abstract methods:\n",
    "    -`_to_tensors`\n",
    "    -`_to_notesequences`\n",
    "  \"\"\"\n",
    "\n",
    "  __metaclass__ = abc.ABCMeta\n",
    "\n",
    "  def __init__(self, input_depth, input_dtype, output_depth, output_dtype,\n",
    "               control_depth=0, control_dtype=np.bool, end_token=None,\n",
    "               presplit_on_time_changes=True,\n",
    "               max_tensors_per_notesequence=None):\n",
    "    \"\"\"Initializes BaseNoteSequenceConverter.\n",
    "\n",
    "    Args:\n",
    "      input_depth: Depth of final dimension of input (encoder) tensors.\n",
    "      input_dtype: DType of input (encoder) tensors.\n",
    "      output_depth: Depth of final dimension of output (decoder) tensors.\n",
    "      output_dtype: DType of output (decoder) tensors.\n",
    "      control_depth: Depth of final dimension of control tensors, or zero if not\n",
    "          conditioning on control tensors.\n",
    "      control_dtype: DType of control tensors.\n",
    "      end_token: Optional end token.\n",
    "      presplit_on_time_changes: Whether to split NoteSequence on time changes\n",
    "        before converting.\n",
    "      max_tensors_per_notesequence: The maximum number of outputs to return\n",
    "        for each NoteSequence.\n",
    "    \"\"\"\n",
    "    super(BaseNoteSequenceConverter, self).__init__(\n",
    "        input_depth, input_dtype, output_depth, output_dtype,\n",
    "        control_depth, control_dtype, end_token,\n",
    "        max_tensors_per_item=max_tensors_per_notesequence,\n",
    "        str_to_item_fn=music_pb2.NoteSequence.FromString)\n",
    "\n",
    "    self._presplit_on_time_changes = presplit_on_time_changes\n",
    "\n",
    "  @property\n",
    "  def max_tensors_per_notesequence(self):\n",
    "    return self.max_tensors_per_item\n",
    "\n",
    "  @max_tensors_per_notesequence.setter\n",
    "  def max_tensors_per_notesequence(self, value):\n",
    "    self.max_tensors_per_item = value\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _to_notesequences(self, samples, controls=None):\n",
    "    \"\"\"Implementation that decodes model samples into list of NoteSequences.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def to_notesequences(self, samples, controls=None):\n",
    "    \"\"\"Python method that decodes samples into list of NoteSequences.\"\"\"\n",
    "    return self._to_items(samples, controls)\n",
    "\n",
    "  def to_tensors(self, note_sequence):\n",
    "    \"\"\"Python method that converts `note_sequence` into list of tensors.\"\"\"\n",
    "    note_sequences = preprocess_notesequence(\n",
    "        note_sequence, self._presplit_on_time_changes)\n",
    "\n",
    "    results = []\n",
    "    for ns in note_sequences:\n",
    "      results.append(super(BaseNoteSequenceConverter, self).to_tensors(ns))\n",
    "    return self._combine_to_tensor_results(results)\n",
    "\n",
    "  def _to_items(self, samples, controls=None):\n",
    "    \"\"\"Python method that decodes samples into list of NoteSequences.\"\"\"\n",
    "    if controls is None:\n",
    "      return self._to_notesequences(samples)\n",
    "    else:\n",
    "      return self._to_notesequences(samples, controls)\n",
    "\n",
    "\n",
    "class LegacyEventListOneHotConverter(BaseNoteSequenceConverter):\n",
    "  \"\"\"Converts NoteSequences using legacy OneHotEncoding framework.\n",
    "\n",
    "  Quantizes the sequences, extracts event lists in the requested size range,\n",
    "  uniquifies, and converts to encoding. Uses the OneHotEncoding's\n",
    "  output encoding for both the input and output.\n",
    "\n",
    "  Args:\n",
    "    event_list_fn: A function that returns a new EventSequence.\n",
    "    event_extractor_fn: A function for extracing events into EventSequences. The\n",
    "      sole input should be the quantized NoteSequence.\n",
    "    legacy_encoder_decoder: An instantiated OneHotEncoding object to use.\n",
    "    add_end_token: Whether or not to add an end token. Recommended to be False\n",
    "      for fixed-length outputs.\n",
    "    slice_bars: Optional size of window to slide over raw event lists after\n",
    "      extraction.\n",
    "    steps_per_quarter: The number of quantization steps per quarter note.\n",
    "      Mututally exclusive with `steps_per_second`.\n",
    "    steps_per_second: The number of quantization steps per second.\n",
    "      Mututally exclusive with `steps_per_quarter`.\n",
    "    quarters_per_bar: The number of quarter notes per bar.\n",
    "    pad_to_total_time: Pads each input/output tensor to the total time of the\n",
    "      NoteSequence.\n",
    "    max_tensors_per_notesequence: The maximum number of outputs to return\n",
    "      for each NoteSequence.\n",
    "    presplit_on_time_changes: Whether to split NoteSequence on time changes\n",
    "      before converting.\n",
    "    chord_encoding: An instantiated OneHotEncoding object to use for encoding\n",
    "      chords on which to condition, or None if not conditioning on chords.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, event_list_fn, event_extractor_fn,\n",
    "               legacy_encoder_decoder, add_end_token=False, slice_bars=None,\n",
    "               slice_steps=None, steps_per_quarter=None, steps_per_second=None,\n",
    "               quarters_per_bar=4, pad_to_total_time=False,\n",
    "               max_tensors_per_notesequence=None,\n",
    "               presplit_on_time_changes=True, chord_encoding=None):\n",
    "    if (steps_per_quarter, steps_per_second).count(None) != 1:\n",
    "      raise ValueError(\n",
    "          'Exactly one of `steps_per_quarter` and `steps_per_second` should be '\n",
    "          'provided.')\n",
    "    if (slice_bars, slice_steps).count(None) == 0:\n",
    "      raise ValueError(\n",
    "          'At most one of `slice_bars` and `slice_steps` should be provided.')\n",
    "    self._event_list_fn = event_list_fn\n",
    "    self._event_extractor_fn = event_extractor_fn\n",
    "    self._legacy_encoder_decoder = legacy_encoder_decoder\n",
    "    self._chord_encoding = chord_encoding\n",
    "    self._steps_per_quarter = steps_per_quarter\n",
    "    if steps_per_quarter:\n",
    "      self._steps_per_bar = steps_per_quarter * quarters_per_bar\n",
    "    self._steps_per_second = steps_per_second\n",
    "    if slice_bars:\n",
    "      self._slice_steps = self._steps_per_bar * slice_bars\n",
    "    else:\n",
    "      self._slice_steps = slice_steps\n",
    "    self._pad_to_total_time = pad_to_total_time\n",
    "\n",
    "    depth = legacy_encoder_decoder.num_classes + add_end_token\n",
    "    control_depth = (chord_encoding.num_classes\n",
    "                     if chord_encoding is not None else 0)\n",
    "    super(LegacyEventListOneHotConverter, self).__init__(\n",
    "        input_depth=depth,\n",
    "        input_dtype=np.bool,\n",
    "        output_depth=depth,\n",
    "        output_dtype=np.bool,\n",
    "        control_depth=control_depth,\n",
    "        control_dtype=np.bool,\n",
    "        end_token=legacy_encoder_decoder.num_classes if add_end_token else None,\n",
    "        presplit_on_time_changes=presplit_on_time_changes,\n",
    "        max_tensors_per_notesequence=max_tensors_per_notesequence)\n",
    "\n",
    "  def _to_tensors(self, note_sequence):\n",
    "    \"\"\"Converts NoteSequence to unique, one-hot tensor sequences.\"\"\"\n",
    "    try:\n",
    "      if self._steps_per_quarter:\n",
    "        quantized_sequence = mm.quantize_note_sequence(\n",
    "            note_sequence, self._steps_per_quarter)\n",
    "        if (mm.steps_per_bar_in_quantized_sequence(quantized_sequence) !=\n",
    "            self._steps_per_bar):\n",
    "          return ConverterTensors()\n",
    "      else:\n",
    "        quantized_sequence = mm.quantize_note_sequence_absolute(\n",
    "            note_sequence, self._steps_per_second)\n",
    "    except (mm.BadTimeSignatureException, mm.NonIntegerStepsPerBarException,\n",
    "            mm.NegativeTimeException) as e:\n",
    "      return ConverterTensors()\n",
    "\n",
    "    if self._chord_encoding and not any(\n",
    "        ta.annotation_type == CHORD_SYMBOL\n",
    "        for ta in quantized_sequence.text_annotations):\n",
    "      # We are conditioning on chords but sequence does not have chords. Try to\n",
    "      # infer them.\n",
    "      try:\n",
    "        mm.infer_chords_for_sequence(quantized_sequence)\n",
    "      except mm.ChordInferenceException:\n",
    "        return ConverterTensors()\n",
    "\n",
    "    event_lists, unused_stats = self._event_extractor_fn(quantized_sequence)\n",
    "    if self._pad_to_total_time:\n",
    "      for e in event_lists:\n",
    "        e.set_length(len(e) + e.start_step, from_left=True)\n",
    "        e.set_length(quantized_sequence.total_quantized_steps)\n",
    "    if self._slice_steps:\n",
    "      sliced_event_lists = []\n",
    "      for l in event_lists:\n",
    "        for i in range(self._slice_steps, len(l) + 1, self._steps_per_bar):\n",
    "          sliced_event_lists.append(l[i - self._slice_steps: i])\n",
    "    else:\n",
    "      sliced_event_lists = event_lists\n",
    "\n",
    "    if self._chord_encoding:\n",
    "      try:\n",
    "        sliced_chord_lists = chords_lib.event_list_chords(\n",
    "            quantized_sequence, sliced_event_lists)\n",
    "      except chords_lib.CoincidentChordsException:\n",
    "        return ConverterTensors()\n",
    "      sliced_event_lists = [zip(el, cl) for el, cl in zip(sliced_event_lists,\n",
    "                                                          sliced_chord_lists)]\n",
    "\n",
    "    # TODO(adarob): Consider handling the fact that different event lists can\n",
    "    # be mapped to identical tensors by the encoder_decoder (e.g., Drums).\n",
    "\n",
    "    unique_event_tuples = list(set(tuple(l) for l in sliced_event_lists))\n",
    "    unique_event_tuples = self._maybe_sample_outputs(unique_event_tuples)\n",
    "\n",
    "    if not unique_event_tuples:\n",
    "      return ConverterTensors()\n",
    "\n",
    "    control_seqs = []\n",
    "    if self._chord_encoding:\n",
    "      unique_event_tuples, unique_chord_tuples = zip(\n",
    "          *[zip(*t) for t in unique_event_tuples if t])\n",
    "      for t in unique_chord_tuples:\n",
    "        try:\n",
    "          chord_tokens = [self._chord_encoding.encode_event(e) for e in t]\n",
    "          if self.end_token:\n",
    "            # Repeat the last chord instead of using a special token; otherwise\n",
    "            # the model may learn to rely on the special token to detect\n",
    "            # endings.\n",
    "            chord_tokens.append(chord_tokens[-1] if chord_tokens else\n",
    "                                self._chord_encoding.encode_event(mm.NO_CHORD))\n",
    "        except (mm.ChordSymbolException, mm.ChordEncodingException):\n",
    "          return ConverterTensors()\n",
    "        control_seqs.append(\n",
    "            np_onehot(chord_tokens, self.control_depth, self.control_dtype))\n",
    "\n",
    "    seqs = []\n",
    "    for t in unique_event_tuples:\n",
    "      seqs.append(np_onehot(\n",
    "          [self._legacy_encoder_decoder.encode_event(e) for e in t] +\n",
    "          ([] if self.end_token is None else [self.end_token]),\n",
    "          self.output_depth, self.output_dtype))\n",
    "\n",
    "    return ConverterTensors(inputs=seqs, outputs=seqs, controls=control_seqs)\n",
    "\n",
    "  def _to_notesequences(self, samples, controls=None):\n",
    "    output_sequences = []\n",
    "    for i, sample in enumerate(samples):\n",
    "      s = np.argmax(sample, axis=-1)\n",
    "      if self.end_token is not None and self.end_token in s.tolist():\n",
    "        end_index = s.tolist().index(self.end_token)\n",
    "      else:\n",
    "        end_index = len(s)\n",
    "      s = s[:end_index]\n",
    "      event_list = self._event_list_fn()\n",
    "      for e in s:\n",
    "        assert e != self.end_token\n",
    "        event_list.append(self._legacy_encoder_decoder.decode_event(e))\n",
    "      if self._steps_per_quarter:\n",
    "        qpm = mm.DEFAULT_QUARTERS_PER_MINUTE\n",
    "        seconds_per_step = 60.0 / (self._steps_per_quarter * qpm)\n",
    "        sequence = event_list.to_sequence(velocity=OUTPUT_VELOCITY, qpm=qpm)\n",
    "      else:\n",
    "        seconds_per_step = 1.0 / self._steps_per_second\n",
    "        sequence = event_list.to_sequence(velocity=OUTPUT_VELOCITY)\n",
    "      if self._chord_encoding and controls is not None:\n",
    "        chords = [self._chord_encoding.decode_event(e)\n",
    "                  for e in np.argmax(controls[i], axis=-1)[:end_index]]\n",
    "        chord_times = [step * seconds_per_step for step in event_list.steps]\n",
    "        chords_lib.add_chords_to_sequence(sequence, chords, chord_times)\n",
    "      output_sequences.append(sequence)\n",
    "    return output_sequences\n",
    "\n",
    "\n",
    "class OneHotMelodyConverter(LegacyEventListOneHotConverter):\n",
    "  \"\"\"Converter for legacy MelodyOneHotEncoding.\n",
    "\n",
    "  Args:\n",
    "    min_pitch: The minimum pitch to model. Those below this value will be\n",
    "      ignored.\n",
    "    max_pitch: The maximum pitch to model. Those above this value will be\n",
    "      ignored.\n",
    "    valid_programs: Optional set of program numbers to allow.\n",
    "    skip_polyphony: Whether to skip polyphonic instruments. If False, the\n",
    "      highest pitch will be taken in polyphonic sections.\n",
    "    max_bars: Optional maximum number of bars per extracted melody, before\n",
    "      slicing.\n",
    "    slice_bars: Optional size of window to slide over raw Melodies after\n",
    "      extraction.\n",
    "    gap_bars: If this many bars or more of non-events follow a note event, the\n",
    "       melody is ended. Disabled when set to 0 or None.\n",
    "    steps_per_quarter: The number of quantization steps per quarter note.\n",
    "    quarters_per_bar: The number of quarter notes per bar.\n",
    "    pad_to_total_time: Pads each input/output tensor to the total time of the\n",
    "      NoteSequence.\n",
    "    add_end_token: Whether to add an end token at the end of each sequence.\n",
    "    max_tensors_per_notesequence: The maximum number of outputs to return\n",
    "      for each NoteSequence.\n",
    "    chord_encoding: An instantiated OneHotEncoding object to use for encoding\n",
    "      chords on which to condition, or None if not conditioning on chords.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, min_pitch=PIANO_MIN_MIDI_PITCH,\n",
    "               max_pitch=PIANO_MAX_MIDI_PITCH, valid_programs=None,\n",
    "               skip_polyphony=False, max_bars=None, slice_bars=None,\n",
    "               gap_bars=1.0, steps_per_quarter=4, quarters_per_bar=4,\n",
    "               add_end_token=False, pad_to_total_time=False,\n",
    "               max_tensors_per_notesequence=5, presplit_on_time_changes=True,\n",
    "               chord_encoding=None):\n",
    "    self._min_pitch = min_pitch\n",
    "    self._max_pitch = max_pitch\n",
    "    self._valid_programs = valid_programs\n",
    "    steps_per_bar = steps_per_quarter * quarters_per_bar\n",
    "    max_steps_truncate = steps_per_bar * max_bars if max_bars else None\n",
    "\n",
    "    def melody_fn():\n",
    "      return mm.Melody(\n",
    "          steps_per_bar=steps_per_bar, steps_per_quarter=steps_per_quarter)\n",
    "    melody_extractor_fn = functools.partial(\n",
    "        mm.extract_melodies,\n",
    "        min_bars=1,\n",
    "        gap_bars=gap_bars or float('inf'),\n",
    "        max_steps_truncate=max_steps_truncate,\n",
    "        min_unique_pitches=1,\n",
    "        ignore_polyphonic_notes=not skip_polyphony,\n",
    "        pad_end=True)\n",
    "    super(OneHotMelodyConverter, self).__init__(\n",
    "        melody_fn,\n",
    "        melody_extractor_fn,\n",
    "        mm.MelodyOneHotEncoding(min_pitch, max_pitch + 1),\n",
    "        add_end_token=add_end_token,\n",
    "        slice_bars=slice_bars,\n",
    "        pad_to_total_time=pad_to_total_time,\n",
    "        steps_per_quarter=steps_per_quarter,\n",
    "        quarters_per_bar=quarters_per_bar,\n",
    "        max_tensors_per_notesequence=max_tensors_per_notesequence,\n",
    "        presplit_on_time_changes=presplit_on_time_changes,\n",
    "        chord_encoding=chord_encoding)\n",
    "\n",
    "  def _to_tensors(self, note_sequence):\n",
    "    def is_valid(note):\n",
    "      if (self._valid_programs is not None and\n",
    "          note.program not in self._valid_programs):\n",
    "        return False\n",
    "      return self._min_pitch <= note.pitch <= self._max_pitch\n",
    "    notes = list(note_sequence.notes)\n",
    "    del note_sequence.notes[:]\n",
    "    note_sequence.notes.extend([n for n in notes if is_valid(n)])\n",
    "    return super(OneHotMelodyConverter, self)._to_tensors(note_sequence)\n",
    "\n",
    "\n",
    "class DrumsConverter(BaseNoteSequenceConverter):\n",
    "  \"\"\"Converter for legacy drums with either pianoroll or one-hot tensors.\n",
    "\n",
    "  Inputs/outputs are either a \"pianoroll\"-like encoding of all possible drum\n",
    "  hits at a given step, or a one-hot encoding of the pianoroll.\n",
    "\n",
    "  The \"roll\" input encoding includes a final NOR bit (after the optional end\n",
    "  token).\n",
    "\n",
    "  Args:\n",
    "    max_bars: Optional maximum number of bars per extracted drums, before\n",
    "      slicing.\n",
    "    slice_bars: Optional size of window to slide over raw Melodies after\n",
    "      extraction.\n",
    "    gap_bars: If this many bars or more follow a non-empty drum event, the\n",
    "      drum track is ended. Disabled when set to 0 or None.\n",
    "    pitch_classes: A collection of collections, with each sub-collection\n",
    "      containing the set of pitches representing a single class to group by. By\n",
    "      default, groups valid drum pitches into 9 different classes.\n",
    "    add_end_token: Whether or not to add an end token. Recommended to be False\n",
    "      for fixed-length outputs.\n",
    "    steps_per_quarter: The number of quantization steps per quarter note.\n",
    "    quarters_per_bar: The number of quarter notes per bar.\n",
    "    pad_to_total_time: Pads each input/output tensor to the total time of the\n",
    "      NoteSequence.\n",
    "    roll_input: Whether to use a pianoroll-like representation as the input\n",
    "      instead of a one-hot encoding.\n",
    "    roll_output: Whether to use a pianoroll-like representation as the output\n",
    "      instead of a one-hot encoding.\n",
    "    max_tensors_per_notesequence: The maximum number of outputs to return\n",
    "      for each NoteSequence.\n",
    "    presplit_on_time_changes: Whether to split NoteSequence on time changes\n",
    "      before converting.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, max_bars=None, slice_bars=None, gap_bars=1.0,\n",
    "               pitch_classes=None, add_end_token=False, steps_per_quarter=4,\n",
    "               quarters_per_bar=4, pad_to_total_time=False, roll_input=False,\n",
    "               roll_output=False, max_tensors_per_notesequence=5,\n",
    "               presplit_on_time_changes=True):\n",
    "    self._pitch_classes = pitch_classes or REDUCED_DRUM_PITCH_CLASSES\n",
    "    self._pitch_class_map = {\n",
    "        p: i for i, pitches in enumerate(self._pitch_classes) for p in pitches}\n",
    "\n",
    "    self._steps_per_quarter = steps_per_quarter\n",
    "    self._steps_per_bar = steps_per_quarter * quarters_per_bar\n",
    "    self._slice_steps = self._steps_per_bar * slice_bars if slice_bars else None\n",
    "    self._pad_to_total_time = pad_to_total_time\n",
    "    self._roll_input = roll_input\n",
    "    self._roll_output = roll_output\n",
    "\n",
    "    self._drums_extractor_fn = functools.partial(\n",
    "        mm.extract_drum_tracks,\n",
    "        min_bars=1,\n",
    "        gap_bars=gap_bars or float('inf'),\n",
    "        max_steps_truncate=self._steps_per_bar * max_bars if max_bars else None,\n",
    "        pad_end=True)\n",
    "\n",
    "    num_classes = len(self._pitch_classes)\n",
    "\n",
    "    self._pr_encoder_decoder = mm.PianorollEncoderDecoder(\n",
    "        input_size=num_classes + add_end_token)\n",
    "    # Use pitch classes as `drum_type_pitches` since we have already done the\n",
    "    # mapping.\n",
    "    self._oh_encoder_decoder = mm.MultiDrumOneHotEncoding(\n",
    "        drum_type_pitches=[(i,) for i in range(num_classes)])\n",
    "\n",
    "    output_depth = (num_classes if self._roll_output else\n",
    "                    self._oh_encoder_decoder.num_classes) + add_end_token\n",
    "    super(DrumsConverter, self).__init__(\n",
    "        input_depth=(\n",
    "            num_classes + 1 if self._roll_input else\n",
    "            self._oh_encoder_decoder.num_classes) + add_end_token,\n",
    "        input_dtype=np.bool,\n",
    "        output_depth=output_depth,\n",
    "        output_dtype=np.bool,\n",
    "        end_token=output_depth - 1 if add_end_token else None,\n",
    "        presplit_on_time_changes=presplit_on_time_changes,\n",
    "        max_tensors_per_notesequence=max_tensors_per_notesequence)\n",
    "\n",
    "  def _to_tensors(self, note_sequence):\n",
    "    \"\"\"Converts NoteSequence to unique sequences.\"\"\"\n",
    "    try:\n",
    "      quantized_sequence = mm.quantize_note_sequence(\n",
    "          note_sequence, self._steps_per_quarter)\n",
    "      if (mm.steps_per_bar_in_quantized_sequence(quantized_sequence) !=\n",
    "          self._steps_per_bar):\n",
    "        return ConverterTensors()\n",
    "    except (mm.BadTimeSignatureException, mm.NonIntegerStepsPerBarException,\n",
    "            mm.NegativeTimeException) as e:\n",
    "      return ConverterTensors()\n",
    "\n",
    "    new_notes = []\n",
    "    for n in quantized_sequence.notes:\n",
    "      if not n.is_drum:\n",
    "        continue\n",
    "      if n.pitch not in self._pitch_class_map:\n",
    "        continue\n",
    "      n.pitch = self._pitch_class_map[n.pitch]\n",
    "      new_notes.append(n)\n",
    "    del quantized_sequence.notes[:]\n",
    "    quantized_sequence.notes.extend(new_notes)\n",
    "\n",
    "    event_lists, unused_stats = self._drums_extractor_fn(quantized_sequence)\n",
    "\n",
    "    if self._pad_to_total_time:\n",
    "      for e in event_lists:\n",
    "        e.set_length(len(e) + e.start_step, from_left=True)\n",
    "        e.set_length(quantized_sequence.total_quantized_steps)\n",
    "    if self._slice_steps:\n",
    "      sliced_event_tuples = []\n",
    "      for l in event_lists:\n",
    "        for i in range(self._slice_steps, len(l) + 1, self._steps_per_bar):\n",
    "          sliced_event_tuples.append(tuple(l[i - self._slice_steps: i]))\n",
    "    else:\n",
    "      sliced_event_tuples = [tuple(l) for l in event_lists]\n",
    "\n",
    "    unique_event_tuples = list(set(sliced_event_tuples))\n",
    "    unique_event_tuples = self._maybe_sample_outputs(unique_event_tuples)\n",
    "\n",
    "    rolls = []\n",
    "    oh_vecs = []\n",
    "    for t in unique_event_tuples:\n",
    "      if self._roll_input or self._roll_output:\n",
    "        if self.end_token is not None:\n",
    "          t_roll = list(t) + [(self._pr_encoder_decoder.input_size - 1,)]\n",
    "        else:\n",
    "          t_roll = t\n",
    "        rolls.append(np.vstack([\n",
    "            self._pr_encoder_decoder.events_to_input(t_roll, i).astype(np.bool)\n",
    "            for i in range(len(t_roll))]))\n",
    "      if not (self._roll_input and self._roll_output):\n",
    "        labels = [self._oh_encoder_decoder.encode_event(e) for e in t]\n",
    "        if self.end_token is not None:\n",
    "          labels += [self._oh_encoder_decoder.num_classes]\n",
    "        oh_vecs.append(np_onehot(\n",
    "            labels,\n",
    "            self._oh_encoder_decoder.num_classes + (self.end_token is not None),\n",
    "            np.bool))\n",
    "\n",
    "    if self._roll_input:\n",
    "      input_seqs = [\n",
    "          np.append(roll, np.expand_dims(np.all(roll == 0, axis=1), axis=1),\n",
    "                    axis=1) for roll in rolls]\n",
    "    else:\n",
    "      input_seqs = oh_vecs\n",
    "\n",
    "    output_seqs = rolls if self._roll_output else oh_vecs\n",
    "\n",
    "    return ConverterTensors(inputs=input_seqs, outputs=output_seqs)\n",
    "\n",
    "  def _to_notesequences(self, samples):\n",
    "    output_sequences = []\n",
    "    for s in samples:\n",
    "      if self._roll_output:\n",
    "        if self.end_token is not None:\n",
    "          end_i = np.where(s[:, self.end_token])\n",
    "          if len(end_i):  # pylint: disable=g-explicit-length-test\n",
    "            s = s[:end_i[0]]\n",
    "        events_list = [frozenset(np.where(e)[0]) for e in s]\n",
    "      else:\n",
    "        s = np.argmax(s, axis=-1)\n",
    "        if self.end_token is not None and self.end_token in s:\n",
    "          s = s[:s.tolist().index(self.end_token)]\n",
    "        events_list = [self._oh_encoder_decoder.decode_event(e) for e in s]\n",
    "      # Map classes to exemplars.\n",
    "      events_list = [\n",
    "          frozenset(self._pitch_classes[c][0] for c in e) for e in events_list]\n",
    "      track = mm.DrumTrack(\n",
    "          events=events_list, steps_per_bar=self._steps_per_bar,\n",
    "          steps_per_quarter=self._steps_per_quarter)\n",
    "      output_sequences.append(track.to_sequence(velocity=OUTPUT_VELOCITY))\n",
    "    return output_sequences\n",
    "\n",
    "\n",
    "class TrioConverter(BaseNoteSequenceConverter):\n",
    "  \"\"\"Converts to/from 3-part (mel, drums, bass) multi-one-hot events.\n",
    "\n",
    "  Extracts overlapping segments with melody, drums, and bass (determined by\n",
    "  program number) and concatenates one-hot tensors from OneHotMelodyConverter\n",
    "  and OneHotDrumsConverter. Takes the cross products from the sets of\n",
    "  instruments of each type.\n",
    "\n",
    "  Args:\n",
    "    slice_bars: Optional size of window to slide over full converted tensor.\n",
    "    gap_bars: The number of consecutive empty bars to allow for any given\n",
    "      instrument. Note that this number is effectively doubled for internal\n",
    "      gaps.\n",
    "    max_bars: Optional maximum number of bars per extracted sequence, before\n",
    "      slicing.\n",
    "    steps_per_quarter: The number of quantization steps per quarter note.\n",
    "    quarters_per_bar: The number of quarter notes per bar.\n",
    "    max_tensors_per_notesequence: The maximum number of outputs to return\n",
    "      for each NoteSequence.\n",
    "    chord_encoding: An instantiated OneHotEncoding object to use for encoding\n",
    "      chords on which to condition, or None if not conditioning on chords.\n",
    "  \"\"\"\n",
    "\n",
    "  class InstrumentType(object):\n",
    "    UNK = 0\n",
    "    MEL = 1\n",
    "    BASS = 2\n",
    "    DRUMS = 3\n",
    "    INVALID = 4\n",
    "\n",
    "  def __init__(\n",
    "      self, slice_bars=None, gap_bars=2, max_bars=1024, steps_per_quarter=4,\n",
    "      quarters_per_bar=4, max_tensors_per_notesequence=5, chord_encoding=None):\n",
    "    self._melody_converter = OneHotMelodyConverter(\n",
    "        gap_bars=None, steps_per_quarter=steps_per_quarter,\n",
    "        pad_to_total_time=True, presplit_on_time_changes=False,\n",
    "        max_tensors_per_notesequence=None, chord_encoding=chord_encoding)\n",
    "    self._drums_converter = DrumsConverter(\n",
    "        gap_bars=None, steps_per_quarter=steps_per_quarter,\n",
    "        pad_to_total_time=True, presplit_on_time_changes=False,\n",
    "        max_tensors_per_notesequence=None)\n",
    "    self._slice_bars = slice_bars\n",
    "    self._gap_bars = gap_bars\n",
    "    self._max_bars = max_bars\n",
    "    self._steps_per_quarter = steps_per_quarter\n",
    "    self._steps_per_bar = steps_per_quarter * quarters_per_bar\n",
    "    self._chord_encoding = chord_encoding\n",
    "\n",
    "    self._split_output_depths = (\n",
    "        self._melody_converter.output_depth,\n",
    "        self._melody_converter.output_depth,\n",
    "        self._drums_converter.output_depth)\n",
    "    output_depth = sum(self._split_output_depths)\n",
    "\n",
    "    self._program_map = dict(\n",
    "        [(i, TrioConverter.InstrumentType.MEL) for i in MEL_PROGRAMS] +\n",
    "        [(i, TrioConverter.InstrumentType.BASS) for i in BASS_PROGRAMS])\n",
    "\n",
    "    super(TrioConverter, self).__init__(\n",
    "        input_depth=output_depth,\n",
    "        input_dtype=np.bool,\n",
    "        output_depth=output_depth,\n",
    "        output_dtype=np.bool,\n",
    "        control_depth=self._melody_converter.control_depth,\n",
    "        control_dtype=self._melody_converter.control_dtype,\n",
    "        end_token=False,\n",
    "        presplit_on_time_changes=True,\n",
    "        max_tensors_per_notesequence=max_tensors_per_notesequence)\n",
    "\n",
    "  def _to_tensors(self, note_sequence):\n",
    "    try:\n",
    "      quantized_sequence = mm.quantize_note_sequence(\n",
    "          note_sequence, self._steps_per_quarter)\n",
    "      if (mm.steps_per_bar_in_quantized_sequence(quantized_sequence) !=\n",
    "          self._steps_per_bar):\n",
    "        return ConverterTensors()\n",
    "    except (mm.BadTimeSignatureException, mm.NonIntegerStepsPerBarException,\n",
    "            mm.NegativeTimeException):\n",
    "      return ConverterTensors()\n",
    "\n",
    "    if self._chord_encoding and not any(\n",
    "        ta.annotation_type == CHORD_SYMBOL\n",
    "        for ta in quantized_sequence.text_annotations):\n",
    "      # We are conditioning on chords but sequence does not have chords. Try to\n",
    "      # infer them.\n",
    "      try:\n",
    "        mm.infer_chords_for_sequence(quantized_sequence)\n",
    "      except mm.ChordInferenceException:\n",
    "        return ConverterTensors()\n",
    "\n",
    "      # The trio parts get extracted from the original NoteSequence, so copy the\n",
    "      # inferred chords back to that one.\n",
    "      for qta in quantized_sequence.text_annotations:\n",
    "        if qta.annotation_type == CHORD_SYMBOL:\n",
    "          ta = note_sequence.text_annotations.add()\n",
    "          ta.annotation_type = CHORD_SYMBOL\n",
    "          ta.time = qta.time\n",
    "          ta.text = qta.text\n",
    "\n",
    "    total_bars = int(\n",
    "        np.ceil(quantized_sequence.total_quantized_steps / self._steps_per_bar))\n",
    "    total_bars = min(total_bars, self._max_bars)\n",
    "\n",
    "    # Assign an instrument class for each instrument, and compute its coverage.\n",
    "    # If an instrument has multiple classes, it is considered INVALID.\n",
    "    instrument_type = np.zeros(MAX_INSTRUMENT_NUMBER + 1, np.uint8)\n",
    "    coverage = np.zeros((total_bars, MAX_INSTRUMENT_NUMBER + 1), np.bool)\n",
    "    for note in quantized_sequence.notes:\n",
    "      i = note.instrument\n",
    "      if i > MAX_INSTRUMENT_NUMBER:\n",
    "        tf.logging.warning('Skipping invalid instrument number: %d', i)\n",
    "        continue\n",
    "      inferred_type = (\n",
    "          self.InstrumentType.DRUMS if note.is_drum else\n",
    "          self._program_map.get(note.program, self.InstrumentType.INVALID))\n",
    "      if not instrument_type[i]:\n",
    "        instrument_type[i] = inferred_type\n",
    "      elif instrument_type[i] != inferred_type:\n",
    "        instrument_type[i] = self.InstrumentType.INVALID\n",
    "\n",
    "      start_bar = note.quantized_start_step // self._steps_per_bar\n",
    "      end_bar = int(np.ceil(note.quantized_end_step / self._steps_per_bar))\n",
    "\n",
    "      if start_bar >= total_bars:\n",
    "        continue\n",
    "      coverage[start_bar:min(end_bar, total_bars), i] = True\n",
    "\n",
    "    # Group instruments by type.\n",
    "    instruments_by_type = collections.defaultdict(list)\n",
    "    for i, type_ in enumerate(instrument_type):\n",
    "      if type_ not in (self.InstrumentType.UNK, self.InstrumentType.INVALID):\n",
    "        instruments_by_type[type_].append(i)\n",
    "    if len(instruments_by_type) < 3:\n",
    "      # This NoteSequence doesn't have all 3 types.\n",
    "      return ConverterTensors()\n",
    "\n",
    "    # Encode individual instruments.\n",
    "    # Set total time so that instruments will be padded correctly.\n",
    "    note_sequence.total_time = (\n",
    "        total_bars * self._steps_per_bar *\n",
    "        60 / note_sequence.tempos[0].qpm / self._steps_per_quarter)\n",
    "    encoded_instruments = {}\n",
    "    encoded_chords = None\n",
    "    for i in (instruments_by_type[self.InstrumentType.MEL] +\n",
    "              instruments_by_type[self.InstrumentType.BASS]):\n",
    "      tensors = self._melody_converter.to_tensors(\n",
    "          _extract_instrument(note_sequence, i))\n",
    "      if tensors.outputs:\n",
    "        encoded_instruments[i] = tensors.outputs[0]\n",
    "        if encoded_chords is None:\n",
    "          encoded_chords = tensors.controls[0]\n",
    "        elif not np.array_equal(encoded_chords, tensors.controls[0]):\n",
    "          tf.logging.warning('Trio chords disagreement between instruments.')\n",
    "      else:\n",
    "        coverage[:, i] = False\n",
    "    for i in instruments_by_type[self.InstrumentType.DRUMS]:\n",
    "      tensors = self._drums_converter.to_tensors(\n",
    "          _extract_instrument(note_sequence, i))\n",
    "      if tensors.outputs:\n",
    "        encoded_instruments[i] = tensors.outputs[0]\n",
    "      else:\n",
    "        coverage[:, i] = False\n",
    "\n",
    "    # Fill in coverage gaps up to self._gap_bars.\n",
    "    og_coverage = coverage.copy()\n",
    "    for j in range(total_bars):\n",
    "      coverage[j] = np.any(\n",
    "          og_coverage[\n",
    "              max(0, j-self._gap_bars):min(total_bars, j+self._gap_bars) + 1],\n",
    "          axis=0)\n",
    "\n",
    "    # Take cross product of instruments from each class and compute combined\n",
    "    # encodings where they overlap.\n",
    "    seqs = []\n",
    "    control_seqs = []\n",
    "    for grp in itertools.product(\n",
    "        instruments_by_type[self.InstrumentType.MEL],\n",
    "        instruments_by_type[self.InstrumentType.BASS],\n",
    "        instruments_by_type[self.InstrumentType.DRUMS]):\n",
    "      # Consider an instrument covered within gap_bars from the end if any of\n",
    "      # the other instruments are. This allows more leniency when re-encoding\n",
    "      # slices.\n",
    "      grp_coverage = np.all(coverage[:, grp], axis=1)\n",
    "      grp_coverage[:self._gap_bars] = np.any(coverage[:self._gap_bars, grp])\n",
    "      grp_coverage[-self._gap_bars:] = np.any(coverage[-self._gap_bars:, grp])\n",
    "      for j in range(total_bars - self._slice_bars + 1):\n",
    "        if (np.all(grp_coverage[j:j + self._slice_bars]) and\n",
    "            all(i in encoded_instruments for i in grp)):\n",
    "          start_step = j * self._steps_per_bar\n",
    "          end_step = (j + self._slice_bars) * self._steps_per_bar\n",
    "          seqs.append(np.concatenate(\n",
    "              [encoded_instruments[i][start_step:end_step] for i in grp],\n",
    "              axis=-1))\n",
    "          if encoded_chords is not None:\n",
    "            control_seqs.append(encoded_chords[start_step:end_step])\n",
    "\n",
    "    return ConverterTensors(inputs=seqs, outputs=seqs, controls=control_seqs)\n",
    "\n",
    "  def _to_notesequences(self, samples, controls=None):\n",
    "    output_sequences = []\n",
    "    dim_ranges = np.cumsum(self._split_output_depths)\n",
    "    for i, s in enumerate(samples):\n",
    "      mel_ns = self._melody_converter.to_notesequences(\n",
    "          [s[:, :dim_ranges[0]]],\n",
    "          [controls[i]] if controls is not None else None)[0]\n",
    "      bass_ns = self._melody_converter.to_notesequences(\n",
    "          [s[:, dim_ranges[0]:dim_ranges[1]]])[0]\n",
    "      drums_ns = self._drums_converter.to_notesequences(\n",
    "          [s[:, dim_ranges[1]:]])[0]\n",
    "\n",
    "      for n in bass_ns.notes:\n",
    "        n.instrument = 1\n",
    "        n.program = ELECTRIC_BASS_PROGRAM\n",
    "      for n in drums_ns.notes:\n",
    "        n.instrument = 9\n",
    "\n",
    "      ns = mel_ns\n",
    "      ns.notes.extend(bass_ns.notes)\n",
    "      ns.notes.extend(drums_ns.notes)\n",
    "      ns.total_time = max(\n",
    "          mel_ns.total_time, bass_ns.total_time, drums_ns.total_time)\n",
    "      output_sequences.append(ns)\n",
    "    return output_sequences\n",
    "\n",
    "\n",
    "def count_examples(examples_path, data_converter,\n",
    "                   file_reader=tf.python_io.tf_record_iterator):\n",
    "  \"\"\"Counts the number of examples produced by the converter from files.\"\"\"\n",
    "  filenames = tf.gfile.Glob(examples_path)\n",
    "\n",
    "  num_examples = 0\n",
    "\n",
    "  for f in filenames:\n",
    "    tf.logging.info('Counting examples in %s.', f)\n",
    "    reader = file_reader(f)\n",
    "    for item_str in reader:\n",
    "      item = data_converter.str_to_item_fn(item_str)\n",
    "      tensors = data_converter.to_tensors(item)\n",
    "      num_examples += len(tensors.inputs)\n",
    "  tf.logging.info('Total examples: %d', num_examples)\n",
    "  return num_examples\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    config,\n",
    "    num_threads=1,\n",
    "    tf_file_reader=tf.data.TFRecordDataset,\n",
    "    prefetch_size=4,\n",
    "    is_training=False):\n",
    "  \"\"\"Get input tensors from dataset for training or evaluation.\n",
    "\n",
    "  Args:\n",
    "    config: A Config object containing dataset information.\n",
    "    num_threads: The number of threads to use for pre-processing.\n",
    "    tf_file_reader: The tf.data.Dataset class to use for reading files.\n",
    "    prefetch_size: The number of batches to prefetch. Disabled when 0.\n",
    "    is_training: Whether or not the dataset is used in training. Determines\n",
    "      whether dataset is shuffled and repeated, etc.\n",
    "\n",
    "  Returns:\n",
    "    A tf.data.Dataset containing input, output, control, and length tensors.\n",
    "  \"\"\"\n",
    "  batch_size = config.hparams.batch_size\n",
    "  examples_path = (\n",
    "      config.train_examples_path if is_training else config.eval_examples_path)\n",
    "  note_sequence_augmenter = (\n",
    "      config.note_sequence_augmenter if is_training else None)\n",
    "  data_converter = config.data_converter\n",
    "  data_converter.is_training = is_training\n",
    "\n",
    "  tf.logging.info('Reading examples from: %s', examples_path)\n",
    "\n",
    "  num_files = len(tf.gfile.Glob(examples_path))\n",
    "  files = tf.data.Dataset.list_files(examples_path)\n",
    "  if is_training:\n",
    "    files = files.apply(\n",
    "        tf.contrib.data.shuffle_and_repeat(buffer_size=num_files))\n",
    "\n",
    "  reader = files.apply(\n",
    "      tf.contrib.data.parallel_interleave(\n",
    "          tf_file_reader,\n",
    "          cycle_length=num_threads,\n",
    "          sloppy=True))\n",
    "\n",
    "  def _remove_pad_fn(padded_seq_1, padded_seq_2, padded_seq_3, length):\n",
    "    if length.shape.ndims == 0:\n",
    "      return (padded_seq_1[0:length], padded_seq_2[0:length],\n",
    "              padded_seq_3[0:length], length)\n",
    "    else:\n",
    "      # Don't remove padding for hierarchical examples.\n",
    "      return padded_seq_1, padded_seq_2, padded_seq_3, length\n",
    "\n",
    "  dataset = reader\n",
    "  if note_sequence_augmenter is not None:\n",
    "    dataset = dataset.map(note_sequence_augmenter.tf_augment)\n",
    "  dataset = (dataset\n",
    "             .map(data_converter.tf_to_tensors,\n",
    "                  num_parallel_calls=num_threads)\n",
    "             .flat_map(lambda *t: tf.data.Dataset.from_tensor_slices(t))\n",
    "             .map(_remove_pad_fn))\n",
    "  if is_training:\n",
    "    dataset = dataset.shuffle(buffer_size=batch_size * 4)\n",
    "\n",
    "  dataset = dataset.padded_batch(batch_size, dataset.output_shapes)\n",
    "\n",
    "  if prefetch_size:\n",
    "    dataset = dataset.prefetch(prefetch_size)\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
