{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MG LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cipher000/anaconda3/envs/tensorflow1.4/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cipher000/anaconda3/envs/tensorflow1.4/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/cipher000/anaconda3/envs/tensorflow1.4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_file = '../data/notes'\n",
    "midi_files = '../data/MidiWorld/BackstreetBoys/*.mid'\n",
    "weights_file = '../weights/lstm_weights.hdf5'\n",
    "# midi_files = '../data/MidiWorld/BritneySpears/DriveMeCrazyBritneySpears.mid'\n",
    "output_name = 'BS1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100 # the lstm will predict the next note based on the last set of notes heard\n",
    "node1 = 512\n",
    "node2 = 256\n",
    "drop = 0.3\n",
    "epochs = 50 # 200\n",
    "batch_size = 64\n",
    "notes_generated = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_notes():\n",
    "    notes = []\n",
    "    notes_dict = {}\n",
    "    cnt = 0\n",
    "    \n",
    "    for file in glob.glob(midi_files):\n",
    "        print(file)\n",
    "        notes_per_file = []\n",
    "        try:\n",
    "            midi = converter.parse(file)\n",
    "            parts = instrument.partitionByInstrument(midi)\n",
    "\n",
    "            if parts:\n",
    "                notes_to_parse = parts.parts[0].recurse()\n",
    "            else:\n",
    "                notes_to_parse = midi.flat.notes\n",
    "\n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                    notes_per_file.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    to_append = '.'.join(str(n) for n in element.normalOrder)\n",
    "                    notes.append(to_append)\n",
    "                    notes_per_file.append(to_append)\n",
    "            notes_dict[file] = notes_per_file\n",
    "            n_vocab = len(set(notes))\n",
    "            cnt +=1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    with open(notes_file, 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "    print(\"{} midi files and {} notes\".format(cnt,len(notes)))\n",
    "    print(\"Notes Converted\")\n",
    "    return notes, n_vocab, cnt\n",
    "\n",
    "def prep_train_sequences(notes, n_vocab):\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    \n",
    "    for i in range(0,len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        \n",
    "        input_append = [note_to_int[char] for char in sequence_in]\n",
    "        network_input.append(input_append)\n",
    "        output_append = note_to_int[sequence_out]\n",
    "        network_output.append(output_append)\n",
    "        \n",
    "    n_patterns = len(network_input)\n",
    "    \n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "    print(\"Sequences Prepared\")\n",
    "    return pitchnames, network_input, network_output\n",
    "\n",
    "def create_train_network(network_input, n_vocab):\n",
    "    model = Sequential()\n",
    "    # Layer 1\n",
    "    model.add(LSTM(node1,input_shape=(network_input.shape[1], network_input.shape[2]),return_sequences=True))\n",
    "    model.add(Dropout(drop))\n",
    "    # Layer 2\n",
    "    model.add(LSTM(node1, return_sequences=True))\n",
    "    model.add(Dropout(drop))\n",
    "    # Layer 3\n",
    "    model.add(LSTM(node1))\n",
    "    model.add(Dense(node2))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    print(\"Network Created\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train(model, network_input, network_output):\n",
    "    filepath = \"../weights/weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "#     print(checkpoint)\n",
    "    \n",
    "    callbacks_list = [checkpoint]\n",
    "#     print(callbacks_list)\n",
    "    \n",
    "    model.fit(\n",
    "        network_input, \n",
    "        network_output, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        callbacks=callbacks_list)\n",
    "    model.save_weights('../weights/lstm_weights.hdf5')\n",
    "#     weights_file = filepath\n",
    "#     print(\"Final weights saved at {}\".format(weights_file))\n",
    "#     return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-.mid\n",
      "expected 0 arguments, got 1\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-AChild.mid\n",
      "expected 0 arguments, got 1\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-forYou.mid\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-GotItGoinOn.mid\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-HeartStaysWithYou.mid\n",
      "expected 0 arguments, got 1\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-IHavetoGive.mid\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-LongAsYouLoveMe.mid\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-MetheMeaning.mid\n",
      "expected 0 arguments, got 1\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-NeedYouTonight.mid\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-NeverBreakYourHeart.mid\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-ofMyHeart.mid\n",
      "expected 0 arguments, got 1\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-One.mid\n",
      "expected 0 arguments, got 1\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-OneElseComesClose.mid\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-PlayingGamesWithMyHeart.mid\n",
      "expected 0 arguments, got 1\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-Promises.mid\n",
      "expected 0 arguments, got 1\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-ThanLife.mid\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-theMusicHealYourSoul.mid\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-toYourHeart.mid\n",
      "../data/MidiWorld/BackstreetBoys/BackstreetBoys-WantItThatWay.mid\n",
      "11 midi files and 15162 notes\n",
      "Notes Converted\n",
      "Sequences Prepared\n",
      "Network Created\n",
      "Epoch 1/50\n",
      "15062/15062 [==============================] - 790s 52ms/step - loss: 4.3072\n",
      "Epoch 2/50\n",
      "15062/15062 [==============================] - 760s 50ms/step - loss: 3.8384\n",
      "Epoch 3/50\n",
      "15062/15062 [==============================] - 779s 52ms/step - loss: 3.6342\n",
      "Epoch 4/50\n",
      "15062/15062 [==============================] - 732s 49ms/step - loss: 3.4608\n",
      "Epoch 5/50\n",
      "15062/15062 [==============================] - 722s 48ms/step - loss: 3.2742\n",
      "Epoch 6/50\n",
      "15062/15062 [==============================] - 723s 48ms/step - loss: 3.1225\n",
      "Epoch 7/50\n",
      "15062/15062 [==============================] - 735s 49ms/step - loss: 2.9733\n",
      "Epoch 8/50\n",
      "15062/15062 [==============================] - 810s 54ms/step - loss: 2.8533\n",
      "Epoch 9/50\n",
      "15062/15062 [==============================] - 873s 58ms/step - loss: 2.7015\n",
      "Epoch 10/50\n",
      "15062/15062 [==============================] - 874s 58ms/step - loss: 2.5746\n",
      "Epoch 11/50\n",
      "15062/15062 [==============================] - 891s 59ms/step - loss: 2.4379\n",
      "Epoch 12/50\n",
      "15062/15062 [==============================] - 881s 58ms/step - loss: 2.2935\n",
      "Epoch 13/50\n",
      "15062/15062 [==============================] - 865s 57ms/step - loss: 2.1697\n",
      "Epoch 14/50\n",
      "15062/15062 [==============================] - 870s 58ms/step - loss: 2.0376\n",
      "Epoch 15/50\n",
      "15062/15062 [==============================] - 882s 59ms/step - loss: 1.9124\n",
      "Epoch 16/50\n",
      "13824/15062 [==========================>...] - ETA: 1:26 - loss: 1.8017"
     ]
    }
   ],
   "source": [
    "notes, n_vocab, cnt = convert_to_notes()\n",
    "\n",
    "# n_vocab = len(set(notes))\n",
    "\n",
    "pitchnames, network_input, network_output = prep_train_sequences(notes, n_vocab)\n",
    "\n",
    "model = create_train_network(network_input, n_vocab)\n",
    "\n",
    "train(model, network_input, network_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_output_sequences(notes, pitchnames, n_vocab):\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    \n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "        \n",
    "    n_patterns = len(network_input)\n",
    "    \n",
    "    normalized_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "    \n",
    "    return network_input, normalized_input\n",
    "\n",
    "\n",
    "def create_output_network(network_input, n_vocab,weights_file):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(node1,input_shape=(network_input.shape[1], network_input.shape[2]),return_sequences=True))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(LSTM(node1, return_sequences=True))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(LSTM(node1))\n",
    "    model.add(Dense(node2))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    print(\"Network Created\")\n",
    "    \n",
    "    model.load_weights(weights_file)\n",
    "    print(\"Weights loaded from {}\".format(weights_file))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    start = np.random.randint(0,len(network_input)-1)\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "    \n",
    "    for note_index in range(notes_generated):\n",
    "        prediction_input = np.reshape(pattern, (1,len(pattern),1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "        \n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "        \n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "        \n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "        \n",
    "    return prediction_output\n",
    "\n",
    "def create_midi(prediction_output,output_name, epochs):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    \n",
    "    for pattern in prediction_output:\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "        \n",
    "        offset += 0.5\n",
    "        \n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    output_file = '../output/lstm_midi.mid'\n",
    "    midi_stream.write('midi',fp=output_file)\n",
    "    return midi_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input, normalized_input = prep_output_sequences(notes, pitchnames, n_vocab)\n",
    "\n",
    "model = create_output_network(normalized_input, n_vocab,weights_file)\n",
    "\n",
    "prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "\n",
    "midi = create_midi(prediction_output,output_name,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model adapted from Sigurour Skuli's [How to Generate Music using a LSTM Neural Network in Keras](https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
