{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MG LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cipher000/anaconda3/envs/tensorflow1.4/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/cipher000/anaconda3/envs/tensorflow1.4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cipher000/anaconda3/envs/tensorflow1.4/bin/python\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "print(sys.executable)\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_file = '../data/notes'\n",
    "midi_files = '../data/MidiWorld/Pop/AceofBase-ThatSheWants.mid'\n",
    "# weights_file = '../weights/lstm_weights.hdf5'\n",
    "\n",
    "output_name = midi_files.split('/')[-2]\n",
    "\n",
    "timestamp = str(datetime.datetime.now()).split()[0].replace('-','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100 # the lstm will predict the next note based on the last set of notes heard\n",
    "node1 = 512\n",
    "node2 = 256\n",
    "drop = 0.3\n",
    "epochs = 1 # 200\n",
    "batch_size = 64\n",
    "notes_generated = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_notes():\n",
    "    notes = []\n",
    "    notes_dict = {}\n",
    "    cnt = 0\n",
    "    \n",
    "    print(\"Loading Midi files\")\n",
    "    for file in glob.glob(midi_files):\n",
    "        print(file)\n",
    "        notes_per_file = []\n",
    "        try:\n",
    "            midi = converter.parse(file)\n",
    "            parts = instrument.partitionByInstrument(midi)\n",
    "\n",
    "            if parts:\n",
    "                notes_to_parse = parts.parts[0].recurse()\n",
    "            else:\n",
    "                notes_to_parse = midi.flat.notes\n",
    "\n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                    notes_per_file.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    to_append = '.'.join(str(n) for n in element.normalOrder)\n",
    "                    notes.append(to_append)\n",
    "                    notes_per_file.append(to_append)\n",
    "            notes_dict[file] = notes_per_file\n",
    "            cnt +=1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    with open(notes_file, 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "    print(\"{} midi files and {} notes\".format(cnt,len(notes)))\n",
    "    print(\"Notes Converted\")\n",
    "    n_vocab = len(set(notes))\n",
    "    return notes, n_vocab, cnt\n",
    "\n",
    "def prep_train_sequences(notes, n_vocab):\n",
    "    print(\"Preparing sequences for training\")\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    \n",
    "    for i in range(0,len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        \n",
    "        input_append = [note_to_int[char] for char in sequence_in]\n",
    "        network_input.append(input_append)\n",
    "        output_append = note_to_int[sequence_out]\n",
    "        network_output.append(output_append)\n",
    "        \n",
    "    n_patterns = len(network_input)\n",
    "    \n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "    print(\"Sequences Prepared\")\n",
    "    return pitchnames, network_input, network_output\n",
    "\n",
    "def create_network(network_input, n_vocab,weights_file=None):\n",
    "    print(\"Keras model initializing\")\n",
    "    model = Sequential()\n",
    "    # Layer 1\n",
    "    model.add(LSTM(node1,input_shape=(network_input.shape[1], network_input.shape[2]),return_sequences=True))\n",
    "    model.add(Dropout(drop))\n",
    "    # Layer 2\n",
    "    model.add(LSTM(node1, return_sequences=True))\n",
    "    model.add(Dropout(drop))\n",
    "    # Layer 3\n",
    "    model.add(LSTM(node1))\n",
    "    model.add(Dense(node2))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    if weights_file:\n",
    "        model.load_weights(weights_file)\n",
    "        print(\"LSTM model initialized for midi creation with weights from {}\".format(weights_file))\n",
    "    else:\n",
    "        print(\"LSTM model initialized for training - no weights file\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train(model, network_input, network_output):\n",
    "    print(\"Training LSTM network\")\n",
    "    filepath = \"../weights/weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    \n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    model.fit(\n",
    "        network_input, \n",
    "        network_output, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        callbacks=callbacks_list)\n",
    "    weights_file = '../weights/{}-{}-lstm_weights.hdf5'.format(timestamp, output_name)\n",
    "    model.save_weights(weights_file)\n",
    "    print(\"LSTM training complete - weights saved at: {}\".format(weights_file))\n",
    "    return weights_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_output_sequences(notes, pitchnames, n_vocab):\n",
    "    print(\"Preparing sequences for output\")\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    \n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "        \n",
    "    n_patterns = len(network_input)\n",
    "    \n",
    "    normalized_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "    \n",
    "    return network_input, normalized_input\n",
    "\n",
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    print(\"Generating notes\")\n",
    "    start = np.random.randint(0,len(network_input)-1)\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "    \n",
    "    for note_index in range(notes_generated):\n",
    "        prediction_input = np.reshape(pattern, (1,len(pattern),1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "        \n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "        \n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "        \n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "        \n",
    "    return prediction_output\n",
    "\n",
    "def create_midi(prediction_output,output_name, epochs):\n",
    "    print(\"Creating midi\")\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    \n",
    "    for pattern in prediction_output:\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Flute()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "        \n",
    "        offset += 0.5\n",
    "        \n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    output_file = '../output/{}-{}-lstm_midi-{}.mid'.format(timestamp,output_name,epochs)\n",
    "    midi_stream.write('midi',fp=output_file)\n",
    "    print(\"Midi saved at: {}\".format(output_file))\n",
    "    return midi_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_create_midi():\n",
    "    # Midi preparation\n",
    "    notes, n_vocab, cnt = convert_to_notes()\n",
    "    pitchnames, network_input, network_output = prep_train_sequences(notes, n_vocab)\n",
    "    # LSTM training\n",
    "    model = create_network(network_input, n_vocab)\n",
    "    weights_file = train(model, network_input, network_output)\n",
    "    # Midi creation\n",
    "    network_input, normalized_input = prep_output_sequences(notes, pitchnames, n_vocab)\n",
    "    model = create_network(normalized_input, n_vocab,weights_file)\n",
    "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "    midi = create_midi(prediction_output,output_name,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Midi files\n",
      "../data/MidiWorld/Pop/AceofBase-ThatSheWants.mid\n",
      "1 midi files and 992 notes\n",
      "Notes Converted\n",
      "Preparing sequences for training\n",
      "Sequences Prepared\n",
      "Keras model initializing\n",
      "LSTM model initialized for training - no weights file\n",
      "Training LSTM network\n",
      "Epoch 1/1\n",
      "892/892 [==============================] - 43s 48ms/step - loss: 3.5625\n",
      "LSTM training complete - weights saved at: ../weights/20180323-Pop-lstm_weights.hdf5\n",
      "Preparing sequences for output\n",
      "Keras model initializing\n",
      "LSTM model initialized for midi creation with weights from ../weights/20180323-Pop-lstm_weights.hdf5\n",
      "Generating notes\n",
      "Creating midi\n",
      "Midi saved at: ../output/20180323-Pop-lstm_midi-1.mid\n"
     ]
    }
   ],
   "source": [
    "train_create_midi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model adapted from Sigurour Skuli's [How to Generate Music using a LSTM Neural Network in Keras](https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
