{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MG LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cipher000/anaconda3/envs/tensorflow1.4/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/cipher000/anaconda3/envs/tensorflow1.4/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cipher000/anaconda3/envs/tensorflow1.4/bin/python\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "print(sys.executable)\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notes_filepath = '../data/output_notes'\n",
    "midi_files = '../data/MidiWorld/Pop/AceofBase-ThatSheWants.mid'\n",
    "# weights_file = '../weights/lstm_weights.hdf5'\n",
    "\n",
    "\n",
    "\n",
    "output_name = midi_files.split('/')[-2]\n",
    "\n",
    "timestamp = str(datetime.datetime.now()).split()[0].replace('-','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100 # the lstm will predict the next note based on the last set of notes heard\n",
    "node1 = 512\n",
    "node2 = 256\n",
    "drop = 0.3\n",
    "epochs = 1 # 200\n",
    "batch_size = 64\n",
    "notes_generated = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_notes():\n",
    "    notes = [] # list of notes and chords\n",
    "    cnt = 0\n",
    "    \n",
    "    print(\"\\n**Loading Midi files**\")\n",
    "    for file in glob.glob(midi_files): # loading midi filepaths\n",
    "        print(file)\n",
    "        try:\n",
    "            midi = converter.parse(file) # midi type music21.stream.Score\n",
    "            parts = instrument.partitionByInstrument(midi) # parts type music21.stream.Score\n",
    "\n",
    "            if parts: \n",
    "                notes_to_parse = parts.parts[0].recurse()\n",
    "            else:\n",
    "                notes_to_parse = midi.flat.notes \n",
    "            # notes_to_parse type music21.stream.iterator.RecursiveIterator\n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    to_append = '.'.join(str(n) for n in element.normalOrder)\n",
    "                    notes.append(to_append)\n",
    "            cnt +=1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "#     with open(notes_file, 'wb') as filepath:\n",
    "#         pickle.dump(notes, filepath)\n",
    "    n_vocab = len(set(notes))\n",
    "    print(\"Notes Converted.\\n# Midi Files: {} # Notes: {} # Unique Notes: {}\".format(cnt,len(notes),n_vocab))\n",
    "    return notes, n_vocab\n",
    "\n",
    "def prep_input(notes, n_vocab):\n",
    "    print(\"\\n**Preparing sequences for training**\")\n",
    "    pitchnames = sorted(set(item for item in notes)) # list of unique chords and notes\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames)) # enumerate pitchnames in dict\n",
    "    \n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    \n",
    "    # i equals total notes less declared sequence length of LSTM (ie 5000 - 100)\n",
    "    # sequence input for each i is list of notes i to end of sequence length (ie 0-100 for i = 0)\n",
    "    # sequence output for each i is single note at i + sequence length (ie 100 for i = 0)\n",
    "    for i in range(0,len(notes) - sequence_length, 1): \n",
    "        sequence_in = notes[i:i + sequence_length] # 100\n",
    "        sequence_out = notes[i + sequence_length] # 1\n",
    "        \n",
    "        # enumerate notes and chord sequences with note_to_int enumerated encoding\n",
    "        # network input/output is a list of encoded notes and chords based on note_to_int encoding\n",
    "        # if 100 unique notes/chords, the encoding will be between 0-100\n",
    "        input_add = [note_to_int[char] for char in sequence_in]\n",
    "        network_input.append(input_add) # sequence length\n",
    "        output_add = note_to_int[sequence_out]\n",
    "        network_output.append(output_add) # single note\n",
    "        \n",
    "    n_patterns = len(network_input) # notes less sequence length\n",
    "    print(\"Pitchnames: {}\".format(pitchnames))\n",
    "    print(\"Inputting {} encoded notes/chords into network; sequence length {}\".format(n_patterns,sequence_length))\n",
    "\n",
    "    print(\"\\n**Reshaping for training**\")\n",
    "    net_input_len = len(network_input)\n",
    "    # convert network input/output from lists to numpy arrays\n",
    "    # reshape input to (notes less sequence length, sequence length)\n",
    "    # reshape output to (notes less sequence length, unique notes/chords)\n",
    "    network_input = np.reshape(network_input, (net_input_len, sequence_length, 1))\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "    print(\"Shapes. Network Input: {} Network Output: {}\".format(network_input.shape, network_output.shape))\n",
    "    return pitchnames, network_input, network_output\n",
    "\n",
    "def create_network(network_input, n_vocab,weights_file=None):\n",
    "    print(\"\\n**LSTM model initializing**\")\n",
    "    model = Sequential()\n",
    "    # TODO determine layers - 2 or 3?\n",
    "    # Layer 1\n",
    "    model.add(LSTM(node1,input_shape=(network_input.shape[1], network_input.shape[2]),return_sequences=True))\n",
    "    model.add(Dropout(drop))\n",
    "    # Layer 2\n",
    "    model.add(LSTM(node1, return_sequences=True))\n",
    "    model.add(Dropout(drop))\n",
    "    # Layer 3\n",
    "    model.add(LSTM(node1))\n",
    "    model.add(Dense(node2))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    if weights_file:\n",
    "        model.load_weights(weights_file)\n",
    "        print(\"LSTM model initialized for midi creation with weights from {}\".format(weights_file))\n",
    "    else:\n",
    "        print(\"LSTM model initialized for training (no weights file)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train(model, network_input, network_output):\n",
    "    print(\"\\n**Training LSTM network**\")\n",
    "    filepath = \"../weights/weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    \n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    print(\"Fitting Model. \\nNetwork Input Shape: {} Network Output Shape: {}\".format(network_input.shape,network_output.shape))\n",
    "    print(\"Epochs: {} Batch Size: {}\".format(epochs, batch_size))\n",
    "    \n",
    "    model.fit(\n",
    "        network_input, \n",
    "        network_output, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        callbacks=callbacks_list)\n",
    "    weights_file = '../weights/{}-{}-lstm_weights.hdf5'.format(timestamp, output_name)\n",
    "    model.save_weights(weights_file)\n",
    "    print(\"LSTM training complete - weights saved at: {}\".format(weights_file))\n",
    "    return weights_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MIDI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_output_sequences(notes, pitchnames, n_vocab):\n",
    "#     TODO determine if this function is duplicative\n",
    "    print(\"\\nPreparing sequences for output\")\n",
    " \n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    \n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    print(\"Note to Int: {}\".format(note_to_int))\n",
    "    \n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "        \n",
    "    n_patterns = len(network_input)\n",
    "    \n",
    "    normalized_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    normalized_input = normalized_input / float(len(pitchnames))\n",
    "    \n",
    "    return network_input, normalized_input\n",
    "\n",
    "# def reshape_for_creation(network_input, sequence_length,pitchnames):\n",
    "#     print(\"\\n**Reshaping for midi creation**\")\n",
    "#     net_input_len = len(network_input)\n",
    "#     # convert network input/output from lists to numpy arrays\n",
    "#     # reshape input to (notes less sequence length, sequence length)\n",
    "#     # reshape output to (notes less sequence length, unique notes/chords)\n",
    "#     normalized_input = np.reshape(network_input, (net_input_len, sequence_length, 1))\n",
    "#     normalized_input = normalized_input / float(len(pitchnames))\n",
    "#     print(\"Shapes. Network Input: {} Normalized Input: {}\".format(network_input.shape, normalized_input.shape))\n",
    "#     return normalized_input\n",
    "\n",
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    print(\"\\n**Generating notes**\")\n",
    "    start = np.random.randint(0,len(network_input)-1)\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames)) # convert integers back to notes\n",
    "    \n",
    "    pattern = network_input[start] # randomly instantiate with single number from 0 to length of network input\n",
    "    prediction_output = []\n",
    "    \n",
    "    # for each note in notes generated declared as hyperparameter above (ie 500)\n",
    "    for note_index in range(notes_generated): \n",
    "#         print(\"Note Index: {}\".format(note_index))\n",
    "        prediction_input = np.reshape(pattern, (1,len(pattern),1))\n",
    "#         print(\"Prediction Input Shape: {}\".format(prediction_input.shape))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "#         print(\"Prediction Input Shape after dividing by unique chords/notes: {}\".format(prediction_input.shape))\n",
    "        \n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "#         print(\"Prediction: {}\".format(prediction))\n",
    "        \n",
    "        index = np.argmax(prediction)\n",
    "#         print(\"Index: {}\".format(index))\n",
    "        result = int_to_note[index]\n",
    "#         print(\"Result: {}\".format(result))\n",
    "        prediction_output.append(result)\n",
    "        \n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "#         print(\"Pattern: {}\".format(pattern))\n",
    "    print(\"Prediction Output: {}\".format(prediction_output))\n",
    "        \n",
    "    return prediction_output\n",
    "\n",
    "def create_midi(prediction_output,output_name, epochs):\n",
    "    print(\"\\n**Creating midi**\")\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    \n",
    "    for pattern in prediction_output:\n",
    "        sound = instrument.Flute() # declare a Music21 package instrument\n",
    "        # prepares chords (if) and notes (else)\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = sound \n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = sound \n",
    "            output_notes.append(new_note)\n",
    "        \n",
    "        offset += 0.5\n",
    "        \n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    output_file = '../output/{}-{}-lstm_midi-{}.mid'.format(timestamp,output_name,epochs)\n",
    "    midi_stream.write('midi',fp=output_file)\n",
    "    print(\"Midi saved at: {}\".format(output_file))\n",
    "    with open(output_notes_filepath, 'wb') as f:\n",
    "        pickle.dump(output_notes, f)\n",
    "    return output_notes, midi_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_create_midi():\n",
    "    # Execute all functions from training to midi creation\n",
    "    # Midi preparation\n",
    "    notes, n_vocab = convert_to_notes() \n",
    "    pitchnames, network_input, network_output = prep_input(notes, n_vocab) \n",
    "    # LSTM training\n",
    "    model = create_network(network_input, n_vocab)\n",
    "    weights_file = train(model, network_input, network_output)\n",
    "    # Midi creation\n",
    "    normalized_input = prep_output_sequences(notes, pitchnames, n_vocab)\n",
    "    model = create_network(normalized_input, pitchnames,weights_file)\n",
    "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "    output_notes, midi = create_midi(prediction_output,output_name,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Loading Midi files**\n",
      "../data/MidiWorld/Pop/AceofBase-ThatSheWants.mid\n",
      "Notes Converted.\n",
      "# Midi Files: 1 # Notes: 992 # Unique Notes: 32\n",
      "\n",
      "**Preparing sequences for training**\n",
      "Pitchnames: ['1.4.8', '1.5.8', '1.6', '11.3.6', '6.10.1', '6.9.1', '8.0.3', 'A3', 'A4', 'A5', 'B-3', 'B-4', 'B-5', 'B2', 'B4', 'C#3', 'C#4', 'C#5', 'C#6', 'C2', 'C3', 'C4', 'C5', 'E-5', 'E5', 'F#3', 'F#5', 'F5', 'G#2', 'G#3', 'G#4', 'G#5']\n",
      "Inputting 892 encoded notes/chords into network; sequence length 100\n",
      "\n",
      "**Reshaping for training**\n",
      "Shapes. Network Input: (892, 100, 1) Network Output: (892, 32)\n",
      "\n",
      "**LSTM model initializing**\n",
      "LSTM model initialized for training (no weights file)\n",
      "\n",
      "**Training LSTM network**\n",
      "Fitting Model. \n",
      "Network Input Shape: (892, 100, 1) Network Output Shape: (892, 32)\n",
      "Epochs: 1 Batch Size: 64\n",
      "Epoch 1/1\n",
      "892/892 [==============================] - 42s 47ms/step - loss: 3.6359\n",
      "LSTM training complete - weights saved at: ../weights/20180323-Pop-lstm_weights.hdf5\n",
      "\n",
      "Preparing sequences for output\n",
      "Note to Int: {'1.4.8': 0, '1.5.8': 1, '1.6': 2, '11.3.6': 3, '6.10.1': 4, '6.9.1': 5, '8.0.3': 6, 'A3': 7, 'A4': 8, 'A5': 9, 'B-3': 10, 'B-4': 11, 'B-5': 12, 'B2': 13, 'B4': 14, 'C#3': 15, 'C#4': 16, 'C#5': 17, 'C#6': 18, 'C2': 19, 'C3': 20, 'C4': 21, 'C5': 22, 'E-5': 23, 'E5': 24, 'F#3': 25, 'F#5': 26, 'F5': 27, 'G#2': 28, 'G#3': 29, 'G#4': 30, 'G#5': 31}\n",
      "\n",
      "**LSTM model initializing**\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8dbab1c3fd85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_create_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-d96e385d3c5f>\u001b[0m in \u001b[0;36mtrain_create_midi\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Midi creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnormalized_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_output_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitchnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitchnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprediction_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_notes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitchnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moutput_notes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmidi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8e0a3fbfd4d2>\u001b[0m in \u001b[0;36mcreate_network\u001b[0;34m(network_input, n_vocab, weights_file)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# TODO determine layers - 2 or 3?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Layer 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Layer 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_create_midi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model adapted from Sigurour Skuli's [How to Generate Music using a LSTM Neural Network in Keras](https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
