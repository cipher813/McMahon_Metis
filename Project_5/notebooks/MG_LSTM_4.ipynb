{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MG LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/matplotlib/__init__.py:1057: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/matplotlib/__init__.py:1057: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/MusicGenerator1/bin/python\n",
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "print(sys.executable)\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notes_file = '../data/output_notes'\n",
    "midi_files = '../data/BritneySpears/D*.mid'\n",
    "# weights_file = '../weights/lstm_weights.hdf5'\n",
    "\n",
    "output_name = midi_files.split('/')[-2]\n",
    "\n",
    "timestamp = str(datetime.datetime.now()).split()[0].replace('-','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100 # the lstm will predict the next note based on the last set of notes heard\n",
    "node1 = 512\n",
    "node2 = 256\n",
    "drop = 0.3\n",
    "epochs = 1 # 200\n",
    "batch_size = 64\n",
    "notes_generated = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_notes():\n",
    "    notes = [] # list of notes and chords\n",
    "    cnt = 0\n",
    "    \n",
    "    print(\"\\n**Loading Midi files**\")\n",
    "    for file in glob.glob(midi_files): # loading midi filepaths\n",
    "        print(file)\n",
    "        try:\n",
    "            midi = converter.parse(file) # midi type music21.stream.Score\n",
    "            parts = instrument.partitionByInstrument(midi) # parts type music21.stream.Score\n",
    "\n",
    "            if parts: \n",
    "                notes_to_parse = parts.parts[0].recurse()\n",
    "            else:\n",
    "                notes_to_parse = midi.flat.notes \n",
    "            # notes_to_parse type music21.stream.iterator.RecursiveIterator\n",
    "            # appends to notes_per file; notes_per_file then appended to\n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    to_append = '.'.join(str(n) for n in element.normalOrder)\n",
    "                    notes.append(to_append)\n",
    "            cnt +=1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "#     with open(notes_file, 'wb') as filepath:\n",
    "#         pickle.dump(notes, filepath)\n",
    "    n_vocab = len(set(notes))\n",
    "    print(\"Notes Converted.\\n# Midi Files: {} # Notes: {} # Unique Notes: {}\".format(cnt,len(notes),n_vocab))\n",
    "    return notes\n",
    "\n",
    "def prep_input(notes):\n",
    "    print(\"\\n**Preparing sequences for training**\")\n",
    "    pitchnames = sorted(set(item for item in notes)) # list of unique chords and notes\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames)) # enumerate pitchnames in dict\n",
    "    \n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    \n",
    "    # i equals total notes less declared sequence length of LSTM (ie 5000 - 100)\n",
    "    # sequence input for each i is list of notes i to end of sequence length (ie 0-100 for i = 0)\n",
    "    # sequence output for each i is single note at i + sequence length (ie 100 for i = 0)\n",
    "    for i in range(0,len(notes) - sequence_length, 1): \n",
    "        sequence_in = notes[i:i + sequence_length] # 100\n",
    "        sequence_out = notes[i + sequence_length] # 1\n",
    "        \n",
    "        # enumerate notes and chord sequences with note_to_int enumerated encoding\n",
    "        # network input/output is a list of encoded notes and chords based on note_to_int encoding\n",
    "        # if 100 unique notes/chords, the encoding will be between 0-100\n",
    "        input_add = [note_to_int[char] for char in sequence_in]\n",
    "        network_input.append(input_add) # sequence length\n",
    "        output_add = note_to_int[sequence_out]\n",
    "        network_output.append(output_add) # single note\n",
    "        \n",
    "    net_input_len = len(network_input) # notes less sequence length\n",
    "    print(\"Pitchnames: {}\".format(pitchnames))\n",
    "    print(\"Inputting {} encoded notes/chords into network; sequence length {}\".format(net_input_len,sequence_length))\n",
    "    return pitchnames, network_input, network_output\n",
    "\n",
    "def reshape_for_training(network_input, network_output, sequence_length):\n",
    "    print(\"\\n**Reshaping for training**\")\n",
    "    net_input_len = len(network_input)\n",
    "    # convert network input/output from lists to numpy arrays\n",
    "    # reshape input to (notes less sequence length, sequence length)\n",
    "    # reshape output to (notes less sequence length, unique notes/chords)\n",
    "    network_input = np.reshape(network_input, (net_input_len, sequence_length, 1))\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "    print(\"Shapes. Network Input: {} Network Output: {}\".format(network_input.shape, network_output.shape))\n",
    "    return network_input, network_output\n",
    "\n",
    "def create_network(network_input, pitchnames,weights_file=None):\n",
    "    print(\"\\n**LSTM model initializing**\")\n",
    "    n_vocab = len(pitchnames)\n",
    "    model = Sequential()\n",
    "    # TODO determine layers - 2 or 3?\n",
    "    # Layer 1\n",
    "    timesteps = network_input.shape[1] \n",
    "    data_dim = network_input.shape[2]\n",
    "    model.add(LSTM(node1,input_shape=(timesteps, data_dim),return_sequences=True))\n",
    "    model.add(Dropout(drop))\n",
    "    # Layer 2\n",
    "    model.add(LSTM(node1, return_sequences=True))\n",
    "    model.add(Dropout(drop))\n",
    "    # Layer 3\n",
    "    model.add(LSTM(node1))\n",
    "    model.add(Dense(node2))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    if weights_file:\n",
    "        model.load_weights(weights_file)\n",
    "        print(\"LSTM model initialized for midi creation with weights from {}\".format(weights_file))\n",
    "    else:\n",
    "        print(\"LSTM model initialized for training (no weights file)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train(model, network_input, network_output):\n",
    "    print(\"\\n**Training LSTM network**\")\n",
    "    filepath = \"../weights/weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min')\n",
    "    \n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    print(\"Fitting Model. \\nNetwork Input Shape: {} Network Output Shape: {}\".format(network_input.shape,network_output.shape))\n",
    "    print(\"Epochs: {} Batch Size: {}\".format(epochs, batch_size))\n",
    "    \n",
    "    model.fit(\n",
    "        network_input, \n",
    "        network_output, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        callbacks=callbacks_list)\n",
    "    weights_file = '../weights/{}-{}-lstm_weights.hdf5'.format(timestamp, output_name)\n",
    "    model.save_weights(weights_file)\n",
    "    print(\"LSTM training complete - weights saved at: {}\".format(weights_file))\n",
    "    return weights_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create MIDI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_creation(network_input, sequence_length,pitchnames):\n",
    "    print(\"\\n**Reshaping for midi creation**\")\n",
    "    net_input_len = len(network_input)\n",
    "    # convert network input/output from lists to numpy arrays\n",
    "    # reshape input to (notes less sequence length, sequence length)\n",
    "    # reshape output to (notes less sequence length, unique notes/chords)\n",
    "    normalized_input = np.reshape(network_input, (net_input_len, sequence_length, 1))\n",
    "    normalized_input = normalized_input / float(len(pitchnames))\n",
    "    print(\"Shapes. Network Input: {} Normalized Input: {}\".format(network_input.shape, normalized_input.shape))\n",
    "    return network_input, normalized_input\n",
    "\n",
    "def generate_notes(model, network_input, pitchnames):\n",
    "    print(\"\\n**Generating notes**\")\n",
    "    n_vocab = len(pitchnames)\n",
    "    start = np.random.randint(0,len(network_input)-1)\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames)) # convert integers back to notes\n",
    "    \n",
    "    pattern = network_input[start] # randomly instantiate with single number from 0 to length of network input\n",
    "    prediction_output = []\n",
    "    \n",
    "    # for each note in notes generated declared as hyperparameter above (ie 500)\n",
    "    for note_index in range(notes_generated): \n",
    "        print(\"Note Index: {}\".format(note_index))\n",
    "        prediction_input = np.reshape(pattern, (1,len(pattern),1))\n",
    "        print(\"Prediction Input Shape: {}\".format(prediction_input.shape))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "        print(\"Prediction Input Shape after dividing by unique chords/notes: {}\".format(prediction_input))\n",
    "        \n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "        print(\"Prediction: {}\".format(prediction))\n",
    "        \n",
    "        index = np.argmax(prediction)\n",
    "        print(\"Index: {}\".format(index))\n",
    "        result = int_to_note[index]\n",
    "        print(\"Result: {}\".format(result))\n",
    "        prediction_output.append(result)\n",
    "        \n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "        print(\"Pattern: {}\".format(pattern))\n",
    "    print(\"Prediction Output: {}\".format(prediction_output))\n",
    "        \n",
    "    return prediction_output\n",
    "\n",
    "def create_midi(prediction_output,output_name, epochs):\n",
    "    print(\"\\n**Creating midi**\")\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    \n",
    "    for pattern in prediction_output:\n",
    "        sound = instrument.Flute() # declare a Music21 package instrument\n",
    "        # prepares chords (if) and notes (else)\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = sound \n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = sound \n",
    "            output_notes.append(new_note)\n",
    "        \n",
    "        offset += 0.5\n",
    "        \n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    output_file = '../output/{}-{}-lstm_midi-{}.mid'.format(timestamp,output_name,epochs)\n",
    "    midi_stream.write('midi',fp=output_file)\n",
    "    print(\"Midi saved at: {}\".format(output_file))\n",
    "    with open(output_notes_filepath, 'wb') as f:\n",
    "        pickle.dump(output_notes, f)\n",
    "    return output_notes, midi_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_create_midi():\n",
    "    # Execute all functions from training to midi creation\n",
    "    # Midi preparation\n",
    "    notes = convert_to_notes() \n",
    "    pitchnames, network_input, network_output = prep_input(notes) \n",
    "    network_input, network_output = reshape_for_training(network_input, network_output, sequence_length)\n",
    "    # LSTM training\n",
    "    model = create_network(network_input, pitchnames)\n",
    "    weights_file = train(model, network_input, network_output)\n",
    "    # Midi creation\n",
    "    network_input, normalized_input = reshape_for_creation(network_input, sequence_length,pitchnames)\n",
    "    model = create_network(normalized_input, pitchnames,weights_file)\n",
    "    prediction_output = generate_notes(model, network_input, pitchnames)\n",
    "    output_notes, midi = create_midi(prediction_output,output_name,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Loading Midi files**\n",
      "../data/BritneySpears/DriveMeCrazyBritneySpears.mid\n",
      "expected 0 arguments, got 1\n",
      "../data/BritneySpears/DriveMeCrazyRemixBritneySpears.mid\n",
      "Notes Converted.\n",
      "# Midi Files: 1 # Notes: 2302 # Unique Notes: 9\n",
      "\n",
      "**Preparing sequences for training**\n",
      "Pitchnames: ['0.6', '3.6', '6.8', '6.8.10', 'C2', 'E-2', 'F#2', 'F#3', 'G#3']\n",
      "Inputting 2202 encoded notes/chords into network; sequence length 100\n",
      "\n",
      "**Reshaping for training**\n",
      "Shapes. Network Input: (2202, 100, 1) Network Output: (2202, 9)\n",
      "\n",
      "**LSTM model initializing**\n",
      "LSTM model initialized for training (no weights file)\n",
      "\n",
      "**Training LSTM network**\n",
      "Fitting Model. \n",
      "Network Input Shape: (2202, 100, 1) Network Output Shape: (2202, 9)\n",
      "Epochs: 1 Batch Size: 64\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[100,64,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lstm_1/TensorArrayStack/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[\"loc:@lstm_1/TensorArray\"], dtype=DT_FLOAT, element_shape=[?,512], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_1/TensorArray, lstm_1/TensorArrayStack/range, lstm_1/while/Exit_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_109 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_4779_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'lstm_1/TensorArrayStack/TensorArrayGatherV3', defined at:\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/asyncio/base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tornado/ioloop.py\", line 760, in _run_callback\n    ret = callback()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-8dbab1c3fd85>\", line 1, in <module>\n    train_create_midi()\n  File \"<ipython-input-6-01d00ba9feba>\", line 8, in train_create_midi\n    model = create_network(network_input, pitchnames)\n  File \"<ipython-input-4-61942f9e247a>\", line 81, in create_network\n    model.add(LSTM(node1,input_shape=(timesteps, data_dim),return_sequences=True))\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/models.py\", line 467, in add\n    layer(x)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 499, in __call__\n    return super(RNN, self).__call__(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 2151, in call\n    initial_state=initial_state)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 608, in call\n    input_length=timesteps)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2772, in rnn\n    outputs = output_ta.stack()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 893, in stack\n    return self._implementation.stack(name=name)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 291, in stack\n    return self.gather(math_ops.range(0, self.size()), name=name)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 305, in gather\n    element_shape=element_shape)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 4186, in _tensor_array_gather_v3\n    flow_in=flow_in, dtype=dtype, element_shape=element_shape, name=name)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,64,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lstm_1/TensorArrayStack/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[\"loc:@lstm_1/TensorArray\"], dtype=DT_FLOAT, element_shape=[?,512], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_1/TensorArray, lstm_1/TensorArrayStack/range, lstm_1/while/Exit_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_109 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_4779_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    517\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100,64,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lstm_1/TensorArrayStack/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[\"loc:@lstm_1/TensorArray\"], dtype=DT_FLOAT, element_shape=[?,512], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_1/TensorArray, lstm_1/TensorArrayStack/range, lstm_1/while/Exit_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_109 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_4779_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8dbab1c3fd85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_create_midi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-01d00ba9feba>\u001b[0m in \u001b[0;36mtrain_create_midi\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# LSTM training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitchnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mweights_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Midi creation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnetwork_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape_for_creation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpitchnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-61942f9e247a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, network_input, network_output)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mweights_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../weights/{}-{}-lstm_weights.hdf5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100,64,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lstm_1/TensorArrayStack/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[\"loc:@lstm_1/TensorArray\"], dtype=DT_FLOAT, element_shape=[?,512], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_1/TensorArray, lstm_1/TensorArrayStack/range, lstm_1/while/Exit_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_109 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_4779_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'lstm_1/TensorArrayStack/TensorArrayGatherV3', defined at:\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/asyncio/base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tornado/ioloop.py\", line 760, in _run_callback\n    ret = callback()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-8dbab1c3fd85>\", line 1, in <module>\n    train_create_midi()\n  File \"<ipython-input-6-01d00ba9feba>\", line 8, in train_create_midi\n    model = create_network(network_input, pitchnames)\n  File \"<ipython-input-4-61942f9e247a>\", line 81, in create_network\n    model.add(LSTM(node1,input_shape=(timesteps, data_dim),return_sequences=True))\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/models.py\", line 467, in add\n    layer(x)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 499, in __call__\n    return super(RNN, self).__call__(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/engine/topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 2151, in call\n    initial_state=initial_state)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 608, in call\n    input_length=timesteps)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2772, in rnn\n    outputs = output_ta.stack()\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 893, in stack\n    return self._implementation.stack(name=name)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 291, in stack\n    return self.gather(math_ops.range(0, self.size()), name=name)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 305, in gather\n    element_shape=element_shape)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 4186, in _tensor_array_gather_v3\n    flow_in=flow_in, dtype=dtype, element_shape=element_shape, name=name)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/MusicGenerator1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[100,64,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lstm_1/TensorArrayStack/TensorArrayGatherV3 = TensorArrayGatherV3[_class=[\"loc:@lstm_1/TensorArray\"], dtype=DT_FLOAT, element_shape=[?,512], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_1/TensorArray, lstm_1/TensorArrayStack/range, lstm_1/while/Exit_1)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_109 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_4779_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "train_create_midi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model adapted from Sigurour Skuli's [How to Generate Music using a LSTM Neural Network in Keras](https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_musicgenerator1)",
   "language": "python",
   "name": "conda_musicgenerator1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
